{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Large Language Models: LLaMA and Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will dive into two famous large language models, LLaMA and Mistral, along with their instruction-tuned versions. We'll explore how each model performs on various tasks, with a particular focus on generating structured responses in JSON format.\n",
    "\n",
    "These models have been fine-tuned to follow instructions, making them suitable for a range of NLP applications. Through this lab, you will:\n",
    "\n",
    "- Learn how to load and interact with LLaMA and Mistral models using the `pipeline` and `chat_template` functions.\n",
    "- Examine the performance of their instruction-based variants.\n",
    "- Generate structured outputs, specifically in JSON, for practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Disclaimer**: Before starting this lab, ensure you have requested access to the required models on Hugging Face and have logged in to your Hugging Face account. Access is necessary for the following models:\n",
    ">\n",
    "> - [Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)\n",
    "> - [Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)\n",
    "> - [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    ">\n",
    "> You can log in to Hugging Face directly from this notebook using the provided code snippet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Login to the Hugging Face model hub to be able to upload models\n",
    "token = \"YOUR HUGGING FACE TOKEN\"\n",
    "\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lab, we will explore **LLaMA (Large Language Model Meta AI)**, which is one of the most known large language models developed by Meta (Facebook). \n",
    "\n",
    "Next, we will focus on **Instruction LLaMA**, a version of LLaMA fine-tuned to better understand and follow user instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Llama 3.2 (released in September 2024). In particular, we will adopt the 1B version. On the scale of things, this model is on the smaller side, but it is still a very powerful model.\n",
    "\n",
    "It has been released (along with a 3B version) with the intention of allowing running it on devices with modest hardware (e.g., mobile phones or other edge devices). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# TODO: Load the model with `torch.float16` precision and the tokenizer \n",
    "# (You can specify the precision with `torch_dtype=torch.float16`)\n",
    "model = ...\n",
    "tokenizer = ...\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this model to generate text using the generate() method. We use random sampling (`do_sample=True`) and extract 5 samples (`num_return_sequences=5`). You can find other generation parameters [here](https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/text_generation#transformers.GenerationConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n",
    "batch = model.generate(**tokens, do_sample=True, max_length=50, num_return_sequences=5, pad_token_id=tokenizer.eos_token_id) # (assigning pad_token_id avoids a warning)\n",
    "tokenizer.batch_decode(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding the `tokenizer.chat_template`**\n",
    "\n",
    "In this section, we will explore the **chat template** that is used to format and structure messages for a conversational assistant. The `tokenizer.chat_template` is a convenient way for organizing interactions between the user, system, and assistant in a way that the model can easily process and generate coherent responses.\n",
    "\n",
    "### **What is a Chat Template?**\n",
    "\n",
    "The chat template is a predefined format that ensures consistent structure for conversations. It marks the different roles in the interaction (system, user, assistant), and separates the various elements of the conversation using special tokens. This helps the language model understand which parts of the dialogue are instructions, which parts are user inputs, and where the assistant’s response should be generated.\n",
    "\n",
    "Let's create an example of a possible (simplified) chat template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "chat_template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: \"\"\"+datetime.datetime.now().strftime(\"%d %b %Y\")+\"\"\"\n",
    "\n",
    "{system_message}\n",
    "\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_message}\n",
    "\n",
    "<|eot_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hugging Face Pipeline Overview**\n",
    "\n",
    "The **`pipeline`** method from Hugging Face’s Transformers library is a high-level API designed to streamline the process of using pre-trained models for a wide variety of **natural language processing (NLP) tasks**.\n",
    "\n",
    "#### **What is a Pipeline?**\n",
    "\n",
    "A pipeline is a modular tool that wraps around a pre-trained model, tokenizer, and task-specific configurations. It makes it easy to load and apply these models directly to different tasks, such as:\n",
    "- **Text generation**\n",
    "- **Text classification**\n",
    "- **Question answering**\n",
    "- **Summarization**\n",
    "- **Translation**\n",
    "\n",
    "By simply specifying the type of task (e.g., `\"text-generation\"`), `pipeline` takes care of loading and configuring a compatible model and tokenizer, providing a ready-to-use interface for generating results.\n",
    "\n",
    "You can find a full list of supported pipelines on the [Hugging Face documentation](https://huggingface.co/docs/transformers/main_classes/pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline with the model and tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
    "]\n",
    "\n",
    "# Format the messages using the chat template\n",
    "formatted_messages = chat_template.format(\n",
    "    system_message=messages[0][\"content\"],\n",
    "    user_message=messages[1][\"content\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(formatted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember that for models to follow instruction tuning, they need to have been tuned on this kind of data. In this case, we are not using the instruction-tuned version. \n",
    "\n",
    "So, we can expect the model to produce a garbage response (it has never seen that kind of inputs before!). But let's try it anyway!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the output text \n",
    "outputs = pipe(\n",
    "    formatted_messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Differences Between Standard and Instruct Versions of Large Language Models (LLMs)**\n",
    "\n",
    "Large Language Models (LLMs) come in different versions, with **standard** and **instruction-tuned (Instruct)** versions being the most common. Here’s a brief comparison:\n",
    "\n",
    "#### **1. Purpose and Training**:\n",
    "   - **Standard LLM**: The standard model is generally pre-trained on large datasets without specific instruction-following capabilities. Typically generates more open-ended responses, which can be useful for creative writing or general information retrieval where the response style is flexible.\n",
    "   - **Instruct LLM**: Instruction-tuned models, like the **Llama-3.2 Instruct**, are fine-tuned on datasets designed to help the model understand and follow instructions effectively. This tuning enhances the model's ability to respond directly to user prompts and handle structured requests. It is fine-tuned to produce concise, direct responses that are often more relevant in task-specific or conversational AI applications.\n",
    "\n",
    "Let's compare the outputs of the standard and Instruct versions of LLaMA to see the differences in their responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# TODO: Load the model with `torch.float16` precision and the tokenizer \n",
    "model = ...\n",
    "tokenizer = ...\n",
    "\n",
    "# TODO: Set the pad token to the end of the sequence token\n",
    "tokenizer.pad_token = ...\n",
    "\n",
    "# TODO: Create the pipeline with the model and tokenizer\n",
    "pipe = ...\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
    "]\n",
    "\n",
    "# TODO: Format the messages using the chat template and generate the output text\n",
    "formatted_messages = ...\n",
    "outputs = ...\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation of the Tokenizer Chat Template**\n",
    "\n",
    "Actually, the chat template of `meta-llama/Llama-3.2-1B-Instruct` is much more complex than the example above. It includes various components that help the model understand the context of the conversation, manage dates, handle tools, and structure messages effectively.\n",
    "\n",
    "The template is written in [jinja](https://jinja.palletsprojects.com/en/stable/templates/), a language that allows for the dynamic generation of content based on variables, conditions and loops.\n",
    "\n",
    "\n",
    "Let's print it and analyze its key components:\n",
    " \n",
    "\n",
    "#### **Key Components of the Template**:\n",
    "1. **System Message Extraction**:\n",
    "   - The system message is extracted if the first role in the message list is labeled \"system.\" This allows the template to clearly differentiate between user queries and system instructions.\n",
    "   - If a system message exists, it is added to the template between special tokens (`<|start_header_id|>` and `<|end_header_id|>`), ensuring that the model knows when the system message starts and ends.\n",
    "\n",
    "2. **Date Management**:\n",
    "   - The template automatically handles the current date using either a provided `strftime_now` function or a default date (`\"26 Jul 2024\"`). This can be useful when the model needs to be aware of the date in contexts such as time-sensitive responses.\n",
    "\n",
    "3. **Handling Tools**:\n",
    "   - The template checks if **tools** are defined. If tools are available, it includes a description of these tools in the system message or the user message, depending on where they need to appear.\n",
    "   - If the tools are part of the user message, the template ensures that the first user message prompts the user to respond in a structured format, such as using JSON for function calls.\n",
    "\n",
    "4. **Message Processing**:\n",
    "   - The template loops through the list of messages and processes each based on the role (`user`, `assistant`, `ipython`, or `tool`). It formats each message using start and end tokens for the roles, helping the model understand the structure of the conversation.\n",
    "   - If the message involves tool calls, the template ensures that they are properly formatted into a structured JSON format to be passed back to the model for further processing.\n",
    "\n",
    "5. **Ending the Assistant's Response**:\n",
    "   - The template leaves a placeholder for the assistant’s response, which the model will generate during inference. This ensures that the assistant's response begins in the correct format, ready to be populated with the generated content.\n",
    "\n",
    "#### **Why Is This Template Needed?**\n",
    "\n",
    "- **Maintains Consistency**: This template ensures that the conversation is structured in a consistent manner, which is crucial for models designed to follow complex instructions or engage in multi-turn conversations.\n",
    "- **Handles Tools**: By incorporating the ability to dynamically introduce tools and functionality, the template allows the model to expand beyond simple text-based conversations and perform function-based tasks.\n",
    "- **Structured Outputs for Tools**: When the conversation involves tool calls (e.g., through APIs or function calls), the template ensures that these interactions are formatted properly for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate again the same example using the `chat_template` of `meta-llama/Llama-3.2-1B-Instruct` and analyze the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a tokenizer that supports the chat template, we can directly call the `apply_chat_template()` method to convert a list of messages (each one a dictionary in the already discussed format) into a prompt.\n",
    "\n",
    "Notice that, since we are not using any particular tools or other functionalities, our template will be similar to the one we manually introduced earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the pipeline with the model and tokenizer\n",
    "pipe = ...\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "input_tokens = tokenizer.apply_chat_template(messages)\n",
    "print(tokenizer.decode(input_tokens))\n",
    "\n",
    "# TODO: Generate the output text (pass input_tokens\n",
    "# as the input. You can use the `max_new_tokens` parameter\n",
    "# to control the length of the output)\n",
    "outputs = ...\n",
    "\n",
    "# we are getting back the full conversation history\n",
    "# as a list of messages outputs[0][\"generated_text\"]\n",
    "# -1 : last message (assistant response)\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the pipeline already supports chat mode, so we can pass the list of messages (as long as they contain role/content keys) directly to the pipeline.\n",
    "\n",
    "Alternatively, we could have passed the prompt as a string. In this case, however, we would have to manually extract the output from the model and parse it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "# TODO: Format the messages using the chat template and generate the output text\n",
    "input_tokens = ...\n",
    "prompt_string = ...\n",
    "\n",
    "outputs = ...\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will explore the use of `Mistral-7B-Instruct-v0.2`developed by Mistral AI to generate structured responses in JSON format. \n",
    "\n",
    "In this exercise, we will generate random math questions and instruct Mistral-7B to respond in a structured JSON format. We will then save the responses to a JSON file and verify the answers programmatically. \n",
    "\n",
    "Let's first repeat the same example we did with LLaMA, but now using Mistral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model ID\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# TODO: Load the model and the tokenizer \n",
    "model = ...\n",
    "\n",
    "tokenizer = ...\n",
    "tokenizer.pad_token = ...\n",
    "\n",
    "# TODO: Initialize the pipeline for text generation\n",
    "pipe = ...\n",
    "\n",
    "# Define the message prompts for the conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
    "]\n",
    "\n",
    "# TODO: Generate the response\n",
    "outputs = pipe(messages, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Print the model's generated response\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a random math question and instruct Mistral-7B to respond in a structured JSON format. We will then save the responses to a JSON file and verify the answers programmatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Generate random math questions:** Use Python to create questions with random numbers in a conversational style (e.g., “What is the sum of 245 and 173?”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_math_questions(num_samples=5):\n",
    "    \"\"\"\n",
    "    Generate random math questions with two numbers for a given number of samples.\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): The number of math questions to generate.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: A list of tuples containing the math question and the two numbers (question, num1, num2).\n",
    "    \"\"\"\n",
    "\n",
    "    # Define templates for math questions with two numbers\n",
    "    templates = [\n",
    "        \"What is the sum of {} and {}?\",\n",
    "        \"Can you add {} and {}?\",\n",
    "        \"Calculate the sum of {} and {} for me.\",\n",
    "        \"How much is {} plus {}?\",\n",
    "        \"Please add {} and {}.\"\n",
    "    ]\n",
    "    \n",
    "    questions = []\n",
    "    for _ in range(num_samples):\n",
    "        # TODO: Randomly select a template and generate two random numbers\n",
    "        template = ...\n",
    "        num1 = ...\n",
    "        num2 = ...\n",
    "        question = template.format(num1, num2)\n",
    "        questions.append((question, num1, num2))  # store question with numbers for validation\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Instruct the model to respond in JSON:** Use a system role instruction to ensure Mistral-7B answers in a JSON format containing the fields `num_1`, `num_2`, and `answer`. This makes the output compatible with automated processing or JSON parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_instruction = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"Answer each question in JSON format with the fields 'num_1', 'num_2', and 'answer'. Provide only JSON to ensure compatibility with a JSON parser.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Save and verify responses:** Generate and store model responses in a JSON file and check if the answers match expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Generate questions and answers, then save to JSON\n",
    "questions = generate_random_math_questions(num_samples=5)\n",
    "answers = []\n",
    "\n",
    "# Generate structured answers for each question\n",
    "for question, num1, num2 in tqdm(questions):\n",
    "    # TODO: Define the message prompts\n",
    "    formatted_messages = ...\n",
    "    \n",
    "    # TODO: Generate the response\n",
    "    outputs = ...\n",
    "    \n",
    "    # Extract the model's JSON output\n",
    "    structured_answer = outputs[0][\"generated_text\"]\n",
    "\n",
    "    answers.append({\n",
    "        \"question\": question,\n",
    "        \"num_1\": num1,\n",
    "        \"num_2\": num2,\n",
    "        \"model_answer\": structured_answer\n",
    "    })\n",
    "\n",
    "# Save answers to a JSON file\n",
    "with open(\"model_answers.json\", \"w\") as f:\n",
    "    json.dump(answers, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to parse model's answer and verify correctness\n",
    "def verify_answer(entry):\n",
    "    try:\n",
    "        # Extract expected values\n",
    "        num1, num2 = entry[\"num_1\"], entry[\"num_2\"]\n",
    "        expected_answer = num1 + num2\n",
    "        \n",
    "        # Extract the assistant's response from the list of messages\n",
    "        assistant_message = next(\n",
    "            (msg[\"content\"] for msg in entry[\"model_answer\"] if msg[\"role\"] == \"assistant\"), None\n",
    "        )\n",
    "        \n",
    "        if assistant_message is None:\n",
    "            raise ValueError(\"Assistant's message not found in model_answer\")\n",
    "        \n",
    "        # Parse model's structured answer from JSON\n",
    "        model_response = json.loads(assistant_message.strip())  # Ensure model_answer is a string\n",
    "        \n",
    "        print(f\"Expected answer: {num1} + {num2} = {expected_answer}\")\n",
    "        print(f\"Model's answer: {model_response['num_1']} + {model_response['num_2']} = {model_response['answer']}\")\n",
    "        \n",
    "        # Check if the values match\n",
    "        if (model_response[\"num_1\"] == num1 and \n",
    "            model_response[\"num_2\"] == num2 and \n",
    "            model_response[\"answer\"] == expected_answer):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except (json.JSONDecodeError, KeyError, TypeError, ValueError) as e:\n",
    "        # Handle cases where parsing fails or keys are missing\n",
    "        print(f\"Error verifying entry: {entry}. Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Load answers from the JSON file and verify\n",
    "try:\n",
    "    with open(\"model_answers.json\", \"r\") as f:\n",
    "        saved_answers = json.load(f)\n",
    "except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "    print(f\"Error loading JSON file: {e}\")\n",
    "    saved_answers = []\n",
    "\n",
    "for i, entry in enumerate(saved_answers, 1):\n",
    "    result = verify_answer(entry)\n",
    "    print(f\"Question {i}:\", \"Correct\" if result else \"Incorrect\", \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
