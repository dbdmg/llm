{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Language Modeling (CLM) - Preprocess, Training and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will explore **Causal Language Modeling (CLM)**, which is a core task in training autoregressive language models like GPT-2. CLM is the process of predicting the next word in a sequence, given the previous words. This type of modeling forms the backbone of text generation tasks, where the model learns to generate coherent text by focusing only on previous tokens in the sequence.\n",
    "\n",
    "The lab is divided into three major sections:\n",
    "1. **Preprocessing**: Preparing a dataset and the labels for the CLM task and tokenizing them.\n",
    "2. **Training**: Fine-tuning a pre-trained language model like GPT-2 on a specific dataset using the CLM task.\n",
    "3. **Inference**: Evaluating the modelâ€™s performance by generating text based on input prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Load the Domain-Specific Dataset**:\n",
    "   - The first step in fine-tuning GPT-2 is to load a dataset that is specific to the domain of interest. In this case, we are using a publicly available **medical dataset** from the **PubMed** collection. PubMed contains a vast number of medical articles, and fine-tuning on such a dataset can help GPT-2 generate more accurate and context-specific medical text.\n",
    "   - The dataset we're using is `\"japhba/pubmed_simple\"`, which is a simplified version of PubMed data. This dataset can be easily accessed using the `datasets` library from Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"japhba/pubmed_simple\", split=\"train\")\n",
    "train_dataset = ds.shuffle(seed=42).select(range(1000))\n",
    "eval_dataset = ds.shuffle(seed=42).select(range(1000, 1500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the contents of any one of the examples in the dataset to understand the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the each entry is a dictionary with two keys:\n",
    "- \"abstract\": The abstract of the medical article\n",
    "- \"country\": The country where the article was published (we will not use this information in this lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Tokenize the Dataset**:\n",
    "   - The next step is to **tokenize** the data so that it can be processed by GPT-2.\n",
    "   - GPT-2 does not natively use a padding token, since it does not require fixed length inputs. For this reason, we will substite it with the EOS token as suggested by transformers library (remember, we are also passing an attention mask, so whatever value is used for padding will be ignored by the model!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tokenize the Dataset using the GPT-2 tokenizer\n",
    "# Hint: Use the `map` method of the dataset object\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = ...\n",
    "tokenizer.pad_token = ...  # Set padding token to EOS token\n",
    "\n",
    "def tokenize_function(samples):\n",
    "    # Tokenize the text column `abstract` in the dataset (set `truncation=True` and `padding=\"max_length\"`)\n",
    "    # NOTE: You can either use tokenzier as a global variable, or\n",
    "    # use a closure to capture the tokenizer variable\n",
    "    return ...\n",
    "\n",
    "# NOTE: you should map the dataset using the tokenize function. \n",
    "# (You can optionally consider using the argument `batched=True` for better performance,\n",
    "# as long as tokenize_function can handle the batch! [note2: the tokenizer can do that!])\n",
    "tokenized_dataset = ...\n",
    "tokenized_dataset_eval = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Add labels to the Dataset for Next Token Prediction**:\n",
    "   - In this step, we will add the **labels** that will be used during the next token prediction task. \n",
    "   - In autoregressive language modeling, the **labels** represent the same sequence as the input, shifted one token to the right. This is because the model is trained to predict the next token in the sequence given the previous tokens.\n",
    "   - The shifting of the tokens is already handled automatically by the `Trainer` class. We just pass an extra attribute named `labels` to the dataset (when this argument is passed to the model, it will know to compute the loss for us!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add labels to the dataset\n",
    "def add_labels(samples):\n",
    "    # NOTE: The labels, in causal modeling, should be the same as the input_ids\n",
    "    samples[\"labels\"] = ...\n",
    "    return samples\n",
    "\n",
    "tokenized_dataset = ...\n",
    "tokenized_dataset_eval = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Fine-Tune the GPT-2 Model**:\n",
    "   - Set up the model and finetune it using the medical dataset. \n",
    "   - The pipeline to be followed is the same that we have already seen in the previous lab (`lab03 - 01-bert`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Training Parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-medical-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=6,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    eval_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    ")\n",
    "\n",
    "# TODO: Initialize GPT-2 Model\n",
    "model = ...\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# TODO: Fine-Tune the Model with the `Trainer` method and pass also the eval_dataset \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset_eval,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"./gpt2-medical-finetuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Compare Text Generation Before and After Fine-Tuning**:\n",
    "   - Generate text using both the original pre-trained GPT-2 model and the fine-tuned model.\n",
    "   - Provide the same input prompt and observe the differences in the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# TODO: Load the pre-trained and fine-tuned GPT-2 models\n",
    "pretrained_model = ...\n",
    "finetuned_model = ...\n",
    "\n",
    "# TODO: Tokenize the prompt\n",
    "tokenizer = ...\n",
    "tokenizer.pad_token = ...\n",
    "\n",
    "prompt = \"The patient presents with chest pain and shortness of breath.\"\n",
    "\n",
    "inputs = ...\n",
    "input_ids = inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate Output of the Model Before Fine-Tuning (use `generate` and then `decode` methods of the model and tokenizer) \n",
    "output_pretrained = ...\n",
    "generated_pretrained = ...\n",
    "\n",
    "print(\"**Before Fine-Tuning (Pre-Trained GPT-2)**:\")\n",
    "print(generated_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate Output of the Model After Fine-Tuning\n",
    "output_finetuned = ...\n",
    "generated_finetuned = ...\n",
    "\n",
    "print(\"**After Fine-Tuning (Fine-Tuned on Medical Dataset)**:\")\n",
    "print(generated_finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Extra stuff!</span>\n",
    "\n",
    "Training the model in this way produces batches with potentially very different lengths. This can be inefficient, as the model will have to pad the sequences to the length of the longest sequence in the batch.\n",
    "\n",
    "To avoid this, we can use a technique called **Dynamic Padding**. This technique groups the sequences in the batch by length and pads them to the length of the longest sequence in each group. This way, the model only has to pad the sequences to the length of the longest sequence in each group, which can significantly reduce the amount of padding required.\n",
    "\n",
    "As a first exercise, quantify the number of pad tokens being used in various situations:\n",
    "1. You pad all batches to the maximum allowed sequence length (1024 for GPT-2, this is what we used so far)\n",
    "2. You pad the entire batch to the length of the longest sequence in the batch (generate the batches by randomly sampling sentences)\n",
    "3. You pad the entire batch to the length of the longest sequence in the batch (generate the batches by placing sentences of similar lengths together)\n",
    "\n",
    "Next, introduce dynamic padding and compare the execution times of the previous execution and the one with dynamic padding.\n",
    "\n",
    "You can use the following resources to help you with this exercise:\n",
    "- `group_by_length` parameter ([TrainingArguments](https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.TrainingArguments.group_by_length)) (parameter to group together samples with similar lengths)\n",
    "- `DataCollatorForSeq2Seq` ([DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForSeq2Seq)) (collator function that aggregates samples into batches and pads them to the maximum length of the batch)\n",
    "\n",
    "Note: you may find that the validation losses you observe may be different from the previous ones. This is because the cross entropy loss is computed as an average across tokens, and the number of tokens in a batch can vary depending on the padding strategy used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
