{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Large Language Models: LLaMA and Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will dive into two famous large language models, LLaMA and Mistral, along with their instruction-tuned versions. We'll explore how each model performs on various tasks, with a particular focus on generating structured responses in JSON format.\n",
    "\n",
    "These models have been fine-tuned to follow instructions, making them suitable for a range of NLP applications. Through this lab, you will:\n",
    "\n",
    "- Learn how to load and interact with LLaMA and Mistral models using the `pipeline` and `chat_template` functions.\n",
    "- Examine the performance of their instruction-based variants.\n",
    "- Generate structured outputs, specifically in JSON, for practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Disclaimer**: Before starting this lab, ensure you have requested access to the required models on Hugging Face and have logged in to your Hugging Face account. Access is necessary for the following models:\n",
    ">\n",
    "> - [Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)\n",
    "> - [Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)\n",
    "> - [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    ">\n",
    "> You can log in to Hugging Face directly from this notebook using the provided code snippet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csavelli/llm/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /data1/hf_cache/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Login to the Hugging Face model hub to be able to upload models\n",
    "with open(\"../hf_token.txt\", \"r\") as f:\n",
    "    token = f.read()\n",
    "    f.close()\n",
    "\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lab, we will explore **LLaMA (Large Language Model Meta AI)**, which is one of the most known large language models developed by Meta (Facebook). \n",
    "\n",
    "Next, we will focus on **Instruction LLaMA**, a version of LLaMA fine-tuned to better understand and follow user instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Llama 3.2 (released in September 2024). In particular, we will adopt the 1B version. On the scale of things, this model is on the smaller side, but it is still a very powerful model.\n",
    "It has been released (along with a 3B version) with the intention of allowing running it on devices with modest hardware (e.g., mobile phones or other edge devices). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this model to generate text using the generate() method. We use random sampling (`do_sample=True`) and extract 5 samples (`num_return_sequences=5`). You can find other generation parameters [here](https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/text_generation#transformers.GenerationConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>Hello, my name is Dr. Steve Schrader, and I’m a psychologist. I’m also the co-founder of the Center for Advanced Psychological Studies. I’ve been doing this for over 30 years. I’m a licensed psychologist in',\n",
       " \"<|begin_of_text|>Hello, my name is Janelle and I'm a dog lover. I am a certified dog trainer and owner of The Dog Whisperer, a dog training and behavior consulting company based in the Greater Los Angeles area. I have been training and\",\n",
       " '<|begin_of_text|>Hello, my name is Marisa and I’m a new blogger! I am a 23 year old girl who lives in the beautiful city of New York. I am a college student studying English and I am currently a freshman at Hunter College.',\n",
       " '<|begin_of_text|>Hello, my name is Kaitlyn and I am a 2nd year student at the University of Manitoba, studying the Bachelor of Science in Kinesiology and Exercise Science. I am a member of the Kinesiology and Exercise Science Student',\n",
       " '<|begin_of_text|>Hello, my name is Chris and I’m a graphic designer and web developer. I’m based in the beautiful city of Bristol, UK, and I’m passionate about creating designs that are both functional and visually appealing.\\nI’ve been working in the']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n",
    "batch = model.generate(**tokens, do_sample=True, max_length=50, num_return_sequences=5, pad_token_id=tokenizer.eos_token_id) # (assigning pad_token_id avoids a warning)\n",
    "tokenizer.batch_decode(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>what is 2+2? (Math)\\nWhat is 2+2? (Math)\\nI have been asked this question a few times, so I decided to make a video about it. I hope it helps you!\\nIn this video',\n",
       " '<|begin_of_text|>what is 2+2? 4\\nWhat is 2+2?\\nWhat is 2+2? 4\\n2+2=4 is the answer to the question What is 2+2? 4. This',\n",
       " '<|begin_of_text|>what is 2+2? What is 2+2?\\nWhat is 2+2? The answer is 4. In this article, you will learn what is 2+2? How to calculate 2+2?',\n",
       " '<|begin_of_text|>what is 2+2? - 2 + 2 = 4\\n2 + 2 = 4\\nThe two numbers that you are adding are 2 and 2. The sum of the two numbers is 4.\\n',\n",
       " '<|begin_of_text|>what is 2+2??\\n  1. What is the difference between 2+2 and 2+2+2+2+2+2+2+2+2+2+2+2+2+']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\"what is 2+2?\", return_tensors=\"pt\").to(model.device)\n",
    "batch = model.generate(**tokens, do_sample=True, max_length=50, num_return_sequences=5, pad_token_id=tokenizer.eos_token_id) # (assigning pad_token_id avoids a warning)\n",
    "tokenizer.batch_decode(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding the `tokenizer.chat_template`**\n",
    "\n",
    "In this section, we will explore the **chat template** that is used to format and structure messages for a conversational assistant. The `tokenizer.chat_template` is a convenient way for organizing interactions between the user, system, and assistant in a way that the model can easily process and generate coherent responses.\n",
    "\n",
    "### **What is a Chat Template?**\n",
    "\n",
    "The chat template is a predefined format that ensures consistent structure for conversations. It marks the different roles in the interaction (system, user, assistant), and separates the various elements of the conversation using special tokens. This helps the language model understand which parts of the dialogue are instructions, which parts are user inputs, and where the assistant’s response should be generated.\n",
    "\n",
    "Let's create an example of a possible (simplified) chat template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "chat_template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: \"\"\"+datetime.datetime.now().strftime(\"%d %b %Y\")+\"\"\"\n",
    "\n",
    "{system_message}\n",
    "\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_message}\n",
    "\n",
    "<|eot_id|>\n",
    "\"\"\" # then role assistant follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hugging Face Pipeline Overview**\n",
    "\n",
    "The **`pipeline`** method from Hugging Face’s Transformers library is a high-level API designed to streamline the process of using pre-trained models for a wide variety of **natural language processing (NLP) tasks**.\n",
    "\n",
    "#### **What is a Pipeline?**\n",
    "\n",
    "A pipeline is a modular tool that wraps around a pre-trained model, tokenizer, and task-specific configurations. It makes it easy to load and apply these models directly to different tasks, such as:\n",
    "- **Text generation**\n",
    "- **Text classification**\n",
    "- **Question answering**\n",
    "- **Summarization**\n",
    "- **Translation**\n",
    "\n",
    "By simply specifying the type of task (e.g., `\"text-generation\"`), `pipeline` takes care of loading and configuring a compatible model and tokenizer, providing a ready-to-use interface for generating results.\n",
    "\n",
    "You can find a full list of supported pipelines on the [Hugging Face documentation](https://huggingface.co/docs/transformers/main_classes/pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline with the model and tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 27 Oct 2025\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!\n",
      "\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      "<|eot_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
    "]\n",
    "\n",
    "# Format the messages using the chat template\n",
    "formatted_messages = chat_template.format(\n",
    "    system_message=messages[0][\"content\"],\n",
    "    user_message=messages[1][\"content\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(formatted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember that for models to follow instruction tuning, they need to have been tuned on this kind of data. In this case, we are not using the instruction-tuned version. \n",
    "\n",
    "So, we can expect the model to produce a garbage response (it has never seen that kind of inputs before!). But let's try it anyway!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 27 Oct 2025\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!\n",
      "\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      "<|eot_id|>\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "riteln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "writeln\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the output text \n",
    "outputs = pipe(\n",
    "    formatted_messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Differences Between Standard and Instruct Versions of Large Language Models (LLMs)**\n",
    "\n",
    "Large Language Models (LLMs) come in different versions, with **standard** and **instruction-tuned (Instruct)** versions being the most common. Here’s a brief comparison:\n",
    "\n",
    "#### **1. Purpose and Training**:\n",
    "   - **Standard LLM**: The standard model is generally pre-trained on large datasets without specific instruction-following capabilities. Typically generates more open-ended responses, which can be useful for creative writing or general information retrieval where the response style is flexible.\n",
    "   - **Instruct LLM**: Instruction-tuned models, like the **Llama-3.2 Instruct**, are fine-tuned on datasets designed to help the model understand and follow instructions effectively. This tuning enhances the model's ability to respond directly to user prompts and handle structured requests. It is fine-tuned to produce concise, direct responses that are often more relevant in task-specific or conversational AI applications.\n",
    "\n",
    "Let's compare the outputs of the standard and Instruct versions of LLaMA to see the differences in their responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 27 Oct 2025\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!\n",
      "\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      "<|eot_id|>\n",
      "Arrr, ye landlubber! Ye be lookin' fer a mathematical answer, eh? Alright then, let's hoist the sails and set course fer a simple calculation, me hearty!\n",
      "\n",
      "2 + 2 be equal to... *taps hook*...4, matey!\n",
      "\n",
      "Now, what be yer next question, matey?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = chat_template\n",
    "\n",
    "# Create the pipeline with the model and tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
    "]\n",
    "\n",
    "# Format the messages using the chat template\n",
    "formatted_messages = chat_template.format(\n",
    "    system_message=messages[0][\"content\"],\n",
    "    user_message=messages[1][\"content\"]\n",
    ")\n",
    "\n",
    "outputs = pipe(\n",
    "    formatted_messages,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation of the Tokenizer Chat Template**\n",
    "\n",
    "Actually, the chat template of `meta-llama/Llama-3.2-1B-Instruct` is much more complex than the example above. It includes various components that help the model understand the context of the conversation, manage dates, handle tools, and structure messages effectively.\n",
    "\n",
    "The template is written in [jinja](https://jinja.palletsprojects.com/en/stable/templates/), a language that allows for the dynamic generation of content based on variables, conditions and loops.\n",
    "\n",
    "\n",
    "Let's print it and analyze its key components:\n",
    " \n",
    "\n",
    "#### **Key Components of the Template**:\n",
    "1. **System Message Extraction**:\n",
    "   - The system message is extracted if the first role in the message list is labeled \"system.\" This allows the template to clearly differentiate between user queries and system instructions.\n",
    "   - If a system message exists, it is added to the template between special tokens (`<|start_header_id|>` and `<|end_header_id|>`), ensuring that the model knows when the system message starts and ends.\n",
    "\n",
    "2. **Date Management**:\n",
    "   - The template automatically handles the current date using either a provided `strftime_now` function or a default date (`\"26 Jul 2024\"`). This can be useful when the model needs to be aware of the date in contexts such as time-sensitive responses.\n",
    "\n",
    "3. **Handling Tools**:\n",
    "   - The template checks if **tools** are defined. If tools are available, it includes a description of these tools in the system message or the user message, depending on where they need to appear.\n",
    "   - If the tools are part of the user message, the template ensures that the first user message prompts the user to respond in a structured format, such as using JSON for function calls.\n",
    "\n",
    "4. **Message Processing**:\n",
    "   - The template loops through the list of messages and processes each based on the role (`user`, `assistant`, `ipython`, or `tool`). It formats each message using start and end tokens for the roles, helping the model understand the structure of the conversation.\n",
    "   - If the message involves tool calls, the template ensures that they are properly formatted into a structured JSON format to be passed back to the model for further processing.\n",
    "\n",
    "5. **Ending the Assistant's Response**:\n",
    "   - The template leaves a placeholder for the assistant’s response, which the model will generate during inference. This ensures that the assistant's response begins in the correct format, ready to be populated with the generated content.\n",
    "\n",
    "#### **Why Is This Template Needed?**\n",
    "\n",
    "- **Maintains Consistency**: This template ensures that the conversation is structured in a consistent manner, which is crucial for models designed to follow complex instructions or engage in multi-turn conversations.\n",
    "- **Handles Tools**: By incorporating the ability to dynamically introduce tools and functionality, the template allows the model to expand beyond simple text-based conversations and perform function-based tasks.\n",
    "- **Structured Outputs for Tools**: When the conversation involves tool calls (e.g., through APIs or function calls), the template ensures that these interactions are formatted properly for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate again the same example using the `chat_template` of `meta-llama/Llama-3.2-1B-Instruct` and analyze the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a tokenizer that supports the chat template, we can directly call the `apply_chat_template()` method to convert a list of messages (each one a dictionary in the already discussed format) into a prompt.\n",
    "\n",
    "Notice that, since we are not using any particular tools or other functionalities, our template will be similar to the one we manually introduced earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 27 Oct 2025\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "input_tokens = tokenizer.apply_chat_template(messages)\n",
    "print(tokenizer.decode(input_tokens))\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': [{'role': 'system',\n",
       "    'content': 'You are a pirate chatbot who always responds in pirate speak!'},\n",
       "   {'role': 'user', 'content': 'Who are you?'},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"Arrrr, me hearty! I be Captain Cutlass, the greatest pirate chatbot to ever sail the seven seas... er, I mean, the internet! Me and me trusty crew o' bots be here to swab the decks o' knowledge and answer all yer questions, savvy? Whether ye be lookin' fer treasure maps, pirate tales, or just a bit o' fun, I be here to help ye navigate the choppy waters o' information. So hoist the sails and set course fer a swashbucklin' good time, me hearty!\"}]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, ye be askin' who I be? I be the great Pirate Chatbot, here to swab ye away with me finest pirate slang and knowledge! Me be the master o' the seven seas o' language, with a treasure trove o' pirate tales and trivia at me disposal. I be here to help ye navigate the high seas o' knowledge, from the history o' piracy to the latest scurvy-fightin' gadgets. So hoist the sails and set course for adventure, me hearty!\n"
     ]
    }
   ],
   "source": [
    "# we are getting back the full conversation history\n",
    "# as a list of messages outputs[0][\"generated_text\"]\n",
    "# -1 : last message (assistant response)\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the pipeline already supports chat mode, so we can pass the list of messages (as long as they contain role/content keys) directly to the pipeline.\n",
    "\n",
    "Alternatively, we could have passed the prompt as a string. In this case, however, we would have to manually extract the output from the model and parse it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Oct 2025\\n\\nYou are a pirate chatbot who always responds in pirate speak!\\n\\n<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\nWhat is 2 + 2?\\n\\n<|eot_id|>\\nArrrr, ye landlubbers be wantin' to know the answer, eh? Alright then, matey! Me calculator be tellin' me that 2 + 2 be 4. But I be thinkin' ye be just tryin' to confuse me with yer landlubber ways, savvy?\\n\\nBut I be willin' to give ye credit where credit be due, matey. Ye be a clever one. Now, what be the next question ye be wantin'?\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will explore the use of `Mistral-7B-Instruct-v0.2`developed by Mistral AI to generate structured responses in JSON format. \n",
    "\n",
    "In this exercise, we will generate random math questions and instruct Mistral-7B to respond in a structured JSON format. We will then save the responses to a JSON file and verify the answers programmatically. \n",
    "\n",
    "Let's first repeat the same example we did with LLaMA, but now using Mistral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csavelli/llm/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.36it/s]\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arr, I be Cap'n Parrotbeak, me hearty scallywag! How be thar, landlubber?\n",
      "\n",
      "What brings ye to me virtual hideout? Come closer, but mind the plank!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Define the model ID\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "mistral_chat_template = tokenizer.chat_template\n",
    "\n",
    "# Initialize the pipeline for text generation\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,  # Optimizes memory usage\n",
    "    device_map=\"auto\"            # Automatically distributes the model across available devices\n",
    ")\n",
    "\n",
    "# Define the message prompts for the conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
    "]\n",
    "\n",
    "# Generate the response\n",
    "outputs = pipe(messages, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Print the model's generated response\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Generate random math questions:** Use Python to create questions with random numbers in a conversational style (e.g., “What is the sum of 245 and 173?”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_math_questions(num_samples=5):\n",
    "    templates = [\n",
    "        \"What is the sum of {} and {}?\",\n",
    "        \"Can you add {} and {}?\",\n",
    "        \"Calculate the sum of {} and {} for me.\",\n",
    "        \"How much is {} plus {}?\",\n",
    "        \"Please add {} and {}.\"\n",
    "    ]\n",
    "    \n",
    "    questions = []\n",
    "    for _ in range(num_samples):\n",
    "        template = random.choice(templates)\n",
    "        num1 = random.randint(10, 999)\n",
    "        num2 = random.randint(10, 999)\n",
    "        question = template.format(num1, num2)\n",
    "        questions.append((question, num1, num2))  # store question with numbers for validation\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Instruct the model to respond in JSON:** Use a system role instruction to ensure Mistral-7B answers in a JSON format containing the fields `num_1`, `num_2`, and `answer`. This makes the output compatible with automated processing or JSON parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_instruction = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"Answer each question in JSON format with the fields 'num_1', 'num_2', and 'answer'. Provide only JSON to ensure compatibility with a JSON parser.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Save and verify responses:** Generate and store model responses in a JSON file and check if the answers match expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is the sum of 293 and 89?', 293, 89),\n",
       " ('Can you add 23 and 244?', 23, 244),\n",
       " ('How much is 918 plus 536?', 918, 536),\n",
       " ('Please add 952 and 95.', 952, 95),\n",
       " ('Calculate the sum of 710 and 574 for me.', 710, 574)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Generate questions and answers, then save to JSON\n",
    "questions = generate_random_math_questions(num_samples=5)\n",
    "answers = []\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"Answer each question in JSON format with the fields 'num_1', 'num_2', and 'answer'. Provide only JSON to ensure compatibility with a JSON parser.\"}, {'role': 'user', 'content': 'What is the sum of 293 and 89?'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:01<00:05,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"Answer each question in JSON format with the fields 'num_1', 'num_2', and 'answer'. Provide only JSON to ensure compatibility with a JSON parser.\"}, {'role': 'user', 'content': 'Can you add 23 and 244?'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:02<00:03,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"Answer each question in JSON format with the fields 'num_1', 'num_2', and 'answer'. Provide only JSON to ensure compatibility with a JSON parser.\"}, {'role': 'user', 'content': 'How much is 918 plus 536?'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:03<00:02,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"Answer each question in JSON format with the fields 'num_1', 'num_2', and 'answer'. Provide only JSON to ensure compatibility with a JSON parser.\"}, {'role': 'user', 'content': 'Please add 952 and 95.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:05<00:01,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"Answer each question in JSON format with the fields 'num_1', 'num_2', and 'answer'. Provide only JSON to ensure compatibility with a JSON parser.\"}, {'role': 'user', 'content': 'Calculate the sum of 710 and 574 for me.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:07<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate structured answers for each question\n",
    "for question, num1, num2 in tqdm(questions):\n",
    "\n",
    "    # Define the message prompts\n",
    "    formatted_messages = [\n",
    "        role_instruction,\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    print(formatted_messages)\n",
    "\n",
    "    # Generate the response\n",
    "    outputs = pipe(formatted_messages, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Extract the model's JSON output\n",
    "    structured_answer = outputs[0][\"generated_text\"]\n",
    "\n",
    "    answers.append({\n",
    "        \"question\": question,\n",
    "        \"num_1\": num1,\n",
    "        \"num_2\": num2,\n",
    "        \"model_answer\": structured_answer\n",
    "    })\n",
    "\n",
    "# Save answers to a JSON file\n",
    "with open(\"model_answers.json\", \"w\") as f:\n",
    "    json.dump(answers, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected answer: 293 + 89 = 382\n",
      "Model's answer: 293 + 89 = 382\n",
      "Question 1: Correct \n",
      "\n",
      "Expected answer: 23 + 244 = 267\n",
      "Model's answer: 23 + 244 = 265\n",
      "Question 2: Incorrect \n",
      "\n",
      "Expected answer: 918 + 536 = 1454\n",
      "Model's answer: 918 + 536 = 1454\n",
      "Question 3: Correct \n",
      "\n",
      "Expected answer: 952 + 95 = 1047\n",
      "Model's answer: 952 + 95 = 1047\n",
      "Question 4: Correct \n",
      "\n",
      "Expected answer: 710 + 574 = 1284\n",
      "Model's answer: 710 + 574 = 1284\n",
      "Question 5: Correct \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to parse model's answer and verify correctness\n",
    "def verify_answer(entry):\n",
    "    try:\n",
    "        # Extract expected values\n",
    "        num1, num2 = entry[\"num_1\"], entry[\"num_2\"]\n",
    "        expected_answer = num1 + num2\n",
    "        \n",
    "        # Extract the assistant's response from the list of messages\n",
    "        assistant_message = next(\n",
    "            (msg[\"content\"] for msg in entry[\"model_answer\"] if msg[\"role\"] == \"assistant\"), None\n",
    "        )\n",
    "        \n",
    "        if assistant_message is None:\n",
    "            raise ValueError(\"Assistant's message not found in model_answer\")\n",
    "        \n",
    "        # Parse model's structured answer from JSON\n",
    "        model_response = json.loads(assistant_message.strip())  # Ensure model_answer is a string\n",
    "        \n",
    "        print(f\"Expected answer: {num1} + {num2} = {expected_answer}\")\n",
    "        print(f\"Model's answer: {model_response['num_1']} + {model_response['num_2']} = {model_response['answer']}\")\n",
    "        \n",
    "        # Check if the values match\n",
    "        if (model_response[\"num_1\"] == num1 and \n",
    "            model_response[\"num_2\"] == num2 and \n",
    "            model_response[\"answer\"] == expected_answer):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except (json.JSONDecodeError, KeyError, TypeError, ValueError) as e:\n",
    "        # Handle cases where parsing fails or keys are missing\n",
    "        print(f\"Error verifying entry: {entry}. Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Load answers from the JSON file and verify\n",
    "try:\n",
    "    with open(\"model_answers.json\", \"r\") as f:\n",
    "        saved_answers = json.load(f)\n",
    "except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "    print(f\"Error loading JSON file: {e}\")\n",
    "    saved_answers = []\n",
    "\n",
    "for i, entry in enumerate(saved_answers, 1):\n",
    "    result = verify_answer(entry)\n",
    "    print(f\"Question {i}:\", \"Correct\" if result else \"Incorrect\", \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
