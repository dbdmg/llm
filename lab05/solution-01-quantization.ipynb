{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization in Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to Quantization**\n",
    "\n",
    "Quantization is a process used to reduce the memory requirements and computational complexity of large machine learning models. By representing model parameters with lower-precision values, quantization makes it possible to run models more efficiently on devices with limited memory and computational resources.\n",
    "\n",
    "For large language models (LLMs), quantization can:\n",
    "- **Reduce Memory Usage:** Lower-precision data types (such as int8) use less memory than higher-precision types (like float32), allowing models to fit into memory-constrained environments.\n",
    "- **Improve Inference Speed:** By using simpler operations on smaller data types, quantization can reduce the time it takes for a model to process inputs and generate outputs.\n",
    "- **Preserve Accuracy:** Quantization is carefully designed to minimize the impact on model accuracy, though a trade-off often exists between precision and efficiency.\n",
    "\n",
    "We will focus on Post-Training Quantization (PTQ), a quantization technique that applies quantization to a pre-trained model. PTQ is a popular method for quantizing large language models because it can be applied to a wide range of models that we may want to use in inference mode. By contrast, quantization-aware training (QAT) requires retraining the model with quantization in mind, which can be more complex and time-consuming.\n",
    "\n",
    "We will first get a general understanding of quantization by manually implementing two commonly adopted approaches: absmax and minmax (or zero-point). \n",
    "\n",
    "Next, we will explore two different ways (one using PyTorch, the other using HuggingFace) to run a **dynamic quantization** (i.e., PTQ where only the weights are quantized, and not the activations). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Absmax and Minmax Quantization\n",
    "\n",
    "The goal of quantization is, remember, mapping continuos values (e.g., float32) into a discrete set of values (e.g., int8). \n",
    "\n",
    "So let's create a matrix $W$ and a vector $x$ to be quantized. Let's initialize them randomly (but, just to see what happens, let's set W[0,0] and x[0] = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "n_rows = 3\n",
    "n_cols = 5\n",
    "W = torch.randn(n_rows, n_cols)\n",
    "x = torch.randn(n_cols)\n",
    "\n",
    "W[0,0] = 0\n",
    "x[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first compute the matrix multiplication to observe the result. This is the operation that we typically want to execute, and that we want to quantize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will quantize $W$ and $x$ separately, and then multiply them together. Finally, we will need to dequantize the result to compare it with the original result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = W @ x\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Absmax quantization\n",
    "\n",
    "In absmax quantization, we use a symmetric range around 0. This means that we need to identify the maximum absolute value in the matrix $W$ and the vector $x$.\n",
    "\n",
    "We define a function `absmax_quantize` that takes as input any tensor and produces a version of the same tensor, but quantized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_quantize(W):\n",
    "    # NOTE: we assume that we always map to 8-bit integers\n",
    "    max_value = W.abs().max()\n",
    "    scale = 127 / max_value # how \"long\" the step between any two int8 values is\n",
    "    W_q = (W * scale).round().to(torch.int8)\n",
    "    return W_q, scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, we return both the quantized tensor and the scale factor. The scale factor is used to dequantize the tensor. So we might as well define a dequantize function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_dequantize(W_q, scale):\n",
    "    return W_q.float() / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the quantized version of W, and of x. Then, we can check how much we are losing by quantizing the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q, scale_W = absmax_quantize(W)\n",
    "x_q, scale_x = absmax_quantize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(W_q)\n",
    "print(W)\n",
    "W_deq = absmax_dequantize(W_q, scale_W)\n",
    "print(W_deq)\n",
    "print((W - W_deq).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_q)\n",
    "print(x)\n",
    "x_deq = absmax_dequantize(x_q, scale_x)\n",
    "print(x_deq)\n",
    "print((x - x_deq).abs().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, in both cases, absmax maps the value 0 to 0. This is a good property, as it allows us to represent the zero value without losing any information. This property stems from the symmetry around 0 we imposed.\n",
    "\n",
    "However, do note that we are also \"wasting\" some bits of the range! Can you spot where?\n",
    "\n",
    "Let's now compute the matrix multiplication between W_q and x_q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q @ x_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see that there's something wrong? Let's see what one of the rows of W_q and x_q contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q[0], x_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(127, dtype=torch.int8)\n",
    "b = torch.tensor(1, dtype=torch.int8)\n",
    "c = a + b   \n",
    "print(c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product of these two vectors definitely isn't what we get as the first number of the matrix multiplication -- i.e. (W_q @ x_q)[0]. Indeed, we can run as int16, and see that the result is quite different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q.to(torch.int16) @ x_q.to(torch.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the dot product overflows the int8 range. This is a well-known problem. Indeed, the accumulation of results, in quantization, is typically done with higher precision than the single values. This is tricky to do in pure Python/PyTorch, but can be done efficiently in other ways.\n",
    "\n",
    "Let's stick to the simple approach for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_q = W_q.to(torch.int16) @ x_q.to(torch.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the correct result, we need to dequantize the result. This is done by multiplying the result by the scale factor of the two operands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_deq = absmax_dequantize(absmax_dequantize(out_q, scale_W), scale_x)\n",
    "# Or, alternatively:\n",
    "# out_deq = out_q / scale_W / scale_x\n",
    "# out_deq = absmax_dequantize(out_q, scale_W * scale_x)\n",
    "print(out_deq)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, our goal was `out`. How much did we lose by quantizing and dequantizing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(out_deq - out).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Minmax Quantization\n",
    "\n",
    "In minmax quantization, we use the minimum and maximum values in the matrix $W$ and the vector $x$ to define the range. In this way, we get a range that is as tight as possible around the values we are quantizing. This will, however, change the zero value, which will not be mapped to 0 anymore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_quantize(W):\n",
    "    # the following notations come from:\n",
    "    # (1) scaling W to [0,1] ==> W' = (W - min(W)) / (max(W) - min(W)),\n",
    "    # (2) scaling W' to [-128, 127] ==> W_q = W' * 255 - 128\n",
    "    # by combining the two, we get that:\n",
    "    # W_q = W * scale + offset\n",
    "    # we will call the offset \"zero_point\", as it represent the value that maps to 0\n",
    "    delta = W.max() - W.min()\n",
    "    scale = 255 / delta\n",
    "    zero_point = -(128*W.max() + 127*W.min()) / delta\n",
    "    W_q = (W * scale + zero_point).round().to(torch.int8)\n",
    "    return W_q, scale, zero_point\n",
    "\n",
    "def minmax_dequantize(W_q, scale, zero_point):\n",
    "    return (W_q.float() - zero_point) / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q, scale_W, zero_point_W = minmax_quantize(W)\n",
    "x_q, scale_x, zero_point_x = minmax_quantize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results for $W$ (same considerations will apply for $x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W_q)\n",
    "print(W)\n",
    "W_deq = minmax_dequantize(W_q, scale_W, zero_point_W)\n",
    "print(W_deq)\n",
    "print((W - W_deq).abs().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, notice that 0 no longer maps to 0! Indeed, it maps to zero_point_W (after rounding). This implies that the dequantization of 0 will no longer be 0. This may be a problem!\n",
    "\n",
    "But, notice that we are using the full range of the int8 values. This means that we are not wasting any bits of the range! (the minimum value is -128, the maximum value is 127). This can also be seen in the average absolute error, which is lower than what we had with absmax.\n",
    "\n",
    "Similarly to what we did before, let's compute the output of the operation, and then dequantize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_q = W_q.to(torch.int16) @ x_q.to(torch.int16)\n",
    "print(out_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dequantification is a bit trickier, in this case. Can you figure out why we need the following operations?\n",
    "\n",
    "Hint: consider the transformation we are applying to each value (value * scale + zero_point). What happens when we compute the dot product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_deq = (out_q - W.shape[1] * zero_point_W * zero_point_x - W.sum(axis=1) * scale_W * zero_point_x - x.sum() * scale_x * zero_point_W) / (scale_W * scale_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_deq)\n",
    "print(out)\n",
    "print((out_deq - out).abs().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Extra stuff!</span>\n",
    "\n",
    "We could have computed scales and zero points at different granularities (e.g., for each row, or column of $W$). How would that have changed the results? What changes would we have to do to the code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part, we will apply dynamic quantization by using PyTorch or HuggingFace (with BitsAndBytes). We will quantize both to 8 and to 4 bits, and we will see how that affects LLMs (in terms of memory and speed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to the Hugging Face model hub to be able to upload models\n",
    "with open(\"../hf_token.txt\", \"r\") as f:\n",
    "    token = f.read()\n",
    "    f.close()\n",
    "\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load our model (Llama 3.2 1B) and let's see some base statistics (memory usage, inference time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    \"\"\"Get the size of the model in MB\"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.pth\")\n",
    "    size = os.path.getsize(\"temp.pth\") / 1e6  # size in \"MB\" (technically, it should be 1024**2, but we approximate to 1e6 to get an easier conversion #params <=> MB)\n",
    "    os.remove(\"temp.pth\")\n",
    "    return size\n",
    "\n",
    "print(f\"Model size before quantization {(get_model_size(model)):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, wasn't Llama 1B supposed to be 4GB (4 bytes * 1B parameters)? Why do we get ~ 5 GB (i.e., 1.25B parameters)?\n",
    "\n",
    "We are not considering the parameters used in the embedding layer (you can count how many parameters you have in the embedding layer and see that it matches the difference). \n",
    "\n",
    "Additionally, the count does not include the `lm_head`, i.e. the layer used to go from the hidden states to the logits. This is because in Llama (and other models) the `lm_head` is shared with the embedding layer, so it is not counted twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The secret of life is\"\n",
    "# Notice we use a batch of 20 sentences -- we will get better results\n",
    "# on quantized models when processing a batch of inputs\n",
    "inputs = tokenizer([text]*20, return_tensors=\"pt\")\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    baseline_output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "elapsed_time = time.time() - tic\n",
    "\n",
    "baseline_decoded = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nBaseline model output:\", baseline_decoded)\n",
    "print(\"\\nTime taken for baseline model:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic quantization applies lower precision to model weights and activations at runtime. This method doesn’t require modifications to the model architecture or retraining, which makes it relatively easy to apply.\n",
    "\n",
    "- **Advantages:** \n",
    "  - Quick to implement with minimal changes. No calibration step is needed.\n",
    "\n",
    "- **Limitations:** \n",
    "  - Activations are not pre-quantized, meaning some precision is maintained but at the cost of slightly higher resource use at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We use **[TorchAO](https://docs.pytorch.org/ao/stable/api_ref_quantization.html?utm_source=chatgpt.com)** to apply dynamic quantization to a model. TorchAO is the new quantization framework that replaces the deprecated `torch.ao.quantization` APIs.  \n",
    "\n",
    "We can specify a set of layer types to be quantize. Let's stick with Linear layers. We specify the desired type, and off we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchao.quantization import quantize_, Int8DynamicActivationInt8WeightConfig\n",
    "\n",
    "quantize_(\n",
    "    model,\n",
    "    Int8DynamicActivationInt8WeightConfig(),\n",
    "    filter_fn=lambda m, name: isinstance(m, torch.nn.Linear),\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "print(f\"Model size after quantization {get_model_size(model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay -- 2.3GB? Why not 5GB / 4 = 1.25GB? After all, we are going from float32 to int8. \n",
    "\n",
    "That's correct -- technically. Except, we are only encoding linear layers, and not the embedding layer. That means that, of the original 1.25B parameters, we are only quantizing 1B. The rest, in the embedding layer, is kept as float32.\n",
    "\n",
    "If you run the numbers, though, you should still find a problem: 1B * 1 byte + 0.25B * 4 bytes = 2GB. What about the rest? There's one more thing: remember, the `lm_head` was shared with the Embedding layer. However, since it is \"copied\" into a linear layer in Llama, the quantization process will quantize it as well. So that's an extra 0.25B parameters encoded as int8 -- hence 2.3GB.\n",
    "\n",
    "Finally, we could technically also quantize the embeddings (it has been introduced in later versions of PyTorch), but for simplicity we will not do it here (it would require some additional steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "elapsed_time = time.time() - tic\n",
    "\n",
    "output_decoded = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nQuantized model output:\", output_decoded)\n",
    "print(\"\\nTime taken for baseline model:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face provides several built-in quantization options, each suited to different model and deployment needs:\n",
    "https://huggingface.co/docs/transformers/v4.46.0/quantization/overview\n",
    "\n",
    "For this lab, we will use `Quanto`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
    "import torch\n",
    "    \n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Quantize to 8-bit weights\n",
    "quant = QuantoConfig(weights=\"int8\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant,\n",
    "    device_map=\"cpu\",      \n",
    ")\n",
    "\n",
    "print(f\"Model size after quantization: {get_model_size(model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "text = \"The secret of life is\"\n",
    "inputs = tokenizer([text]*20, return_tensors=\"pt\")\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "elapsed_time = time.time() - tic\n",
    "\n",
    "output_decoded = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nquantized model output:\", output_decoded)\n",
    "print(\"\\nTime taken for baseline model:\", elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
