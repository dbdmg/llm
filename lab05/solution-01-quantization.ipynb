{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization in Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to Quantization**\n",
    "\n",
    "Quantization is a process used to reduce the memory requirements and computational complexity of large machine learning models. By representing model parameters with lower-precision values, quantization makes it possible to run models more efficiently on devices with limited memory and computational resources.\n",
    "\n",
    "For large language models (LLMs), quantization can:\n",
    "- **Reduce Memory Usage:** Lower-precision data types (such as int8) use less memory than higher-precision types (like float32), allowing models to fit into memory-constrained environments.\n",
    "- **Improve Inference Speed:** By using simpler operations on smaller data types, quantization can reduce the time it takes for a model to process inputs and generate outputs.\n",
    "- **Preserve Accuracy:** Quantization is carefully designed to minimize the impact on model accuracy, though a trade-off often exists between precision and efficiency.\n",
    "\n",
    "In this lab, we will explore and compare two common types of quantization: **dynamic quantization** and **static quantization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to the Hugging Face model hub to be able to upload models\n",
    "with open(\"../hf_token.txt\", \"r\") as f:\n",
    "    token = f.read()\n",
    "    f.close()\n",
    "\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GPT-2 model\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp.pth\")\n",
    "    size = os.path.getsize(\"temp.pth\") / 1e6  # size in MB\n",
    "    os.remove(\"temp.pth\")\n",
    "    return size\n",
    "\n",
    "print(f\"Model size before quantization: {get_model_size(model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The secret of life is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    baseline_output = model.generate(inputs['input_ids'], max_length=50)\n",
    "\n",
    "elapsed_time = time.time() - tic\n",
    "\n",
    "baseline_decoded = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nBaseline model output:\", baseline_decoded)\n",
    "print(\"\\nTime taken for baseline model:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dynamic Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic quantization applies lower precision to model weights and activations at runtime. This method doesn’t require modifications to the model architecture or retraining, which makes it relatively easy to apply.\n",
    "\n",
    "- **Advantages:** \n",
    "  - Quick to implement with minimal changes. No calibration step is needed.\n",
    "\n",
    "- **Limitations:** \n",
    "  - Activations are not pre-quantized, meaning some precision is maintained but at the cost of slightly higher resource use at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import prepare, convert, get_default_qconfig\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ").to('cpu')\n",
    "\n",
    "# Model size after quantization\n",
    "print(f\"Model size after quantization: {get_model_size(quantized_model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = quantized_model.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "elapsed_time = time.time() - tic\n",
    "\n",
    "output_decoded = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nQuantized model output:\", output_decoded)\n",
    "print(\"\\nTime taken for baseline model:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/v4.46.0/quantization/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import QuantoConfig\n",
    "\n",
    "quantization_config = QuantoConfig(weights=\"int8\",  modules_to_not_convert=[\"lm_head\"])\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "\n",
    "print(f\"Model size after quantization: {get_model_size(quantized_model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = quantized_model.generate(inputs['input_ids'], max_length=50)\n",
    "\n",
    "elapsed_time = time.time() - tic\n",
    "\n",
    "output_decoded = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nquantized model output:\", output_decoded)\n",
    "print(\"\\nTime taken for baseline model:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Static Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static quantization, on the other hand, involves converting both weights and activations to lower-precision values before running the model. This is achieved by calibrating the model on a small subset of data to determine appropriate quantization parameters. \n",
    "\n",
    "- **Advantages:** \n",
    "  - Provides greater memory savings than dynamic quantization.\n",
    "  - Can speed up inference more effectively since activations are pre-quantized.\n",
    "\n",
    "- **Limitations:** \n",
    "  - Requires a calibration dataset for accurate quantization, which adds an extra step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTQuantizer, ORTModelForCausalLM\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoCalibrationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gpt2\"\n",
    "\n",
    "quantized_model = ORTModelForCausalLM.from_pretrained(model_id, export=True, use_cache=False, use_io_binding=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "quantizer = ORTQuantizer.from_pretrained(quantized_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "qconfig = AutoQuantizationConfig.avx512(is_static=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Preprocessing function for the calibration dataset, ensuring consistent shapes with batch size\n",
    "def preprocess(ex, tokenizer):\n",
    "    print(ex.keys())\n",
    "    tokenized_output = tokenizer(ex[\"text\"], truncation=True, padding=\"max_length\", max_length=100)\n",
    "    # for each element in the tokenized_output, add a \"position_ids\" key where it is just a range from 0 to the length of the input_ids - 1 \n",
    "    position_ids = []\n",
    "    for input_ids in tokenized_output[\"input_ids\"]:\n",
    "        position_ids.append(list(range(len(input_ids))))\n",
    "    tokenized_output[\"position_ids\"] = position_ids\n",
    "    return tokenized_output\n",
    "\n",
    "# Carica il dataset di calibrazione\n",
    "calibration_dataset = quantizer.get_calibration_dataset(\n",
    "    \"Salesforce/wikitext\",\n",
    "    dataset_config_name=\"wikitext-2-raw-v1\",\n",
    "    preprocess_function=partial(preprocess, tokenizer=tokenizer),\n",
    "    num_samples=50,\n",
    "    dataset_split=\"test\",\n",
    ")\n",
    "\n",
    "\n",
    "calibration_config = AutoCalibrationConfig.minmax(calibration_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the calibration step: computes the activations quantization ranges\n",
    "ranges = quantizer.fit(\n",
    "    dataset=calibration_dataset,\n",
    "    calibration_config=calibration_config,\n",
    "    operators_to_quantize=qconfig.operators_to_quantize,\n",
    ")\n",
    "\n",
    "# Apply static quantization on the model\n",
    "model_quantized_path = quantizer.quantize(\n",
    "    save_dir=\"static_quantized_model\",\n",
    "    calibration_tensors_range=ranges,\n",
    "    quantization_config=qconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_quantized_path = \"static_quantized_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "quantized_model = ORTModelForCausalLM.from_pretrained(model_quantized_path, use_cache=False, use_io_binding=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The secret of life is just a in\n",
      "39, that by great the, if the, in and (Piss, \". n the from the very as a the to to very. a \" in, with. the a. the the the I\n",
      "; our and it- the the to the, in. and\n",
      "​, for to and/- that the the the in is as, all a in or- a S d- R- all A- and ( for the\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=quantized_model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"The secret of life is\"\n",
    "\n",
    "output = pipe(text, max_length=100, do_sample=True, truncation=True)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
