{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization in Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to Quantization**\n",
    "\n",
    "Quantization is a process used to reduce the memory requirements and computational complexity of large machine learning models. By representing model parameters with lower-precision values, quantization makes it possible to run models more efficiently on devices with limited memory and computational resources.\n",
    "\n",
    "For large language models (LLMs), quantization can:\n",
    "- **Reduce Memory Usage:** Lower-precision data types (such as int8) use less memory than higher-precision types (like float32), allowing models to fit into memory-constrained environments.\n",
    "- **Improve Inference Speed:** By using simpler operations on smaller data types, quantization can reduce the time it takes for a model to process inputs and generate outputs.\n",
    "- **Preserve Accuracy:** Quantization is carefully designed to minimize the impact on model accuracy, though a trade-off often exists between precision and efficiency.\n",
    "\n",
    "We will focus on Post-Training Quantization (PTQ), a quantization technique that applies quantization to a pre-trained model. PTQ is a popular method for quantizing large language models because it can be applied to a wide range of models that we may want to use in inference mode. By contrast, quantization-aware training (QAT) requires retraining the model with quantization in mind, which can be more complex and time-consuming.\n",
    "\n",
    "We will first get a general understanding of quantization by manually implementing two commonly adopted approaches: absmax and minmax (or zero-point). \n",
    "\n",
    "Next, we will explore two different ways (one using PyTorch, the other using HuggingFace) to run a **dynamic quantization** (i.e., PTQ where only the weights are quantized, and not the activations). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Absmax and Minmax Quantization\n",
    "\n",
    "The goal of quantization is, remember, mapping continuos values (e.g., float32) into a discrete set of values (e.g., int8). \n",
    "\n",
    "So let's create a matrix $W$ and a vector $x$ to be quantized. Let's initialize them randomly (but, just to see what happens, let's set W[0,0] and x[0] = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "n_rows = 3\n",
    "n_cols = 5\n",
    "W = torch.randn(n_rows, n_cols)\n",
    "x = torch.randn(n_cols)\n",
    "\n",
    "W[0,0] = 0\n",
    "x[0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first compute the matrix multiplication to observe the result. This is the operation that we typically want to execute, and that we want to quantize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will quantize $W$ and $x$ separately, and then multiply them together. Finally, we will need to dequantize the result to compare it with the original result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4829, 1.3345, 0.2121])\n"
     ]
    }
   ],
   "source": [
    "out = W @ x\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Absmax quantization\n",
    "\n",
    "In absmax quantization, we use a symmetric range around 0. This means that we need to identify the maximum absolute value in the matrix $W$ and the vector $x$.\n",
    "\n",
    "We define a function `absmax_quantize` that takes as input any tensor and produces a version of the same tensor, but quantized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_quantize(W):\n",
    "    # NOTE: we assume that we always map to 8-bit integers\n",
    "    max_value = W.abs().max()\n",
    "    scale = 127 / max_value # how \"long\" the step between any two int8 values is\n",
    "    W_q = (W * scale).round().to(torch.int8)\n",
    "    return W_q, scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, we return both the quantized tensor and the scale factor. The scale factor is used to dequantize the tensor. So we might as well define a dequantize function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_dequantize(W_q, scale):\n",
    "    return W_q.float() / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the quantized version of W, and of x. Then, we can check how much we are losing by quantizing the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q, scale_W = absmax_quantize(W)\n",
    "x_q, scale_x = absmax_quantize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,  -17, -127,   33,  -63],\n",
      "        [ -82,   24,   49,  -42,  -24],\n",
      "        [ -35,   11,  -50,   64,  -62]], dtype=torch.int8)\n",
      "tensor([[ 0.0000, -0.2934, -2.1788,  0.5684, -1.0845],\n",
      "        [-1.3986,  0.4033,  0.8380, -0.7193, -0.4033],\n",
      "        [-0.5966,  0.1820, -0.8567,  1.1006, -1.0712]])\n",
      "tensor([[ 0.0000, -0.2916, -2.1788,  0.5661, -1.0808],\n",
      "        [-1.4068,  0.4117,  0.8406, -0.7205, -0.4117],\n",
      "        [-0.6005,  0.1887, -0.8578,  1.0980, -1.0637]])\n",
      "tensor(0.0039)\n"
     ]
    }
   ],
   "source": [
    "print(W_q)\n",
    "print(W)\n",
    "W_deq = absmax_dequantize(W_q, scale_W)\n",
    "print(W_deq)\n",
    "print((W - W_deq).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,  -48,   31,  -75, -127], dtype=torch.int8)\n",
      "tensor([ 0.0000, -0.5663,  0.3731, -0.8920, -1.5091])\n",
      "tensor([ 0.0000, -0.5704,  0.3684, -0.8912, -1.5091])\n",
      "tensor(0.0019)\n"
     ]
    }
   ],
   "source": [
    "print(x_q)\n",
    "print(x)\n",
    "x_deq = absmax_dequantize(x_q, scale_x)\n",
    "print(x_deq)\n",
    "print((x - x_deq).abs().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, in both cases, absmax maps the value 0 to 0. This is a good property, as it allows us to represent the zero value without losing any information. This property stems from the symmetry around 0 we imposed.\n",
    "\n",
    "However, do note that we are also \"wasting\" some bits of the range! Can you spot where?\n",
    "\n",
    "Let's now compute the matrix multiplication between W_q and x_q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([101, -91, -28], dtype=torch.int8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q @ x_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see that there's something wrong? Let's see what one of the rows of W_q and x_q contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   0,  -17, -127,   33,  -63], dtype=torch.int8),\n",
       " tensor([   0,  -48,   31,  -75, -127], dtype=torch.int8))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q[0], x_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product of these two vectors definitely isn't what we get as the first number of the matrix multiplication -- i.e. (W_q @ x_q)[0]. Indeed, we can run as int16, and see that the result is quite different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2405, 6565,  996], dtype=torch.int16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_q.to(torch.int16) @ x_q.to(torch.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the dot product overflows the int8 range. This is a well-known problem. Indeed, the accumulation of results, in quantization, is typically done with higher precision than the single values. This is tricky to do in pure Python/PyTorch, but can be done efficiently in other ways.\n",
    "\n",
    "Let's stick to the simple approach for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_q = W_q.to(torch.int16) @ x_q.to(torch.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the correct result, we need to dequantize the result. This is done by multiplying the result by the scale factor of the two operands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4903, 1.3383, 0.2030])\n",
      "tensor([0.4829, 1.3345, 0.2121])\n"
     ]
    }
   ],
   "source": [
    "out_deq = absmax_dequantize(absmax_dequantize(out_q, scale_W), scale_x)\n",
    "# Or, alternatively:\n",
    "# out_deq = out_q / scale_W / scale_x\n",
    "# out_deq = absmax_dequantize(out_q, scale_W * scale_x)\n",
    "print(out_deq)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, our goal was `out`. How much did we lose by quantizing and dequantizing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0068)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_deq - out).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Minmax Quantization\n",
    "\n",
    "In minmax quantization, we use the minimum and maximum values in the matrix $W$ and the vector $x$ to define the range. In this way, we get a range that is as tight as possible around the values we are quantizing. This will, however, change the zero value, which will not be mapped to 0 anymore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_quantize(W):\n",
    "    # the following notations come from:\n",
    "    # (1) scaling W to [0,1] ==> W' = (W - min(W)) / (max(W) - min(W)),\n",
    "    # (2) scaling W' to [-128, 127] ==> W_q = W' * 255 - 128\n",
    "    # by combining the two, we get that:\n",
    "    # W_q = W * scale + offset\n",
    "    # we will call the offset \"zero_point\", as it represent the value that maps to 0\n",
    "    delta = W.max() - W.min()\n",
    "    scale = 255 / delta\n",
    "    zero_point = -(128*W.max() + 127*W.min()) / delta\n",
    "    W_q = (W * scale + zero_point).round().to(torch.int8)\n",
    "    return W_q, scale, zero_point\n",
    "\n",
    "def minmax_dequantize(W_q, scale, zero_point):\n",
    "    return (W_q.float() - zero_point) / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q, scale_W, zero_point_W = minmax_quantize(W)\n",
    "x_q, scale_x, zero_point_x = minmax_quantize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results for $W$ (same considerations will apply for $x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  41,   19, -128,   86,  -43],\n",
      "        [ -67,   73,  107,  -15,   10],\n",
      "        [  -5,   56,  -25,  127,  -42]], dtype=torch.int8)\n",
      "tensor([[ 0.0000, -0.2934, -2.1788,  0.5684, -1.0845],\n",
      "        [-1.3986,  0.4033,  0.8380, -0.7193, -0.4033],\n",
      "        [-0.5966,  0.1820, -0.8567,  1.1006, -1.0712]])\n",
      "tensor([[-0.0054, -0.2883, -2.1788,  0.5733, -1.0857],\n",
      "        [-1.3943,  0.4061,  0.8434, -0.7256, -0.4041],\n",
      "        [-0.5970,  0.1875, -0.8542,  1.1006, -1.0728]])\n",
      "tensor(0.0031)\n",
      "zero point tensor(41.4189)\n"
     ]
    }
   ],
   "source": [
    "print(W_q)\n",
    "print(W)\n",
    "W_deq = minmax_dequantize(W_q, scale_W, zero_point_W)\n",
    "print(W_deq)\n",
    "print((W - W_deq).abs().mean())\n",
    "print(\"zero point\", zero_point_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, notice that 0 no longer maps to 0! Indeed, it maps to zero_point_W (after rounding). This implies that the dequantization of 0 will no longer be 0. This may be a problem!\n",
    "\n",
    "But, notice that we are using the full range of the int8 values. This means that we are not wasting any bits of the range! (the minimum value is -128, the maximum value is 127). This can also be seen in the average absolute error, which is lower than what we had with absmax.\n",
    "\n",
    "Similarly to what we did before, let's compute the output of the operation, and then dequantize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_q = W_q.to(torch.int16) @ x_q.to(torch.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dequantification is a bit trickier, in this case. Can you figure out why we need the following operations?\n",
    "\n",
    "Hint: consider the transformation we are applying to each value (value * scale + zero_point). What happens when we compute the dot product?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_deq = (out_q - W.shape[1] * zero_point_W * zero_point_x - W.sum(axis=1) * scale_W * zero_point_x - x.sum() * scale_x * zero_point_W) / (scale_W * scale_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4812, 1.3489, 0.2222])\n",
      "tensor([0.4829, 1.3345, 0.2121])\n",
      "tensor(0.0087)\n"
     ]
    }
   ],
   "source": [
    "print(out_deq)\n",
    "print(out)\n",
    "print((out_deq - out).abs().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Extra stuff!</span>\n",
    "\n",
    "We could have computed scales and zero points at different granularities (e.g., for each row, or column of $W$). How would that have changed the results? What changes would we have to do to the code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part, we will apply dynamic quantization by using PyTorch or HuggingFace (with BitsAndBytes). We will quantize both to 8 and to 4 bits, and we will see how that affects LLMs (in terms of memory and speed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/fgiobergia/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to the Hugging Face model hub to be able to upload models\n",
    "with open(\"../hf_token.txt\", \"r\") as f:\n",
    "    token = f.read()\n",
    "    f.close()\n",
    "\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load our model (Llama 3.2 1B) and let's see some base statistics (memory usage, inference time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GPT-2 model\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size before quantization 4943.31 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp.pth\")\n",
    "    size = os.path.getsize(\"temp.pth\") / 1e6  # size in \"MB\" (technically, it should be 1024**2, but we approximate to 1e6 to get an easier conversion #params <=> MB)\n",
    "    os.remove(\"temp.pth\")\n",
    "    return size\n",
    "\n",
    "print(f\"Model size before quantization {(get_model_size(model)):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, wasn't Llama 1B supposed to be 4GB (4 bytes * 1B parameters)? Why do we get ~ 5 GB (i.e., 1.25B parameters)?\n",
    "\n",
    "We are not considering the parameters used in the embedding layer (you can count how many parameters you have in the embedding layer and see that it matches the difference). \n",
    "\n",
    "Additionally, the count does not include the `lm_head`, i.e. the layer used to go from the hidden states to the logits. This is because in Llama (and other models) the `lm_head` is shared with the embedding layer, so it is not counted twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline model output: The secret of life is to be happy, and to be happy is to be yourself.\n",
      "I believe in life, and I believe in death.\n",
      "I am the only one who can change my life, and I am the only one who can change the world.\n",
      "I believe in a world where everyone has the right to live and to be happy, and I believe that this is a world that is not only possible, but that it is necessary.\n",
      "I believe in the power of love and the power\n",
      "\n",
      "Time taken for baseline model: 24.755378484725952\n"
     ]
    }
   ],
   "source": [
    "text = \"The secret of life is\"\n",
    "# Notice we use a batch of 20 sentences -- we will get better results\n",
    "# on quantized models when processing a batch of inputs\n",
    "inputs = tokenizer([text]*20, return_tensors=\"pt\")\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    baseline_output = model.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "elapsed_time = time.time() - tic\n",
    "\n",
    "baseline_decoded = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nBaseline model output:\", baseline_decoded)\n",
    "print(\"\\nTime taken for baseline model:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic quantization applies lower precision to model weights and activations at runtime. This method doesnâ€™t require modifications to the model architecture or retraining, which makes it relatively easy to apply.\n",
    "\n",
    "- **Advantages:** \n",
    "  - Quick to implement with minimal changes. No calibration step is needed.\n",
    "\n",
    "- **Limitations:** \n",
    "  - Activations are not pre-quantized, meaning some precision is maintained but at the cost of slightly higher resource use at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `quantize_dynamic()` function, available in PyTorch, to apply dynamic quantization to a model.\n",
    "\n",
    "We can specify a set of layer types to be quantize. Let's stick with Linear layers. We specify the desired type (represented by torch.qint8) , and off we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after quantization 2286.83 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ").to('cpu')\n",
    "\n",
    "# Model size after quantization\n",
    "print(f\"Model size after quantization {(get_model_size(quantized_model)):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay -- 2.3GB? Why not 5GB / 4 = 1.25GB? After all, we are going from float32 to int8. \n",
    "\n",
    "That's correct -- technically. Except, we are only encoding linear layers, and not the embedding layer. That means that, of the original 1.25B parameters, we are only quantizing 1B. The rest, in the embedding layer, is kept as float32.\n",
    "\n",
    "If you run the numbers, though, you should still find a problem: 1B * 1 byte + 0.25B * 4 bytes = 2GB. What about the rest? There's one more thing: remember, the `lm_head` was shared with the Embedding layer. However, since it is \"copied\" into a linear layer in Llama, the quantization process will quantize it as well. So that's an extra 0.25B parameters encoded as int8 -- hence 2.3GB.\n",
    "\n",
    "Finally, we could technically also quantize the embeddings (it has been introduced in later versions of PyTorch), but for simplicity we will not do it here (it would require some additional steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): DynamicQuantizedLinear(in_features=2048, out_features=2048, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (k_proj): DynamicQuantizedLinear(in_features=2048, out_features=512, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (v_proj): DynamicQuantizedLinear(in_features=2048, out_features=512, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (o_proj): DynamicQuantizedLinear(in_features=2048, out_features=2048, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): DynamicQuantizedLinear(in_features=2048, out_features=8192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (up_proj): DynamicQuantizedLinear(in_features=2048, out_features=8192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (down_proj): DynamicQuantizedLinear(in_features=8192, out_features=2048, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): DynamicQuantizedLinear(in_features=2048, out_features=128256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantized model output: The secret of life is to be happy, and to be happy is to be yourself.\n",
      "I believe in life, and I believe in death.\n",
      "I am the only one who can change my life, and I am the only one who can change the world.\n",
      "I believe in a world where everyone has the right to live and to be happy, and I believe that this is a world that is not only possible, but that it is necessary.\n",
      "I believe in the power of love and the power\n",
      "\n",
      "Time taken for baseline model: 7.114934682846069\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = quantized_model.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "elapsed_time = time.time() - tic\n",
    "\n",
    "output_decoded = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nQuantized model output:\", output_decoded)\n",
    "print(\"\\nTime taken for baseline model:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/v4.46.0/quantization/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after quantization: 1072.990908 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "\n",
    "print(f\"Model size after quantization: {get_model_size(quantized_model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/home/fgiobergia/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1935: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/fgiobergia/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "quantized model output: The secret of life is to be happy, and to be happy is to be yourself.\n",
      "I believe in life, and I believe in death.\n",
      "I am the only one who can change my life, and I am the only one who can change the world.\n",
      "I believe in a world where everyone has the right to live and to be happy, and I believe that this is a world that is not only possible, but that it is necessary.\n",
      "I believe in the power of love and the power\n",
      "\n",
      "Time taken for baseline model: 5.068130970001221\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = quantized_model.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "elapsed_time = time.time() - tic\n",
    "\n",
    "output_decoded = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nquantized model output:\", output_decoded)\n",
    "print(\"\\nTime taken for baseline model:\", elapsed_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
