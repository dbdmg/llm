{"cells":[{"cell_type":"markdown","metadata":{"id":"VtisuG2Q6lhm"},"source":["# Exercise: User Story generation\n","\n","User stories are brief, simple descriptions of a feature or requirement from the perspective of an end user. They are used in Agile software development to capture the user’s needs and goals in a way that prioritizes functionality and value. A typical user story follows the format:\n","\n","\"As a [type of user], I want [a specific goal] so that [reason or benefit].\"\n","\n","This format ensures the focus stays on delivering solutions that meet the user’s expectations. User stories guide development by defining clear, actionable tasks for teams while remaining flexible and open to adaptation as the project evolves.\n","\n","Our objective is to create an agent architecture like the following one:\n","\n","1. **Problem Specification (Domain Specification):**\n","The process begins with the definition of a problem description, which outlines the domain or context of the system. This serves as the input to the LLM and sets the foundation for generating user stories. Our problem specification is provided as the input to the process.\n","\n","2. **Role(s) Extraction (\"Who\" Aspect):**\n","A prompt is used to extract roles from the problem description.\n","Roles represent the different types of users or stakeholders involved in the system This step answers \"who\" the user stories are about.\n","\n","3. **Function(s) Extraction (\"What\" Aspect):**\n","Another prompt is used to extract functions associated with each role.\n","Functions describe the specific actions or goals the roles aim to perform within the system. This step answers \"what\" the users want to achieve.\n","\n","4. **Purpose(s) Extraction (\"Why\" Aspect):**\n","A subsequent prompt is used to determine the purposes behind the functions identified. Purposes explain the reasons or motivations behind the desired actions of the roles. This step answers \"why\" the users need the functions.\n","\n","5. **User Stories Generation:**\n","Combining the roles (who), functions (what), and purposes (why), a final prompt generates complete user stories. These user stories take the standard Agile format: \"As a \\\u003cwho\\\u003e, I want \\\u003cwhat\\\u003e, so that \\\u003cwhy\\\u003e.\"\n","\n","**Prompt Chain**:\n","The process is iterative, as each step informs the next and ensures that the roles, functions, and purposes are fully aligned. The LLM is guided by a structured \"prompt chain,\" where prompts are carefully designed to extract specific information at each stage.\n","\n","**Output**:\n","The final output consists of user stories that capture the needs and goals of the system's users in a concise, actionable format. These stories guide development teams in implementing the desired features and functionalities.\n","\n","![image.png](attachment:image.png)\n","\n","The diagram above shows the overall flow of the LLM architecture.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":67},"id":"hL2Z-R9s3uXY"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbfacf70838c4d089764b17d5addf899","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/878 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9859ed3ef304306b80e6ed246432ac9","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c63cc3903264c2d9e138d0fc5f1b79e","version_major":2,"version_minor":0},"text/plain":["Fetching 2 files:   0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8b54b076f564e2b84d346b7cc3548df","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fba6c4de2adf425184beed4dc78197cb","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f03b733a22e1493b9a198e2f631b5350","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9990c2f001c041879267f8423baab385","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/189 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08e499a50e4d4e199b84f82eba89364b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/54.5k [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f850d7892fa4fb7bdc7868eeec47639","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"16d806fb1b35485aac3260407305481a","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/296 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"ename":"NameError","evalue":"name 'output' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2006011308.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \"\"\"\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 81\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_a_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prompt: {prompt}\\nResponse: {response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2006011308.py\u001b[0m in \u001b[0;36mmake_a_query\u001b[0;34m(prompt, maxl, temp)\u001b[0m\n\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 67\u001b[0;31m     \u001b[0mgenerated_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Decode and return the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"]}],"source":["\n","#reference requirements_text\n","\n","requirements_text = \"The proposed platform is designed to enhance the hiking experience for various user groups, including visitors, local guides, platform managers, and hut workers. The platform provides a centralized repository of hiking routes, hut information, and parking facilities. It also enables interactive features such as real-time hike tracking, personalized recommendations, and group hike planning. By combining these capabilities, the platform seeks to foster safe, informed, and collaborative hiking experiences.\\\n","The platform will be deployed as a cloud-based web and mobile application accessible to all stakeholders. The distribution strategy includes an app available on major mobile operating systems, such as iOS and Android, alongside a responsive web interface. It will require an internet connection for features like real-time tracking, notifications, and user authentication, though some offline capabilities, such as pre-downloaded hike information, will also be available.\\\n","User authentication will be role-based, ensuring that only authorized users, such as verified hut workers and platform managers, can access sensitive or administrative features.\\\n","Visitors are the primary users of the platform. They can browse a comprehensive list of hiking trails, filter them based on specific criteria such as difficulty, length, or starting point, and view detailed descriptions. To access advanced features like personalized recommendations, visitors can create user accounts by registering on the platform. Registered users can record their fitness parameters, enabling the system to suggest trails tailored to their capabilities.\\\n","During a hike, visitors can record their progress by marking reference points and sharing their live location through a broadcasting URL. They can also initiate group activities by planning hikes, adding group members, and confirming group participation. The platform allows visitors to start, terminate, and track their hikes, with notifications for unfinished hikes or late group members to ensure safety and accountability.\\\n","Local guides enrich the platform by contributing essential information. They can add detailed descriptions of hikes, parking facilities, and huts, ensuring hikers have accurate and comprehensive data. Local guides also link parking lots and huts to specific trails as starting or arrival points, enhancing the planning process.\\\n","To aid in the visual representation and accessibility of information, local guides can upload pictures of huts and connect these locations directly to hikes. This integration simplifies route planning and helps visitors visualize their journey.\\\n","Platform managers oversee the operational integrity and safety of the platform. They verify new hut worker registrations, ensuring that only authorized personnel can update hut-related data. Managers can also broadcast weather alerts for specific areas, notifying all hikers in those regions through push notifications. This ensures that users stay informed about potentially hazardous conditions.\\\n","The platform manager's role includes maintaining an organized and secure user system while facilitating collaboration between local guides, hut workers, and visitors.\\\n","Hut workers are critical to the maintenance of up-to-date trail and accommodation information. After registering and being verified, hut workers can log into the platform to add or update information about their assigned huts, including uploading pictures and describing the facilities available. They can also monitor and report on the condition of nearby trails, ensuring hikers receive current information.\\\n","Hut workers play a vital role in providing situational updates for hikers. For instance, if a nearby trail is impacted by severe weather or physical obstructions, they can communicate these conditions through the platform. This enhances the safety and preparedness of all hikers relying on the platform.\"\n","\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from huggingface_hub import login\n","import torch\n","\n","#----------------Google Colab only---------------------\n","from google.colab import userdata\n","login(userdata.get('HF_TOKEN'))\n","#----------------Google Colab only---------------------\n","\n","#detect the device available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the tokenizer and model\n","model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n","model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=device)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","\n","print(device)\n","model = model.to(device)\n","\n","\n","#we define a method to ask any prompt to llama\n","def make_a_query(prompt, maxl=200, temp=0.7):\n","    \"\"\"\n","    Send a prompt to the Llama model and get a response.\n","\n","    Args:\n","    - prompt (str): The input question or statement to the model.\n","    - max_length (int): The maximum length of the response.\n","    - temperature (float): Controls randomness in the model's output.\n","\n","    Returns:\n","    - str: The model's generated response.\n","    \"\"\"\n","    # Tokenize the prompt\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","    # Compute the lenght of the input prompt to be able to extract the model's response later\n","    input_ids = inputs[\"input_ids\"]\n","    prompt_length = input_ids.shape[1]\n","\n","    # Generate the output\n","    outputs = model.generate(\n","        inputs['input_ids'],  # Tokenized input\n","        max_length=maxl,         # Limit response length to avoid extra text\n","        temperature=temp,        # Lower temperature to reduce randomness\n","        do_sample=True,        # Set do_sample = False if you want to use greedy sampling or beam search\n","        pad_token_id=tokenizer.eos_token_id  # Ensure the model doesn't go beyond the end token\n","    )\n","\n","    generated_tokens = output[0, prompt_length:]\n","\n","    # Decode and return the response\n","    return tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n","\n","# Example usage - without chat template\n","prompt = \"\"\"\u003c|begin_of_text|\u003e\u003c|start_header_id|\u003esystem\u003c|end_header_id|\u003e\n","You are an expert on world capitals.\n","Respond with only the capital city of the given country. Do not repeat the question.\u003c|eot_id|\u003e\n","\n","Query: What is the capital of France?\n","Answer:\n","\"\"\"\n","\n","response = make_a_query(prompt)\n","\n","print(f\"Prompt: {prompt}\\nResponse: {response}\")\n","\n","\n","# Example usage - with chat template\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are an expert on world capitals. Respond with only the capital city of the given country. Do not repeat the question.\"},\n","    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n","]\n","\n","chat = tokenizer.apply_chat_template(messages,\n","                                     tokenize=False, #only the special tokens are added, we'll apply tokenisation later\n","                                     add_generation_prompt=True) # adds the placeholder for the assistant's response\n","\n","\n","response = make_a_query(chat)\n","\n","print(f\"Prompt: {prompt}\\nResponse: {response}\")\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"arsD7VxrQ1Hw"},"source":["## Extracting the Actors\n","\n","To automate the extraction of roles (\"Who\" aspect) from a problem description, we utilize a language model (LLM) to identify key actors or stakeholders involved in the system. Roles, such as \"visitors,\" \"local guides,\" \"platform managers,\" and \"hut workers,\" represent the primary users or contributors to the system and are essential for generating accurate and actionable user stories. This step focuses on using the LLM to extract these roles and then validating the quality of the extraction by comparing the results against a reference dataset.\n","\n","The LLM is provided with a detailed problem description and prompted to extract the roles (actors) associated with the system. The prompt is carefully designed to ensure the LLM captures all relevant **roles** based on the context of the problem. The output is a list of roles that the LLM determines as key stakeholders in the system.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1763880123291,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"YOx9Jv_eSzZP","outputId":"fa5ff1ee-ddf0-415d-839f-427e1025e55b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Extracted Roles - zero shot: \n"," \n","Extracted Roles - one shot: \n"," \n","Extracted Roles - few shots: \n"," \n"]}],"source":["#define three different prompts for extracting users, using the zero-shot, one-shot and few-shot approach. then evaluate the difference in terms of capability to format the output and actual results\n","\n","# IMPORTANT: wheter you use the chat template or not, remember to add the shots into the correct structure composed by user's query and assistant's response (see previous lab)\n","\n","context_zero_shot = \"\"\n","\n","context_one_shot = \"\"\n","\n","context_few_shots = \"\"\n","\n","# Concatenate the prompt with the requirements text (NOTE: in case you decided to use the chat template, the syntax for the following instructions is completely different)\n","prompt_zero_shot = context_zero_shot + requirements_text + '\\nAnswer: '\n","prompt_one_shot = context_one_shot + requirements_text + '\\nAnswer: '\n","prompt_few_shots = context_few_shots + requirements_text + '\\nAnswer: '\n","\n","# Call the LLM function for the three prompts and print the results\n","roles_extracted_zero_shot = \"\"\n","\n","print(\"Extracted Roles - zero shot: \\n\", roles_extracted_zero_shot)\n","\n","roles_extracted_one_shot = \"\"\n","\n","print(\"Extracted Roles - one shot: \\n\", roles_extracted_one_shot)\n","\n","\n","roles_extracted_few_shots = \"\"\n","\n","print(\"Extracted Roles - few shots: \\n\", roles_extracted_few_shots)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i5q9_P3oYh8B"},"source":["While no external dataset is used to refine or influence the role extraction process, we validate the extracted roles against a reference dataset contained in a CSV file named stories.csv. This file includes a specific column listing all the predefined roles associated with the system. These roles serve as the ground truth for validation purposes.\n","\n","\n","To assess the accuracy and completeness of the LLM’s role extraction, we compute the precision and recall metrics by comparing the LLM's output to the ground truth roles:\n","\n","- **Precision**: Measures how many of the roles extracted by the LLM are correct (i.e., exist in the reference dataset).\n","\n","  $P = \\frac{True\\ Positives}{True\\ Positives + False\\ Positives}$\n","\n","\n","- **Recall**: Measures how many of the ground truth roles were successfully identified by the LLM.\n","\n","  $R = \\frac{True\\ Positives}{True\\ Positives + False\\ Negatives}$\n","\n","\n","After extracting the roles using the LLM, the results are compared to the roles in the reference column from the CSV file. For each role identified by the LLM:\n","\n","- If the role exists in the reference dataset, it is considered a true positive.\n","- If the role does not exist in the reference dataset, it is marked as a false positive.\n","- Any role in the reference dataset that the LLM fails to identify is considered a false negative.\n","\n","The final output of this process includes:\n","- A list of roles extracted by the LLM.\n","- Precision and recall metrics, which provide a quantitative assessment of the LLM’s performance in role extraction."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQ2WQlindL0M"},"outputs":[],"source":["import csv\n","import re\n","\n","#define a function to extract unique roles from the .csv files with user stories\n","\n","def extract_unique_roles(file_path):\n","    roles = set()\n","\n","    #write here your code (HINT: look at the user stories in the csv file and use simple \"find\" functions)\n","\n","    return list(roles)\n","\n","\n","# find the unique roles and put them into a list\n","file_path = 'stories.csv'\n","expected_roles = extract_unique_roles(file_path)\n","print(expected_roles)       #EXPECTED RESULT FOR ROLES\n","\n","\n","#now extract unique roles from the result provided by the LLM\n","#for simplicity, we only consider the few shots prompt for this purpose\n","\n","actual_roles = \"\"\n","\n","print(actual_roles)         #ACTUAL RESULT FOR ROLES\n","\n","\n","\n","#write your code here to compute precision and recall\n","#use normalized roles, i.e., don't consider spaces and 's' at the end of the roles\n","def normalize_roles(role_list):\n","    return {role.lower().strip().rstrip('s') for role in role_list}\n","\n","true_positives = 0\n","false_positives = 0\n","false_negatives = 0\n","\n","precision = 0\n","recall = 0\n","\n","\n","# Print results\n","print(\"True Positives:\", true_positives)\n","print(\"False Positives:\", false_positives)\n","print(\"False Negatives:\", false_negatives)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G6OC3Xl4g3wx"},"source":["## Extracting the Functions\n","\n","In this step, the primary goal is to automate the identification of functions required by the roles identified in the previous step (Role Extraction). The function refers to the tasks or actions that the roles need to perform within the system. To achieve this, a language model (LLM) is used to analyze the problem description, and based on the roles identified, it is prompted to extract the functions those roles desire to perform.\n","\n","For example, after identifying the role of \"visitors\" or \"local guides,\" the LLM is prompted to determine the specific actions these roles want to carry out within the system, such as \"view hikes,\" \"add descriptions,\" or \"manage registrations.\" This extraction process is critical for understanding the functionality needed in the system to meet the requirements of its stakeholders.\n","\n","By leveraging the context and the roles passed from the previous step, the LLM extracts a list of desired functions. This list helps define the operational scope of the system and lays the foundation for generating user stories in subsequent steps. The extracted functions are validated to ensure completeness and alignment with the roles' needs.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nuw4wXk7krC2"},"outputs":[],"source":["\n","\n","current_roles = actual_roles\n","MAX_FUNCTIONS = 15\n","\n","\n","#this function will be used to extract the functions from the answer provided by the llm agent\n","\n","def extract_functions(text):\n","    functions = list()\n","\n","    return functions\n","\n","\n","#this function can be used to format a user story in the expected format. it returns a list of tuples (user, story)\n","\n","def create_user_stories(role, functions):\n","    # Generate user stories in the desired format\n","    user_stories = [(role, f\"As a {role}, I want to {function}\") for function in functions]\n","    return user_stories\n","\n","\n","for role in current_roles:\n","\n","    #1. Loop over the list of roles and extract the piece of text from requirements related to that role\n","    print(\"extracting stories for role\", role)\n","\n","\n","    #define a prompt to extract functions for a given user\n","    #the prompt will be composed of the following fields:\n","    # - context: usage persona of the Lllama agent and instructions\n","    # - examples: examples of requirements text, role and user\n","    # - task: description of the task to perform. here you have to provide the current role\n","    # - answer field: placeholder from which the response will start\n","\n","    #a tentative example can be the following one (you can generate many of them for instance by using another LLM agent, e.g. ChatGPT)\n","    example1_text = \"\"\"\n","    ### Example 1:\n","    **Requirements Text**:\n","    \"The application is designed to streamline food delivery operations. Customers can browse menus and place orders. Delivery personnel use the app to view delivery assignments and update order statuses. Restaurant managers can manage their menus and track incoming orders.\"\n","\n","    **Functions for Delivery personnel**:\n","    1. View delivery assignments.\n","    2. Update order status.\n","    \"\"\"\n","\n","    tmp_prompt= \"\"\n","\n","\n","    #now ask the agent and get the response\n","    response = \"\"\n","\n","    print(response)\n","\n","    functions = extract_functions(response)\n","\n","    tmp_user_stories = create_user_stories(role, functions)\n","    user_stories.append(tmp_user_stories)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yxumxD00nwJu"},"source":["## Extracting the purposes\n","\n","In this step, the focus is on extracting the purposes behind the functions identified in the previous step. The purpose refers to the underlying reasons or motivations that drive a role to perform a particular function within the system. The language model (LLM) is prompted to identify these reasons by analyzing the problem description and the functions extracted in the previous step.\n","\n","After the functions (what the roles want to do) are identified, the LLM is tasked with understanding and extracting the why behind each function. For example, a role like \"hiker\" may want to \"register for a hike\" (function), but the purpose behind this action might be \"so that they can track their progress\" or \"to ensure they can participate in the hike.\" Similarly, a \"local guide\" may want to \"add a hike description,\" and the purpose could be \"to provide helpful information to other users.\"\n","\n","The extracted purposes help provide a deeper understanding of why each function is important to the roles. These purposes are essential for refining the system’s objectives and ensuring that the design aligns with the actual needs and motivations of the stakeholders. The LLM identifies these purposes by analyzing both the context of the roles and the corresponding functions, ensuring that all relevant purposes are captured to provide clarity for the user stories that will follow in the next step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XY6gJPs-nvel"},"outputs":[],"source":["#we take the pairs of role - function obtained in the previous step\n","#for simplicity, we only consider two user stories at this point\n","#to complete the task, we would require to perform an iteration over all the stories defined - this would be very computationally expensive and is out of the scope of the current exercise\n","\n","user_story1 = user_stories[0]\n","user_story2 = user_stories[1]\n","\n","print(user_story1)\n","\n","role=user_story1[0]\n","function=user_story1[1]\n","\n","print(role)\n","\n","tmp_prompt = \"\""]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"003136d12fec4e24a3cd921256f422c9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35600f760c6e43b8ba387a9b7387980d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44d62015b6904b3b948232da7821eb4a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49c65f83d95145cd8661e2a884503d75":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55e4698ad4e14efaaae126471d8ecd2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"81af8b97f77b47b8a5b440ee69b140c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1eadae1a5f34205abfa8f90d8cc655e","placeholder":"​","style":"IPY_MODEL_e003a569b21d47609af0bcfb9ed5901d","value":" 878/878 [00:00\u0026lt;00:00, 31.0kB/s]"}},"8f970a77167242dab606f6cdb1a4baf9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_49c65f83d95145cd8661e2a884503d75","max":878,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55e4698ad4e14efaaae126471d8ecd2d","value":878}},"b1eadae1a5f34205abfa8f90d8cc655e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4d64223407844d588f4393f8b914cff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44d62015b6904b3b948232da7821eb4a","placeholder":"​","style":"IPY_MODEL_35600f760c6e43b8ba387a9b7387980d","value":"config.json: 100%"}},"e003a569b21d47609af0bcfb9ed5901d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbfacf70838c4d089764b17d5addf899":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d4d64223407844d588f4393f8b914cff","IPY_MODEL_8f970a77167242dab606f6cdb1a4baf9","IPY_MODEL_81af8b97f77b47b8a5b440ee69b140c7"],"layout":"IPY_MODEL_003136d12fec4e24a3cd921256f422c9"}}}}},"nbformat":4,"nbformat_minor":0}