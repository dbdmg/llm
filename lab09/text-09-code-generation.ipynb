{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: A function to compute average grades\n",
    "\n",
    "In this series of exercises, we will use LLMs to generate code based on specific requirements and then compare the output against predefined test cases to ensure correctness. The focus will be on generating functional code and executing tests to verify that it meets the given specifications.\n",
    "\n",
    "### Step 1: Requirements\n",
    "\n",
    "The function receives the grades of a student on her courses (for simplicity exactly six grades for six courses are considered) and computes the average. Grades can be from 18 to 30, or 30Laude == 33. The average is computed excluding the best and worse grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "\n",
    "#TODO\n",
    "#define here your reference solution\n",
    "\n",
    "def compute_average(grades):\n",
    "    \"\"\"\n",
    "    Computes the average of grades, excluding the best and worst grades.\n",
    "    \n",
    "    Args:\n",
    "        grades (list): A list of six grades, where each grade is between 18 and 33.\n",
    "    \n",
    "    Returns:\n",
    "        float: The computed average.\n",
    "    \"\"\"\n",
    "\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "#TODO\n",
    "#paste here the result of the function as given by chatGPT\n",
    "\n",
    "gpt_result = \"\"\"\"\"\"\n",
    "\n",
    "\n",
    "#the dictionary my_codes will contain all the generated codes. Let's start by adding the one generated by chatgpt\n",
    "\n",
    "my_codes = {}\n",
    "my_codes[\"GPT\"] = gpt_result\n",
    "\n",
    "\n",
    "# Example usage of the function\n",
    "\n",
    "grades = [18, 25, 30, 33, 22, 28]\n",
    "average = compute_average(grades)\n",
    "print(f\"The computed average is: {average:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Test Cases\n",
    "\n",
    "To generate a comprehensive black-box test suite for the compute_average function, we will create tests that cover different equivalence classes, boundary conditions, and typical scenarios.\n",
    "\n",
    "- Valid inputs: \n",
    "  - Standard valid grades within the range of 18 to 33, including \"30Laude\" (33).\n",
    "  - Tests where all grades are different, with some being 30Laude.\n",
    "\n",
    "- Boundary values:\n",
    "  - Input with the lowest valid grade (18) and the highest valid grade (33).\n",
    "\n",
    "- Invalid inputs:\n",
    "  - A grade list that has more or fewer than 6 grades.\n",
    "  - Grades outside the valid range (less than 18 or greater than 33).\n",
    "  - non-numeric inputs\n",
    "\n",
    "- Edge cases:\n",
    "  - All grades are the same.\n",
    "  - All grades are 30Laude (33).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid test cases\n",
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "#an example of a valid test case. Notice the use of the assert function and the pytest.approx to cope with possible roundings\n",
    "\n",
    "def test_valid_grades():\n",
    "    grades = [18, 25, 30, 33, 22, 28]\n",
    "    result = compute_average(grades)\n",
    "    assert result == pytest.approx(26.25, rel=1e-2)\n",
    "\n",
    "#an example of an invalid test case. Notice the use of the with pytest.raises syntax, to catch exceptions in the function\n",
    "\n",
    "# Invalid test cases\n",
    "def test_more_than_six_grades():\n",
    "    grades = [18, 25, 30, 33, 22, 28, 29]\n",
    "    with pytest.raises(ValueError):\n",
    "        compute_average(grades)\n",
    "\n",
    "#TODO\n",
    "#try to define here additional valid and invalid test cases, that can be used to verify the generated code.\n",
    "#the ground truth test cases are defined in test_cases_01.py for the average computation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the test cases\n",
    "\n",
    "This code defines a function run_tests that uses the ipytest library to run test cases within a Jupyter notebook. The ipytest library seeks for test cases in the notebook and launches them. The '-vv' parameter is used to provide a verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test suite\n",
    "\n",
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "def run_tests():\n",
    "    ipytest.run('-vv')  \n",
    "\n",
    "# Running the tests\n",
    "run_tests()\n",
    "\n",
    "#TODO\n",
    "#what is the results of the tests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Generating code with CodeLLAMA\n",
    "\n",
    "In this step, we leverage CodeLLAMA to automatically generate Python code based on specified requirements.\n",
    "\n",
    "In this step, we define a prompt for coding by providing the requirements, the arguments, and the expected returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "model_name = \"codellama/CodeLlama-13b-Instruct-hf\"  # Specify the model name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)  # Move model to GPU if available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#TODO\n",
    "# Define the prompt\n",
    "prompt = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = \"\"\n",
    "\n",
    "# Generate code using the model\n",
    "outputs = \"\"\n",
    "\n",
    "# Decode the output and print the generated code\n",
    "generated_code = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Parsing CodeLLAMA results and creating functions\n",
    "\n",
    "In this step, the focus is on processing the output generated by CodeLLAMA to extract meaningful Python functions that meet the given requirements. The raw output from CodeLLAMA often includes additional context or irrelevant information, so it needs to be parsed and cleaned to isolate the functional code. \n",
    "\n",
    "Once the code is extracted, it can be added in our dictionary of source code results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO format the code generated from the LLM engine to only contain the function\n",
    "formatted_code = \"\"\n",
    "\n",
    "print(formatted_code)\n",
    "\n",
    "my_codes[\"CODELLAMA\"] = formatted_code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: organizing and analyzing test case results with pytest\n",
    "\n",
    "The code automates the process of running test cases in a Python file using pytest and parsing the results. It defines a function, parse_test_results, that extracts the counts of errors, failures, and passes from a summary line of the pytest output using regular expressions. The script runs the specified test file (test_cases_01.py) via a subprocess call to pytest, capturing its output in plain text. From the output, it locates the summary line containing test results and uses the parse_test_results function to extract key metrics. It calculates the functional correctness ratio as the fraction of passed tests to the total number of tests executed and prints the counts of passed, failed, and errored tests, along with the computed functional correctness ratio. This process provides a clear and automated way to assess the reliability of the tested code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_test_results(result_string):\n",
    "    \"\"\"\n",
    "    Parses the test result string to extract numbers of errors, failures, and passes.\n",
    "\n",
    "    Args:\n",
    "        result_string (str): A string containing test results (e.g., \"3 failed, 11 passed in 0.04s\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys 'errors', 'failures', and 'passed' and their respective counts.\n",
    "    \"\"\"\n",
    "    # Regular expressions to match the counts for errors, failures, and passes\n",
    "    errors = re.search(r\"(\\d+)\\s+errors?\", result_string)\n",
    "    failures = re.search(r\"(\\d+)\\s+failed\", result_string)\n",
    "    passes = re.search(r\"(\\d+)\\s+passed\", result_string)\n",
    "\n",
    "    # Extract numbers or default to 0 if not found\n",
    "    return int(errors.group(1)) if errors else 0, int(failures.group(1)) if failures else 0, int(passes.group(1)) if passes else 0\n",
    "\n",
    "# Define the path to your test file\n",
    "test_file = \"test_cases_01.py\"  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO\n",
    "#define code here to write your function generated with GPT in a file \"function_01.py\". This generated function will be called by the test cases in the file test_cases_01.py\n",
    "\n",
    "#with ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run the pytest command and capture the output\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", test_file, \"--disable-warnings\", \"--tb=short\", \"-q\", \"--color=no\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Extract test results from the pytest output\n",
    "output_lines = result.stdout.split(\"\\n\")\n",
    "summary_line = next((line for line in output_lines if \"passed\" in line or \"failed\" in line or \"error\" in line), None)\n",
    "result_line = output_lines[-2]\n",
    "\n",
    "errors, failures, passes = parse_test_results(result_line)\n",
    "gpt_functional_correctness = passes/(errors+failures+passes)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"GPT\")\n",
    "print(f\"# Passed: {passes}\")\n",
    "print(f\"# Failed: {failures}\")\n",
    "print(f\"# Errors: {errors}\")\n",
    "print(f\"Functional Correctness Ratio: {gpt_functional_correctness:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: comparing results\n",
    "\n",
    "We now extend our analysis to compare the results provided by different agents.\n",
    "\n",
    "For simplicity, we store the function always on the same file (e.g., function_01.py) and we re-execute the same test file (e.g., test_cases_01.py)\n",
    "\n",
    "We evaluate the results based on the functional correctness ratio, and we select the best option that was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO\n",
    "#define code here to write your function generated with LLAMA in the same file \"function_01.py\". This generated function will be called by the test cases in the file test_cases_01.py\n",
    "\n",
    "#with ...\n",
    "\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", test_file, \"--disable-warnings\", \"--tb=short\", \"-q\", \"--color=no\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Extract test results from the pytest output\n",
    "output_lines = result.stdout.split(\"\\n\")\n",
    "summary_line = next((line for line in output_lines if \"passed\" in line or \"failed\" in line or \"error\" in line), None)\n",
    "\n",
    "\n",
    "result_line = output_lines[-2]\n",
    "\n",
    "errors, failures, passes = parse_test_results(result_line)\n",
    "llama_functional_correctness = passes/(errors+failures+passes)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"CODELLAMA\")\n",
    "print(f\"# Passed: {passes}\")\n",
    "print(f\"# Failed: {failures}\")\n",
    "print(f\"# Errors: {errors}\")\n",
    "print(f\"Functional Correctness Ratio: {llama_functional_correctness:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Analyzing and comparing the quality of the generated code\n",
    "\n",
    "Static code quality metrics are used to evaluate the structure, readability, and maintainability of code without executing it. Below is a Python program that calculates some common static code quality metrics for a given Python file:\n",
    "\n",
    "- Lines of Code (LOC): The total number of lines in the file.\n",
    "- Comment Density: The percentage of lines that are comments.\n",
    "- Cyclomatic Complexity: A measure of the complexity of a program based on the number of linearly independent paths.\n",
    "- Maintainability Index (MI): a software metric used to predict how maintainable the code is. Itâ€™s often calculated using a combination of Cyclomatic Complexity (CC), Lines of Code (LOC), and Halstead Volume (HV).\n",
    "\n",
    "The **Radon** tool is a Python package that helps analyze various aspects of code quality, such as Cyclomatic Complexity (CC), Maintainability Index (MI), Raw Metrics (LOC), and Halstead metrics. You can use Radon to analyze the static code quality of your Python code by running it from the command line or through Python code.\n",
    "\n",
    "The tool can be installed by the following command: pip install radon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import radon.complexity as radon_complexity\n",
    "import radon.metrics as radon_metrics\n",
    "import radon.raw as radon_raw\n",
    "\n",
    "def analyze_code_with_radon(file_path):\n",
    "    \"\"\"\n",
    "    Analyzes a Python file and calculates code quality metrics:\n",
    "    - Cyclomatic Complexity (CC)\n",
    "    - Maintainability Index (MI)\n",
    "    - Raw Metrics (LOC, number of functions, etc.)\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the Python file to analyze.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        code = file.read()\n",
    "\n",
    "    # Cyclomatic Complexity Analysis (CC)\n",
    "    cc_results = radon_complexity.cc_visit(code)\n",
    "    print(\"Cyclomatic Complexity:\")\n",
    "    for result in cc_results:\n",
    "        print(f\"Function: {result.name}, Complexity: {result.complexity}\")\n",
    "\n",
    "    # Maintainability Index (MI)\n",
    "    maintainability_index = radon_metrics.mi_visit(code, multi=False)  # Set multi=False for single file\n",
    "    print(f\"\\nMaintainability Index: {maintainability_index}\")\n",
    "\n",
    "    # Raw Metrics (LOC, number of functions, etc.)\n",
    "    raw_metrics = radon_raw.analyze(code)  # Use analyse method for raw metrics\n",
    "    print(f\"\\nLines of Code (LOC): {raw_metrics.loc}\")\n",
    "    print(f\"Number of Comments: {raw_metrics.comments}\")\n",
    "    #print(f\"Blank Lines: {raw_metrics.blank_lines}\")\n",
    "\n",
    "    result = {}\n",
    "    result[\"CC\"] = cc_results[0].complexity\n",
    "    result[\"MI\"] = maintainability_index\n",
    "    result[\"LOC\"] = raw_metrics.loc\n",
    "    result[\"Comments\"] = raw_metrics.comments\n",
    "    return result\n",
    "\n",
    "\n",
    "print(my_codes)\n",
    "\n",
    "\n",
    "overall_results = {}\n",
    "\n",
    "\n",
    "#TODO: cycle over all the pairs language, code in the \"my_codes\" dictionary. For each code, save the function in the function_01.py file and:\n",
    "# - execute the radon analysis with the function above\n",
    "# - run the python tests by using the code provided in a previous cell\n",
    "\n",
    "#Example of the final structure of the overall_results dictionary\n",
    "#{'GPT': {'CC': 4, 'MI': 93.26472421003476, 'LOC': 24, 'Comments': 3, 'FunctionalCorrectness': 0.80}, 'CODELLAMA': {'CC': 1, 'MI': 77.16231550361674, 'LOC': 5, 'Comments': 0, 'FunctionalCorrectness': 0.43}}\n",
    "    \n",
    "\n",
    "print(overall_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Plotting the results\n",
    "\n",
    "generate a set of bar charts to visualize various software metrics for different programming languages, including Cyclomatic Complexity (CC), Maintainability Index (MI), Lines of Code (LOC), and the number of comments. \n",
    "\n",
    "start by extracting the relevant data from the overall_results dictionary, which contains these metrics for each language. Then, using Matplotlib, create a 3x2 grid of subplots, each one dedicated to a different metric. \n",
    "\n",
    "The first four subplots display the values for CC, MI, LOC, and comments, while the last subplot shows the functional correctness values for two models, GPT and LLaMA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "languages = list(overall_results.keys())  \n",
    "\n",
    "\n",
    "\n",
    "cc_values = [overall_results[language]['CC'] for language in languages]\n",
    "mi_values = [overall_results[language]['MI'] for language in languages]\n",
    "loc_values = [overall_results[language]['LOC'] for language in languages]\n",
    "comments_values = [overall_results[language]['Comments'] for language in languages]\n",
    "functional_correctness = [overall_results[language]['FunctionalCorrectness'] for language in languages]\n",
    "\n",
    "# Create a figure with subplots for each metric\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 8))\n",
    "\n",
    "# Plotting CC values\n",
    "axs[0, 0].bar(languages, cc_values, color='skyblue')\n",
    "axs[0, 0].set_title('Cyclomatic Complexity (CC)')\n",
    "axs[0, 0].set_ylabel('CC')\n",
    "\n",
    "# Plotting MI values\n",
    "axs[0, 1].bar(languages, mi_values, color='lightgreen')\n",
    "axs[0, 1].set_title('Maintainability Index (MI)')\n",
    "axs[0, 1].set_ylabel('MI')\n",
    "\n",
    "# Plotting LOC values\n",
    "axs[1, 0].bar(languages, loc_values, color='blue')\n",
    "axs[1, 0].set_title('Lines of Code (LOC)')\n",
    "axs[1, 0].set_ylabel('LOC')\n",
    "\n",
    "# Plotting Comments values\n",
    "axs[1, 1].bar(languages, comments_values, color='lightcoral')\n",
    "axs[1, 1].set_title('Number of Comments')\n",
    "axs[1, 1].set_ylabel('Comments')\n",
    "\n",
    "#plotting functional correctness\n",
    "axs[2, 0].bar(languages, functional_correctness, color='green')\n",
    "axs[2, 0].set_title('Functional correctness')\n",
    "axs[2, 0].set_ylabel('Comments')\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Comparing more LLM agents\n",
    "\n",
    "After analyzing a chat engine (GPT) and a HuggingFace model (CodeLLAMA) try to generate additional examples of code with other engines.\n",
    "\n",
    "You can use to this purpose other models provided by HuggingFace, and/or other ready-made chat engines (e.g., Kwen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO generate additional code samples with other LLM engines\n",
    "qwen_chat = \"\"\n",
    "\n",
    "#add the results to the dictionary of codes\n",
    "\n",
    "#re-execute the analysis in the previous code boxes by comparing more languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Additional functions\n",
    "\n",
    "To evaluate the generalizability of the LLM code generation, now modify the functions on which you are applying your analysis.\n",
    "\n",
    "Consider the following requirements:\n",
    "\n",
    "### Railway company\n",
    "\n",
    "A railway company offers the possibility to people under 15 to travel free. The offer is dedicated to groups\n",
    "from 2 to 5 people travelling together.\n",
    "For being eligible to the offer, at least a member of the group must be at least 18 years old. If this condition\n",
    "applies, all the under 15 members of the group travel free, and the others pay the Base Price.\n",
    "The function computeFee receives as parameters basePrice (the price of the ticket), n_passengers (the\n",
    "number of passengers of the group), n_over18 (the number of passengers at least 18 old), n_under15 (the\n",
    "number of passengers under 15 years old). It gives as output the amount that the whole group has to spend. It\n",
    "gives an error if groups are composed of more than 5 persons.\n",
    "double computeFee(double basePrice, int n_passengers, int n_over18, int n_under15);\n",
    "\n",
    "- define test cases for this case study in test_cases_02.py\n",
    "- use a file function_02.py to host the generated results for this function\n",
    "\n",
    "### Bike Race\n",
    "\n",
    "In a bike race, the bikers must complete the entire track within a maximum time, otherwise their race is not\n",
    "valid. The maximum time is computed, for each race, based on the winner's time, on the average speed on the\n",
    "track, and on the category of the track.\n",
    "For tracks of category 'A' (easy tracks) the maximum time is computed as the winner's time increased by 5% if\n",
    "the average speed is lower than 30 km/h (30 included), 10% if the average speed is between 30 and 35 km/h (35\n",
    "included), and 15% if the average speed is higher than 35 km/h.\n",
    "For tracks of category 'B' (normal tracks) the maximum time is computed as the winner's time increased by 20%\n",
    "if the average speed is lower than 30 km/h (30 included), 25% if the average speed is between 30 and 35 km/h\n",
    "(35 included), and 30% if the average speed is higher than 35 km/h.\n",
    "For tracks of category 'C' (hard tracks) the maximum time does not depend on average speed, and is always\n",
    "computed as the winner's time increased by 50%.\n",
    "The function computeMaxTime receives as parameters winner_time (the time of the winner, in minutes),\n",
    "avg_speed (the average speed of the track, in km/h) and track_type (a char, whose valid values are 'A', 'B', or\n",
    "'C'). It gives as output the maximum time, 0 if there are errors in the input.\n",
    "double computeMaxTime(double winner_time, double avg_speed, char track_type)\n",
    "\n",
    "- define test cases for this case study in test_cases_03.py\n",
    "- use a file function_03.py to host the generated results for this function\n",
    "\n",
    "Re-execute the analysis once you have your test cases and adapted prompts. What changes baed on the complexity of the requirements to implement?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
