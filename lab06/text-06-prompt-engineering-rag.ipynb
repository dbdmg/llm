{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54Msvwf6mreA"
      },
      "source": [
        "## Exercise 1: Prompt Engineering\n",
        "\n",
        "Let's consider LLAMA as our starting point. In the following, we see a typical prompt feeding and text generation with LLAMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOdvJJ6smreB",
        "outputId": "447b2972-a99c-4fda-d603-331c41ee63d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: System: You are an expert on world capitals.\n",
            "Respond with only the capital city of the given country. Do not repeat the question.\n",
            "\n",
            "Query: What is the capital of France?\n",
            "Answer:\n",
            "Paris\n",
            "\n",
            "Query: What is the capital of the USA?\n",
            "Answer:\n",
            "Washington D.C.\n",
            "\n",
            "Query: What is the capital of China?\n",
            "Answer:\n",
            "Beijing\n",
            "\n",
            "Query: What is the capital of India?\n",
            "Answer:\n",
            "New Delhi\n",
            "\n",
            "Query: What is the capital of Russia?\n",
            "Answer:\n",
            "Moscow\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Assuming model and tokenizer are already loaded\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the device (GPU if available)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# Input prompt - Make it clear that you want only the direct answer without any explanations or options\n",
        "prompt = \"\"\"\n",
        "System: You are an expert on world capitals.\n",
        "Respond with only the capital city of the given country. Do not repeat the question.\n",
        "\n",
        "Query: What is the capital of France?\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "# Generate a response\n",
        "output = model.generate(\n",
        "    inputs['input_ids'],  # Tokenized input\n",
        "    max_length=100,         # Limit response length to avoid extra text\n",
        "    temperature=0.7,        # Lower temperature to reduce randomness\n",
        "    do_sample=True,        # Disable sampling for deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id  # Ensure the model doesn't go beyond the end token\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "# Decode the response into human-readable text\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "answer = response.split(\"query:\")[-1].strip()\n",
        "print(\"Response:\", answer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2ydoTNXmreC"
      },
      "source": [
        "### Fitz\n",
        "\n",
        "Reference libraries to install: pip install openai pymupdf faiss-cpu scikit-learn\n",
        "\n",
        "PyMuPDF is a Python library that provides tools for working with PDF files (as well as other document formats like XPS, OpenXPS, CBZ, EPUB, and FB2). It's built on the MuPDF library, a lightweight, high-performance PDF and XPS rendering engine. With PyMuPDF, you can perform various tasks like reading, creating, editing, and extracting content from PDFs, images, and annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh9xHxqQmreD",
        "outputId": "7a72ae81-222c-46fa-9cc2-942226a504e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.12/dist-packages (1.26.6)\n",
            "Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
            "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
            "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
            "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
            "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
            "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
            "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
            " \n",
            "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
            "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
            "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
            "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
            "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
            "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
            "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
            "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
            " \n",
            "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
            "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
            "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
            "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
            "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
            " \n",
            "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
            "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
            "Club Association. \n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF\n",
        "import fitz\n",
        "\n",
        "#open an example pdf\n",
        "doc = fitz.open(\"example2.pdf\")\n",
        "\n",
        "# Extract text from the first page\n",
        "page = doc.load_page(0)\n",
        "text = page.get_text(\"text\")  # Use 'text' mode to get raw text\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgxfRh3jmreD"
      },
      "source": [
        "### Example: Text Summarization\n",
        "\n",
        "Let's ask LLAMA to perform a summarization of the example PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Y8lsfjmreD",
        "outputId": "43ea1d7e-c82d-496f-939c-8e609cd05591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
            "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
            "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
            "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
            "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
            "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
            "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
            " \n",
            "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
            "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
            "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
            "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
            "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
            "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
            "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
            "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
            " \n",
            "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
            "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
            "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
            "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups,\n"
          ]
        }
      ],
      "source": [
        "#define the prompt to ask for text summarization.\n",
        "text_summarization_prompt = \"You are an expert of text summary. Respond with only the text summary.\"      #define your prompt here\n",
        "text                          #load here the FULL text of the article\n",
        "p1 =  \"\"\"{PROMPT}. article: {BODY}. summary:\"\"\".format(PROMPT=text_summarization_prompt, BODY=text)\n",
        "\n",
        "#feed the prompt to llama\n",
        "#print the result of text summarization into bullets\n",
        "inputs = tokenizer(p1, return_tensors='pt').to(device)\n",
        "output = model.generate(\n",
        "    inputs['input_ids'],  # Tokenized input\n",
        "    max_length=1000,         # Limit response length to avoid extra text\n",
        "    temperature=0.7,        # Lower temperature to reduce randomness\n",
        "    do_sample=True,        # Disable sampling for deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id  # Ensure the model doesn't go beyond the end token\n",
        "\n",
        ")\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "r1 = response.split(\"summary\")[-1].strip()\n",
        "print(r1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv0RPc-8mreD"
      },
      "source": [
        "### Adding a System Prompt\n",
        "\n",
        "Llama was trained with a system message that set the context and persona to assume when solving a task. One of the unsung advantages of open-access models is that you have full control over the system prompt in chat applications. This is essential to specify the behavior of your chat assistant –and even imbue it with some personality–, but it's unreachable in models served behind APIs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp5DiYHNmreD",
        "outputId": "59134bca-ce96-4c00-d440-8977bd349a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Milan is an Italian football club that has won 29 Serie A titles, 5 Coppa Italia titles, \n",
            "7 Supercoppa Italiana titles, 7 European Cup titles, 5 Intercontinental Cups, 2 Latin Cups, a \n",
            "joint record of two UEFA Super Cups and one FIFA Club World Cup. The club has won seven UEFA \n",
            "Champions League titles, making it the competition's second-most successful team behind Real Madrid. \n",
            "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
            "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
            "Club Association.\n"
          ]
        }
      ],
      "source": [
        "#default standard system message from the Hugging Face blog to the prompt from above\n",
        "system_prompt = \"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
        "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
        "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
        "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
        "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
        "    please don't share false information. <</SYS>>\"\n",
        "\n",
        "#concatenate the system prompt with your pront and get the response\n",
        "p2 = system_prompt + \"\\n\" + p1\n",
        "\n",
        "inputs = tokenizer(p2, return_tensors='pt').to(device)\n",
        "output = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    max_length=1000,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "r2 = output.split(\"summary:\")[-1].strip()\n",
        "print(r2)\n",
        "\n",
        "#what changes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mejoL-TcmreE"
      },
      "source": [
        "### Customizing the System prompt\n",
        "\n",
        "With Llama we have full control over the system prompt. The following experiment will instruct Llama to assume the persona of a researcher tasked with writing a concise brief.\n",
        "\n",
        "Apply the following changes the original system prompt:\n",
        "- Use the researcher persona and specify the tasks to summarize articles.\n",
        "- Remove safety instructions; they are unnecessary since we ask Llama to be truthful to the article.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryZn4eRImreE",
        "outputId": "cbedfbc4-9518-45cb-b4a2-f028387c7a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Milan is a professional football club based in Milan, Lombardy, Italy. Founded in 1899, the club competes in the Serie A, the top tier of Italian football. In its early history, Milan played its home games in different grounds around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, with whom they contest the Derby della Madonnina, one of the most followed derbies in football. Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the 1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In international competitions, Milan is\n"
          ]
        }
      ],
      "source": [
        "new_system_prompt = \"<<SYS>> You are a helpful and honest assistant. \\\n",
        "    Always answer as helpfully as possible, while being safe. If a question does not make any sense, or is not factually \\\n",
        "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
        "    please don't share false information. You are an expert of text summary. Respond with only the text summary.<</SYS>>\"\n",
        "\n",
        "p3 = new_system_prompt + \"\\n\" + p1\n",
        "\n",
        "inputs = tokenizer(p3, return_tensors='pt').to(device)\n",
        "output = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    max_length=1000,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "r3 = output.split(\"summary:\")[-1].strip()\n",
        "print(r3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRWkFfj7mreE"
      },
      "source": [
        "### Chain-of-Thought prompting\n",
        "\n",
        "Chain-of-thought is when a prompt is being constructed using a previous prompt answer. For our use case to extract information from text, we will first ask Llama what the article is about and then use the response to ask a second question: what problem does [what the article is about] solve?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5lPiKbvmreE"
      },
      "outputs": [],
      "source": [
        "#define a prompt to ask what the article is about\n",
        "\n",
        "\n",
        "r4 = r3\n",
        "\n",
        "#now embed the result of the previous prompt in a new prompt to ask what that solves\n",
        "\n",
        "p5 = \"\"\n",
        "\n",
        "r5 = \"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78C4hENUmreE"
      },
      "source": [
        "### Generating JSONs with Llama\n",
        "\n",
        "Llama needs precise instructions when asking it to generate JSON. In essence, here is what works for me to get valid JSON consistently:\n",
        "\n",
        "- Explicitly state — “ All output must be in valid JSON. Don’t add explanation beyond the JSON” in the system prompt.\n",
        "- Add an “explanation” variable to the JSON example. Llama enjoys explaining its answers. Give it an outlet.\n",
        "- Use the JSON as part of the instruction. See the “in_less_than_ten_words” example below.\n",
        "Change “write the answer” to “output the answer.”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XSxRENtmreE",
        "outputId": "2a93d601-1c63-4842-bfbd-9cdd2c7d40ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"context\": \"Short summary of the text\",\n",
            "  \"explanation\": \"Brief explanation of the main idea or purpose of the text\"\n",
            "}\n",
            "\n",
            "Input:\n",
            "{\n",
            "  \"article\": {\n",
            "    \"title\": \"This is the title of the article\",\n",
            "    \"url\": \"https://en.wikipedia.org/wiki/This_is_the_title_of_the_article\",\n",
            "    \"excerpt\": \"This is the excerpt of the article\",\n",
            "    \"image\": \"https://en.wikipedia.org/wiki/This_is_the_image_of_the_article\",\n",
            "    \"text\": \"This is the text of the article\"\n",
            "  }\n",
            "}\n",
            "\n",
            "Output:\n",
            "{\n",
            "  \"context\": \"Short summary of the text\",\n",
            "  \"explanation\": \"Brief explanation of the main idea or purpose of the text\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#example addition to a prompt to deal with jsons\n",
        "json_prompt_addition = \"Output must be in valid JSON like the following example {{\\\"topic\\\": topic, \\\"explanation\\\": [in_less_than_ten_words]}}. Output must include only JSON.\"\n",
        "\n",
        "#now generate a prompt by correctly concatenating the system prompt, the json prompt instruction, and an article\n",
        "p6 = \"\"\" You are a summarization model.\n",
        "You will summarize the article.\n",
        "Your task is to generate a concise summary of the article and explain its main idea.\n",
        "\n",
        "Return your answer strictly as a JSON object with the following structure:\n",
        "{\n",
        "  \"context\": \"Short summary of the text\",\n",
        "  \"explanation\": \"Brief explanation of the main idea or purpose of the text\"\n",
        "}\n",
        "\n",
        "Do not include any text, comments, or formatting outside the JSON.\n",
        "\n",
        "article:\n",
        "\"\"\"\n",
        "p6 = p6 + text +\"\\n Output the answer:\"\n",
        "\n",
        "inputs = tokenizer(p6, return_tensors='pt').to(device)\n",
        "output = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    max_length=1000,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "r6 = output.split(\"Output the answer:\")[-1].strip()\n",
        "print(r6)\n",
        "\n",
        "#compare the difference between the prompt with the formatting instruction and a regular prompt without formatting instructions. is there any difference?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4v0BCH0mreE"
      },
      "source": [
        "### One-to-Many Shot Learning Prompting\n",
        "\n",
        "One-to-Many Shot Learning is a term that refers to a type of machine learning problem where the goal is to learn to recognize many different classes of objects from only one or a few examples of each class. For example, if you have only one image of a cat and one image of a dog, can you train a model to distinguish between cats and dogs in new images? This is a challenging problem because the model has to generalize well from minimal data (source)\n",
        "\n",
        "Important points about the prompts:\n",
        "\n",
        "- The system prompt includes the instructions to output the answer in JSON.\n",
        "- The prompt consists of an one-to-many shot learning section that starts after ```<</SYS>>``` and ends with ```</s>```.  See the prompt template below will make it easier to understand.\n",
        "- The examples are given in JSON because the answers need to be JSON.\n",
        "- The JSON allows defining the response with name, type, and explanation.\n",
        "- The prompt question start with the second ```<s>[INST]``` and end with the last ```[/INST]```\n",
        "\n",
        "```\n",
        "<s>[INST] <<SYS>>\n",
        "SYSTEM MESSAGE\n",
        "<</SYS>>\n",
        "EXAMPLE QUESTION [/INST]\n",
        "EXAMPLE ANSWER(S)\n",
        "</s>\n",
        "<s>[INST]  \n",
        "QUESTION\n",
        "[/INST]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSJptcN8mreE"
      },
      "outputs": [],
      "source": [
        "#describe all the main nouns in the example.pdf article\n",
        "\n",
        "#use the following addition for one-to-many prompting exampling\n",
        "nouns = \"\"\"[\\\n",
        "{{\"name\": \"semiconductor\", \"type\": \"industry\", \"explanation\": \"Companies engaged in the design and fabrication of semiconductors and semiconductor devices\"}},\\\n",
        "{{\"name\": \"NBA\", \"type\": \"sport league\", \"explanation\": \"NBA is the national basketball league\"}},\\\n",
        "{{\"name\": \"Ford F150\", \"type\": \"vehicle\", \"explanation\": \"Article talks about the Ford F150 truck\"}},\\\n",
        "{{\"name\": \"Ford\", \"type\": \"company\", \"explanation\": \"Ford is a company that built vehicles\"}},\\\n",
        "{{\"name\": \"John Smith\", \"type\": \"person\", \"explanation\": \"Mentioned in the article\"}},\\\n",
        "]\"\"\"\n",
        "\n",
        "#now build the prompt following the template described above\n",
        "p7 = \"\"\n",
        "\n",
        "r7 = \"\"\n",
        "\n",
        "#compare the response of the prompt described above and a zero-shot prompt. Are there any differences?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5dMRR9HmreE"
      },
      "source": [
        "## Exercise 2: RAG (Retrieval-Augmented-Generation)\n",
        "\n",
        "RAG (Retrieval-Augmented Generation) is a powerful framework in Natural Language Processing (NLP) that enhances the performance of language models by combining traditional generative models with external knowledge retrieval. This hybrid approach allows models to retrieve relevant information from a large corpus (like a database or document collection) and incorporate this information into the generation process. It is particularly useful when a model needs to answer questions, generate content, or provide explanations based on real-time or domain-specific data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiGZLSJ7mreE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "#TODO:  Function to extract text from a PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    print(\"\")\n",
        "    #your code here...\n",
        "\n",
        "# Extract text from all uploaded PDF files\n",
        "pdf_texts = {}\n",
        "# your code here...\n",
        "\n",
        "#Display the text from all the PDF files\n",
        "for pdf_file, text in pdf_texts.items():\n",
        "    print(\"\") #implement PDF read"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zauB9H_UmreF"
      },
      "source": [
        "### Creating an index of vectors to represent the documents\n",
        "\n",
        "To perform efficient searches, we need to convert our text data into numerical vectors. To do so, we will use the first step of the BERT transformer.\n",
        "\n",
        "Since our full pdf files are very long to be fed as input into BERT, we perform a step in which we create a structure where we associate a document number to its abstract, and in a separate dictionary we associate a document number to its full text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuZVO8ECmreF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#import the Bert pretrained model from the transformers library\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "#initialization of the dictionary of abstracts. Substitute this with the abstracts of the 10 papers considered as sources for RAG\n",
        "#(we could use functions to read the PDFs to \"cut\" the abstracts from the papers. For simplicity reasons, we will copy and paste them)\n",
        "abstracts_dict = {\n",
        "    0: \"\"\n",
        "}\n",
        "\n",
        "#the text for rag is used as an input to the BERT model\n",
        "\n",
        "#The tokenized inputs are passed to the BERT model for processing.\n",
        "#(#remember padding=True: Ensures that all inputs are padded to the same length, allowing batch processing.)\n",
        "#The model outputs a tensor (last_hidden_state), where each input token is represented by a high-dimensional vector.\n",
        "#last_hidden_state is of shape (batch_size, sequence_length, hidden_size), where:\n",
        "#batch_size: Number of input texts.\n",
        "#sequence_length: Length of each tokenized text (after padding).\n",
        "#hidden_size: Dimensionality of the vector representation for each token (default 768 for bert-base-uncased).\n",
        "\n",
        "#last_hidden_state[:, 0]: Selects the representation of the [CLS] token for each input text. The [CLS] token is a special token added at the start of each input and is often used as the aggregate representation for the entire sequence.\n",
        "\n",
        "abstract_vectors = \"\"\n",
        "\n",
        "#abstract_vectors is a tensor of shape (batch_size, hidden_size) (e.g., (3, 768) in this case), representing each text as a single 768-dimensional vector.\n",
        "\n",
        "print(abstract_vectors.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dagYbH3nmreF"
      },
      "source": [
        "### Search\n",
        "\n",
        "With our text data vectorized and indexed, we can now perform searches. We will define a function to search the index for the most relevant documents based on a query.\n",
        "\n",
        "To perform the search, we need a function (search documents) where we perform the cosine similarity between the query vector and all the abstract vectors. This function will give our the top-k indexes. Once we find the top-k indexes, with another function, we can collect the full text of the documents from the paper dictionary.\n",
        "\n",
        "To compute cosine similarity, refer to the following formula\n",
        "\n",
        "```cs = cosine_similarity(vector_a.detach().numpy(), vector_b.detach().numpy())```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaADea87mreF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_top_k_similar_indices(query_vector, abstract_vectors, k):\n",
        "\n",
        "    #Computes the top k indices of the most similar abstracts to the query based on cosine similarity.\n",
        "\n",
        "    #Parameters:\n",
        "    #- query_vector: A tensor of shape (1, hidden_size) representing the query vector.\n",
        "    #- abstract_vectors: A tensor of shape (batch_size, hidden_size) representing the abstract vectors.\n",
        "    #- k: The number of top indices to return.\n",
        "\n",
        "    #Returns:\n",
        "    #- sorted_indices: A numpy array of shape (1, k) containing the indices of the top k most similar abstracts.\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def retrieve_documents(indices, documents_dict):\n",
        "\n",
        "    #Retrieves the documents corresponding to the given indices and concatenates them into a single string.\n",
        "\n",
        "    #Parameters:\n",
        "    #- indices: A numpy array or list of top-k indices of the most similar documents.\n",
        "    #- documents_dict: A dictionary where keys are document indices (integers) and values are the document texts (strings).\n",
        "\n",
        "    #Returns:\n",
        "    #- concatenated_documents: A string containing the concatenated texts of the retrieved documents.\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "\n",
        "#now I create a vector also for my query\n",
        "\n",
        "query = \"\"\n",
        "\n",
        "query_vector = \"\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLhs-6A7mreF"
      },
      "source": [
        "### A function to perform Retrieval Augmented Generation\n",
        "\n",
        "In this step, we’ll combine the context retrieved from our documents with LLAMA to generate responses. The context will provide the necessary information to the model to produce more accurate and relevant answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F042qv3hmreF",
        "outputId": "66804a35-dec3-495c-9b73-dbdb766b631d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#now we put it all together\n",
        "\n",
        "def generate_augmented_response(query, documents):\n",
        "\n",
        "    system = \"\"             #TODO: define system prompt\n",
        "\n",
        "    context = \"\"               #TODO: concatenate here all the search results\n",
        "\n",
        "\n",
        "    prompt = \"\"                 #TODO: create the prompt for LLAMA (system + context + query)\n",
        "\n",
        "    response = \"\"\n",
        "\n",
        "    #perform a query with LLAMA in the usual way\n",
        "\n",
        "    #return the response\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# TODO: generate the queries!\n",
        "query = \"\"\n",
        "response = generate_augmented_response(query)\n",
        "print(response)\n",
        "\n",
        "#TODO: now compare the results with a prompt without RAG. What are the results?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}