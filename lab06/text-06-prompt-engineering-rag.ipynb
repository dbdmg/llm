{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Prompt Engineering\n",
    "\n",
    "Let's consider LLAMA as our starting point. In the following, we see a typical prompt feeding and text generation with LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T06:31:00.083824Z",
     "start_time": "2025-11-12T06:27:12.636425Z"
    }
   },
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import datetime\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "device = \"cuda\"\n",
    "\n",
    "query = \"Tell me the capital of France.\"\n",
    "\n",
    "prompt = f\"Question: {query}\\n\\nAnswer:\"  # Create the prompt for LLAMA (context + query)\n",
    "\n",
    "\n",
    "\n",
    "#alternative 1: use the tokenizer and produce the output\n",
    "\n",
    "def make_a_query(prompt, tokenizer, model):\n",
    "    # Set pad_token_id if missing\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Tokenize the input with padding and truncation\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate a response\n",
    "    output = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_new_tokens=200,  # Limit the number of new tokens generated (e.g., a single word)\n",
    "        temperature=0.3,  # Reduce randomness\n",
    "        repetition_penalty=2.0,  # Penalize repetition\n",
    "        no_repeat_ngram_size=3,  # Avoid repeating bigrams\n",
    "        do_sample=False,  # Make the output deterministic (not sampled)\n",
    "        eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n",
    "        pad_token_id=tokenizer.pad_token_id  # Avoid padding tokens\n",
    "    )\n",
    "\n",
    "    # Decode the response into human-readable text\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#alternative 2: use the chat template and pipelines from huggingface\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=200,  # Limit the number of new tokens generated (e.g., a single word)\n",
    "    temperature=0.3,  # Reduce randomness\n",
    "    repetition_penalty=2.0,  # Penalize repetition\n",
    "    no_repeat_ngram_size=3,  # Avoid repeating bigrams\n",
    "    do_sample=False,  # Make the output deterministic (not sampled)\n",
    "    eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n",
    "    pad_token_id=tokenizer.pad_token_id  # Avoid padding tokens\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(make_a_query(prompt, tokenizer, model))\n",
    "\n",
    "print(outputs[0][\"generated_text\"])\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZZARNAUDOA\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B.\n401 Client Error. (Request ID: Root=1-69142992-037645e951bbac3b0d00973c;2ea6fd6a-fcb2-4551-8ad2-6ccee6b55330)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:402\u001B[39m, in \u001B[36mhf_raise_for_status\u001B[39m\u001B[34m(response, endpoint_name)\u001B[39m\n\u001B[32m    401\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m402\u001B[39m     \u001B[43mresponse\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    403\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\requests\\models.py:1026\u001B[39m, in \u001B[36mResponse.raise_for_status\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1025\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response=\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[31mHTTPError\u001B[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mGatedRepoError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:479\u001B[39m, in \u001B[36mcached_files\u001B[39m\u001B[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[39m\n\u001B[32m    477\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(full_filenames) == \u001B[32m1\u001B[39m:\n\u001B[32m    478\u001B[39m     \u001B[38;5;66;03m# This is slightly better for only 1 file\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m479\u001B[39m     \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    480\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    481\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilenames\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    482\u001B[39m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    483\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    484\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    485\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    486\u001B[39m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m=\u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    487\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    488\u001B[39m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    489\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    490\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    491\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    492\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    493\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1007\u001B[39m, in \u001B[36mhf_hub_download\u001B[39m\u001B[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001B[39m\n\u001B[32m   1006\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1007\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1008\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[32m   1009\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1010\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[32m   1011\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1012\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1014\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1015\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[32m   1016\u001B[39m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1017\u001B[39m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1018\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1019\u001B[39m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1020\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1021\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[32m   1022\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1023\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1024\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1114\u001B[39m, in \u001B[36m_hf_hub_download_to_cache_dir\u001B[39m\u001B[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[39m\n\u001B[32m   1113\u001B[39m     \u001B[38;5;66;03m# Otherwise, raise appropriate error\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1114\u001B[39m     \u001B[43m_raise_on_head_call_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhead_call_error\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1116\u001B[39m \u001B[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1655\u001B[39m, in \u001B[36m_raise_on_head_call_error\u001B[39m\u001B[34m(head_call_error, force_download, local_files_only)\u001B[39m\n\u001B[32m   1650\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   1651\u001B[39m     \u001B[38;5;28misinstance\u001B[39m(head_call_error, HfHubHTTPError) \u001B[38;5;129;01mand\u001B[39;00m head_call_error.response.status_code == \u001B[32m401\u001B[39m\n\u001B[32m   1652\u001B[39m ):\n\u001B[32m   1653\u001B[39m     \u001B[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001B[39;00m\n\u001B[32m   1654\u001B[39m     \u001B[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1655\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m head_call_error\n\u001B[32m   1656\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1657\u001B[39m     \u001B[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1543\u001B[39m, in \u001B[36m_get_metadata_or_catch_error\u001B[39m\u001B[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001B[39m\n\u001B[32m   1542\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1543\u001B[39m     metadata = \u001B[43mget_hf_file_metadata\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1544\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mendpoint\u001B[49m\n\u001B[32m   1545\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1546\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1460\u001B[39m, in \u001B[36mget_hf_file_metadata\u001B[39m\u001B[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001B[39m\n\u001B[32m   1459\u001B[39m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1460\u001B[39m r = \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1461\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mHEAD\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1462\u001B[39m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1463\u001B[39m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1464\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1465\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1466\u001B[39m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1467\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1468\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1469\u001B[39m hf_raise_for_status(r)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:283\u001B[39m, in \u001B[36m_request_wrapper\u001B[39m\u001B[34m(method, url, follow_relative_redirects, **params)\u001B[39m\n\u001B[32m    282\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[32m--> \u001B[39m\u001B[32m283\u001B[39m     response = \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    284\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    285\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    286\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    287\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    288\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    290\u001B[39m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[32m    291\u001B[39m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:307\u001B[39m, in \u001B[36m_request_wrapper\u001B[39m\u001B[34m(method, url, follow_relative_redirects, **params)\u001B[39m\n\u001B[32m    306\u001B[39m response = http_backoff(method=method, url=url, **params)\n\u001B[32m--> \u001B[39m\u001B[32m307\u001B[39m \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    308\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:419\u001B[39m, in \u001B[36mhf_raise_for_status\u001B[39m\u001B[34m(response, endpoint_name)\u001B[39m\n\u001B[32m    416\u001B[39m     message = (\n\u001B[32m    417\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse.status_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m Client Error.\u001B[39m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCannot access gated repo for url \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse.url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    418\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m419\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m _format(GatedRepoError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    421\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m error_message == \u001B[33m\"\u001B[39m\u001B[33mAccess to this resource is disabled.\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mGatedRepoError\u001B[39m: 401 Client Error. (Request ID: Root=1-69142992-037645e951bbac3b0d00973c;2ea6fd6a-fcb2-4551-8ad2-6ccee6b55330)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pipeline, AutoTokenizer, AutoModelForCausalLM\n\u001B[32m      6\u001B[39m model_id = \u001B[33m\"\u001B[39m\u001B[33mmeta-llama/Llama-3.2-1B\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat16\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mauto\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m tokenizer = AutoTokenizer.from_pretrained(model_id)\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# Assuming model and tokenizer are already loaded\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:549\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    546\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mquantization_config\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    547\u001B[39m     _ = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mquantization_config\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m549\u001B[39m config, kwargs = \u001B[43mAutoConfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    550\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_unused_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    552\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcode_revision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    553\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    554\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    555\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    556\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    558\u001B[39m \u001B[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001B[39;00m\n\u001B[32m    559\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwargs_orig.get(\u001B[33m\"\u001B[39m\u001B[33mtorch_dtype\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) == \u001B[33m\"\u001B[39m\u001B[33mauto\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1332\u001B[39m, in \u001B[36mAutoConfig.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[39m\n\u001B[32m   1329\u001B[39m trust_remote_code = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mtrust_remote_code\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m   1330\u001B[39m code_revision = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mcode_revision\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m-> \u001B[39m\u001B[32m1332\u001B[39m config_dict, unused_kwargs = \u001B[43mPretrainedConfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_config_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1333\u001B[39m has_remote_code = \u001B[33m\"\u001B[39m\u001B[33mauto_map\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mAutoConfig\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict[\u001B[33m\"\u001B[39m\u001B[33mauto_map\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1334\u001B[39m has_local_code = \u001B[33m\"\u001B[39m\u001B[33mmodel_type\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m config_dict[\u001B[33m\"\u001B[39m\u001B[33mmodel_type\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m CONFIG_MAPPING\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:662\u001B[39m, in \u001B[36mPretrainedConfig.get_config_dict\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[39m\n\u001B[32m    660\u001B[39m original_kwargs = copy.deepcopy(kwargs)\n\u001B[32m    661\u001B[39m \u001B[38;5;66;03m# Get config dict associated with the base config file\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m662\u001B[39m config_dict, kwargs = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_config_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    663\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m config_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    664\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m {}, kwargs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:721\u001B[39m, in \u001B[36mPretrainedConfig._get_config_dict\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[39m\n\u001B[32m    717\u001B[39m configuration_file = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33m_configuration_file\u001B[39m\u001B[33m\"\u001B[39m, CONFIG_NAME) \u001B[38;5;28;01mif\u001B[39;00m gguf_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m gguf_file\n\u001B[32m    719\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    720\u001B[39m     \u001B[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m721\u001B[39m     resolved_config_file = \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    722\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    723\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconfiguration_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    724\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    725\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    726\u001B[39m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    727\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    728\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    729\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    730\u001B[39m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m=\u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    731\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    732\u001B[39m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m=\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    733\u001B[39m \u001B[43m        \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    734\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    735\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m resolved_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    736\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, kwargs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:322\u001B[39m, in \u001B[36mcached_file\u001B[39m\u001B[34m(path_or_repo_id, filename, **kwargs)\u001B[39m\n\u001B[32m    264\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcached_file\u001B[39m(\n\u001B[32m    265\u001B[39m     path_or_repo_id: Union[\u001B[38;5;28mstr\u001B[39m, os.PathLike],\n\u001B[32m    266\u001B[39m     filename: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m    267\u001B[39m     **kwargs,\n\u001B[32m    268\u001B[39m ) -> Optional[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m    269\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    270\u001B[39m \u001B[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001B[39;00m\n\u001B[32m    271\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    320\u001B[39m \u001B[33;03m    ```\u001B[39;00m\n\u001B[32m    321\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m     file = \u001B[43mcached_files\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilenames\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    323\u001B[39m     file = file[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m file\n\u001B[32m    324\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m file\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - Vodafone Group\\Desktop\\PhD\\llm-main\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:543\u001B[39m, in \u001B[36mcached_files\u001B[39m\u001B[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[39m\n\u001B[32m    541\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_gated_repo:\n\u001B[32m    542\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m543\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[32m    544\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mYou are trying to access a gated repo.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mMake sure to have access to it at \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    545\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    546\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    547\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, LocalEntryNotFoundError):\n\u001B[32m    548\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_connection_errors:\n",
      "\u001B[31mOSError\u001B[39m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B.\n401 Client Error. (Request ID: Root=1-69142992-037645e951bbac3b0d00973c;2ea6fd6a-fcb2-4551-8ad2-6ccee6b55330)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-1B is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitz\n",
    "\n",
    "Reference libraries to install: pip install openai pymupdf faiss-cpu scikit-learn\n",
    "\n",
    "PyMuPDF is a Python library that provides tools for working with PDF files (as well as other document formats like XPS, OpenXPS, CBZ, EPUB, and FB2). It's built on the MuPDF library, a lightweight, high-performance PDF and XPS rendering engine. With PyMuPDF, you can perform various tasks like reading, creating, editing, and extracting content from PDFs, images, and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "#open an example pdf\n",
    "doc = fitz.open(\"example.pdf\")\n",
    "\n",
    "# Extract text from the first page\n",
    "page = doc.load_page(0)\n",
    "text = page.get_text(\"text\")  # Use 'text' mode to get raw text\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Text Summarization\n",
    "\n",
    "Let's ask LLAMA to perform a summarization of the example PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the prompt to ask for text summarization. \n",
    "text_summarization_prompt = \"\"      #define your prompt here\n",
    "text = \"\"                           #load here the FULL text of the article\n",
    "p1 =  \"\"\"{PROMPT}. article: {BODY}\"\"\".format(PROMPT=text_summarization_prompt, BODY=text)\n",
    "\n",
    "#feed the prompt to llama\n",
    "#print the result of text summarization into bullets\n",
    "\n",
    "r1 = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a System Prompt\n",
    "\n",
    "Llama was trained with a system message that set the context and persona to assume when solving a task. One of the unsung advantages of open-access models is that you have full control over the system prompt in chat applications. This is essential to specify the behavior of your chat assistant –and even imbue it with some personality–, but it's unreachable in models served behind APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default standard system message from the Hugging Face blog to the prompt from above\n",
    "system_prompt = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \\\n",
    "    You are a helpful, respectful and honest assistant. \\\n",
    "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
    "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
    "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
    "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
    "    please don't share false information. <|eot_id|>\"\n",
    "\n",
    "#concatenate the system prompt with your prompt and get the response\n",
    "p2 = \"<|start_header_id|>user<|end_header_id|> \\\n",
    "        [...] <|eot_id|>\"       # NOTE: various libraries exist for automatically managing the special tokens used as delimiters\n",
    "\n",
    "r2 = \"\"\n",
    "\n",
    "#what changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the System prompt\n",
    "\n",
    "With Llama we have full control over the system prompt. The following experiment will instruct Llama to assume the persona of a researcher tasked with writing a concise brief.\n",
    "\n",
    "Apply the following changes the original system prompt:\n",
    "- Use the researcher persona and specify the tasks to summarize articles.\n",
    "- Remove safety instructions; they are unnecessary since we ask Llama to be truthful to the article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_system_prompt = \"\"\n",
    "\n",
    "p3 = \"\"\n",
    "\n",
    "r3 = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought prompting\n",
    "\n",
    "Chain-of-thought is when a prompt is being constructed using a previous prompt answer. For our use case to extract information from text, we will first ask Llama what the article is about and then use the response to ask a second question: what problem does [what the article is about] solve?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a prompt to ask what the article is about\n",
    "\n",
    "p4 = \"\"\n",
    "\n",
    "r4 = \"\"\n",
    "\n",
    "#now embed the result of the previous prompt in a new prompt to ask what that solves\n",
    "\n",
    "p5 = \"\"\n",
    "\n",
    "r5 = \"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating JSONs with Llama\n",
    "\n",
    "Llama needs precise instructions when asking it to generate JSON. In essence, here is what works for me to get valid JSON consistently:\n",
    "\n",
    "- Explicitly state — “ All output must be in valid JSON. Don’t add explanation beyond the JSON” in the system prompt.\n",
    "- Add an “explanation” variable to the JSON example. Llama enjoys explaining its answers. Give it an outlet.\n",
    "- Use the JSON as part of the instruction. See the “in_less_than_ten_words” example below.\n",
    "Change “write the answer” to “output the answer.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#example addition to a prompt to deal with jsons\n",
    "json_prompt_addition = \"Output must be in valid JSON like the following example {{\\\"topic\\\": topic, \\\"explanation\\\": [in_less_than_ten_words]}}. Output must include only JSON.\"\n",
    "\n",
    "#now generate a prompt by correctly concatenating the system prompt, the json prompt instruction, and an article\n",
    "p6 = \"\"\n",
    "\n",
    "r6 = \"\"\n",
    "\n",
    "#compare the difference between the prompt with the formatting instruction and a regular prompt without formatting instructions. is there any difference?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-Many Shot Learning Prompting\n",
    "\n",
    "One-to-Many Shot Learning is a term that refers to a type of machine learning problem where the goal is to learn to recognize many different classes of objects from only one or a few examples of each class. For example, if you have only one image of a cat and one image of a dog, can you train a model to distinguish between cats and dogs in new images? This is a challenging problem because the model has to generalize well from minimal data (source)\n",
    "\n",
    "Important points about the prompts:\n",
    "\n",
    "- The system prompt includes the instructions to output the answer in JSON.\n",
    "- The prompt consists of an one-to-many shot learning section that starts after the end of the system prompt.\n",
    "- Shot examples are represented through pairs composed by user's question an assistant's response, as reported in the template below.\n",
    "- The examples are given in JSON because the answers need to be JSON.\n",
    "- The JSON allows defining the response with name, type, and explanation.\n",
    "- The prompt question is represented by the last user's question - assistant's response pair, where the response is blank and the last <|eot_id|> is missing.\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Istruzioni generali sul comportamento dell’assistente.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Question shot 1.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Answer shot 1.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Question shot 2.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "Answer shot 2.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Final prompt.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe all the main nouns in the example.pdf article\n",
    "\n",
    "#use the following addition for one-to-many prompting exampling\n",
    "nouns = \"\"\"[\\\n",
    "{{\"name\": \"semiconductor\", \"type\": \"industry\", \"explanation\": \"Companies engaged in the design and fabrication of semiconductors and semiconductor devices\"}},\\\n",
    "{{\"name\": \"NBA\", \"type\": \"sport league\", \"explanation\": \"NBA is the national basketball league\"}},\\\n",
    "{{\"name\": \"Ford F150\", \"type\": \"vehicle\", \"explanation\": \"Article talks about the Ford F150 truck\"}},\\\n",
    "{{\"name\": \"Ford\", \"type\": \"company\", \"explanation\": \"Ford is a company that built vehicles\"}},\\\n",
    "{{\"name\": \"John Smith\", \"type\": \"person\", \"explanation\": \"Mentioned in the article\"}},\\\n",
    "]\"\"\"\n",
    "\n",
    "#now build the prompt following the template described above\n",
    "p7 = \"\"\n",
    "\n",
    "r7 = \"\"\n",
    "\n",
    "#compare the response of the prompt described above and a zero-shot prompt. Are there any differences?\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 2: RAG (Retrieval-Augmented-Generation)\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is a powerful framework in Natural Language Processing (NLP) that enhances the performance of language models by combining traditional generative models with external knowledge retrieval. This hybrid approach allows models to retrieve relevant information from a large corpus (like a database or document collection) and incorporate this information into the generation process. It is particularly useful when a model needs to answer questions, generate content, or provide explanations based on real-time or domain-specific data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "#TODO:  Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"\")\n",
    "    #your code here...\n",
    "    \n",
    "# Extract text from all uploaded PDF files\n",
    "pdf_texts = {}\n",
    "# your code here...\n",
    "\n",
    "#Display the text from all the PDF files\n",
    "for pdf_file, text in pdf_texts.items(): \n",
    "    print(\"\") #implement PDF read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an index of vectors to represent the documents\n",
    "\n",
    "To perform efficient searches, we need to convert our text data into numerical vectors. To do so, we will use the first step of the BERT transformer.\n",
    "\n",
    "Since our full pdf files are very long to be fed as input into BERT, we perform a step in which we create a structure where we associate a document number to its abstract, and in a separate dictionary we associate a document number to its full text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import the Bert pretrained model from the transformers library\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "### Don't forget to move it to the device\n",
    "\n",
    "#initialization of the dictionary of abstracts. Substitute this with the abstracts of the 10 papers considered as sources for RAG\n",
    "#(we could use functions to read the PDFs to \"cut\" the abstracts from the papers. For simplicity reasons, we will copy and paste them)\n",
    "abstracts_dict = {\n",
    "    0: \"\"\n",
    "}\n",
    "\n",
    "#the text for rag is used as an input to the BERT model\n",
    "\n",
    "#The tokenized inputs are passed to the BERT model for processing.\n",
    "#(#remember padding=True: Ensures that all inputs are padded to the same length, allowing batch processing.)\n",
    "#The model outputs a tensor (last_hidden_state), where each input token is represented by a high-dimensional vector.\n",
    "#last_hidden_state is of shape (batch_size, sequence_length, hidden_size), where:\n",
    "#batch_size: Number of input texts.\n",
    "#sequence_length: Length of each tokenized text (after padding).\n",
    "#hidden_size: Dimensionality of the vector representation for each token (default 768 for bert-base-uncased).\n",
    "\n",
    "#last_hidden_state[:, 0]: Selects the representation of the [CLS] token for each input text. The [CLS] token is a special token added at the start of each input and is often used as the aggregate representation for the entire sequence.\n",
    "\n",
    "# -------------------------------------- HINT -----------------------------------\n",
    "tokenized_inputs = tokenizer(\n",
    "    list(abstracts_dict.values()),\n",
    "    return_tensors='pt', # stands for 'PyTorch': tensors will be returned (if omitted, we will get Python lists)\n",
    "    padding=True)   # all the abstracts will be padded to the same length, in order to have a uniformed batch of abstracts => an attention mask will be created\n",
    "tokenized_inputs = {key: value.to(device) for key, value in tokenized_inputs.items()}   # move the tensors to the device\n",
    "\n",
    "abstract_vectors = model_indexing(**tokenized_inputs).last_hidden_state[:, 0]\n",
    "\n",
    "#abstract_vectors is a tensor of shape (batch_size, hidden_size) (e.g., (3, 768) in this case), representing each text as a single 768-dimensional vector.\n",
    "\n",
    "print(abstract_vectors.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n",
    "\n",
    "With our text data vectorized and indexed, we can now perform searches. We will define a function to search the index for the most relevant documents based on a query.\n",
    "\n",
    "To perform the search, we need a function (search documents) where we perform the cosine similarity between the query vector and all the abstract vectors. This function will give our the top-k indexes. Once we find the top-k indexes, with another function, we can collect the full text of the documents from the paper dictionary.\n",
    "\n",
    "To compute cosine similarity, refer to the following formula\n",
    "\n",
    "```cs = cosine_similarity(vector_a.detach().numpy(), vector_b.detach().numpy())```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_top_k_similar_indices(query_vector,         # i.e., the QUERY in the RAG system\n",
    "                              abstract_vectors,     # i.e., the KEY in the RAG system\n",
    "                              k):\n",
    "    \n",
    "    #Parameters:\n",
    "    #- query_vector: A tensor of shape (1, hidden_size) representing the query vector.\n",
    "    #- abstract_vectors: A tensor of shape (batch_size, hidden_size) representing the abstract vectors.\n",
    "    #- k: The number of top indices to return.\n",
    "    \n",
    "    #Returns:\n",
    "    #- sorted_indices: A numpy array of shape (1, k) containing the indices of the top k most similar abstracts.\n",
    "\n",
    "    # ------------- HINT ----------------------------------------------------------------\n",
    "    #Computes the top k indices of the most similar abstracts to the query based on cosine similarity.\n",
    "    similarities = cosine_similarity(   # from sklearn.metrics.pairwise\n",
    "        query_vector.cpu().detach().numpy(),    # it is needed to move the tensor on the cpu!\n",
    "        abstract_vectors.cpu().detach().numpy())\n",
    "\n",
    "    # IMPORTANT: reason about the size of 'similarities' to understand how to sort it (you can print it)\n",
    "    # ------------- HINT ----------------------------------------------------------------\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def retrieve_documents(indices, documents_dict):\n",
    "    \n",
    "    #Retrieves the documents corresponding to the given indices and concatenates them into a single string.\n",
    "    \n",
    "    #Parameters:\n",
    "    #- indices: A numpy array or list of top-k indices of the most similar documents.\n",
    "    #- documents_dict: A dictionary where keys are document indices (integers) and values are the document texts (strings).\n",
    "    \n",
    "    #Returns:\n",
    "    #- concatenated_documents: A string containing the concatenated texts of the retrieved documents.\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "\n",
    "#now I create a vector also for my query \n",
    "\n",
    "# 1) from NL to tokens\n",
    "\n",
    "query = \"\"  # remember to move the tokenised input on the correct device (the same as the model)\n",
    "\n",
    "# 2) from tokens to vector (i.e., embedding)\n",
    "\n",
    "query_vector = \"\"\n",
    "\n",
    "# TODO: get the top k abstracts similar to the query ( call get_top_k_similar_indices() )\n",
    "\n",
    "# TODO: get the relative texts ( call retrieve_documents() )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to perform Retrieval Augmented Generation\n",
    "\n",
    "In this step, we’ll combine the context retrieved from our documents with LLAMA to generate responses. The context will provide the necessary information to the model to produce more accurate and relevant answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#now we put it all together\n",
    "\n",
    "def generate_augmented_response(query, documents):\n",
    "\n",
    "    system = \"\"             #TODO: define system prompt\n",
    "\n",
    "    context = \"\"               #TODO: concatenate here all the search results\n",
    "\n",
    "    \n",
    "    prompt = \"\"                 #TODO: create the prompt for LLAMA (system + context + query)\n",
    "\n",
    "    response = \"\"\n",
    "\n",
    "    #perform a query with LLAMA in the usual way ( call make_a_query() )\n",
    "    \n",
    "    #return the response\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# TODO: generate the queries!\n",
    "query = \"\"\n",
    "response = generate_augmented_response(query)\n",
    "print(response)\n",
    "\n",
    "#TODO: now compare the results with a prompt without RAG. What are the results?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
