{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54Msvwf6mreA"
      },
      "source": [
        "## Exercise 1: Prompt Engineering\n",
        "\n",
        "Let's consider LLAMA as our starting point. In the following, we see a typical prompt feeding and text generation with LLAMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOdvJJ6smreB",
        "outputId": "447b2972-a99c-4fda-d603-331c41ee63d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: System: You are an expert on world capitals.\n",
            "Respond with only the capital city of the given country. Do not repeat the question.\n",
            "\n",
            "Query: What is the capital of France?\n",
            "Answer:\n",
            "Paris\n",
            "\n",
            "Query: What is the capital of the USA?\n",
            "Answer:\n",
            "Washington D.C.\n",
            "\n",
            "Query: What is the capital of China?\n",
            "Answer:\n",
            "Beijing\n",
            "\n",
            "Query: What is the capital of India?\n",
            "Answer:\n",
            "New Delhi\n",
            "\n",
            "Query: What is the capital of Russia?\n",
            "Answer:\n",
            "Moscow\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Assuming model and tokenizer are already loaded\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the device (GPU if available)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# Input prompt - Make it clear that you want only the direct answer without any explanations or options\n",
        "prompt = \"\"\"\n",
        "System: You are an expert on world capitals.\n",
        "Respond with only the capital city of the given country. Do not repeat the question.\n",
        "\n",
        "Query: What is the capital of France?\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "# Generate a response\n",
        "output = model.generate(\n",
        "    inputs['input_ids'],  # Tokenized input\n",
        "    max_length=100,         # Limit response length to avoid extra text\n",
        "    temperature=0.7,        # Lower temperature to reduce randomness\n",
        "    do_sample=True,        # Disable sampling for deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id  # Ensure the model doesn't go beyond the end token\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "# Decode the response into human-readable text\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "answer = response.split(\"query:\")[-1].strip()\n",
        "print(\"Response:\", answer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2ydoTNXmreC"
      },
      "source": [
        "### Fitz\n",
        "\n",
        "Reference libraries to install: pip install openai pymupdf faiss-cpu scikit-learn\n",
        "\n",
        "PyMuPDF is a Python library that provides tools for working with PDF files (as well as other document formats like XPS, OpenXPS, CBZ, EPUB, and FB2). It's built on the MuPDF library, a lightweight, high-performance PDF and XPS rendering engine. With PyMuPDF, you can perform various tasks like reading, creating, editing, and extracting content from PDFs, images, and annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh9xHxqQmreD",
        "outputId": "7a72ae81-222c-46fa-9cc2-942226a504e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.12/dist-packages (1.26.6)\n",
            "Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
            "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
            "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
            "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
            "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
            "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
            "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
            " \n",
            "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
            "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
            "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
            "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
            "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
            "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
            "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
            "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
            " \n",
            "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
            "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
            "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
            "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
            "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
            " \n",
            "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
            "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
            "Club Association. \n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF\n",
        "import fitz\n",
        "\n",
        "#open an example pdf\n",
        "doc = fitz.open(\"example2.pdf\")\n",
        "\n",
        "# Extract text from the first page\n",
        "page = doc.load_page(0)\n",
        "text = page.get_text(\"text\")  # Use 'text' mode to get raw text\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgxfRh3jmreD"
      },
      "source": [
        "### Example: Text Summarization\n",
        "\n",
        "Let's ask LLAMA to perform a summarization of the example PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Y8lsfjmreD",
        "outputId": "43ea1d7e-c82d-496f-939c-8e609cd05591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
            "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
            "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
            "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
            "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
            "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
            "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
            " \n",
            "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
            "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
            "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
            "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
            "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
            "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
            "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
            "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
            " \n",
            "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
            "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
            "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
            "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups,\n"
          ]
        }
      ],
      "source": [
        "#define the prompt to ask for text summarization.\n",
        "text_summarization_prompt = \"You are an expert of text summary. Respond with only the text summary.\"      #define your prompt here\n",
        "text                          #load here the FULL text of the article\n",
        "p1 =  \"\"\"{PROMPT}. article: {BODY}. summary:\"\"\".format(PROMPT=text_summarization_prompt, BODY=text)\n",
        "\n",
        "#feed the prompt to llama\n",
        "#print the result of text summarization into bullets\n",
        "inputs = tokenizer(p1, return_tensors='pt').to(device)\n",
        "output = model.generate(\n",
        "    inputs['input_ids'],  # Tokenized input\n",
        "    max_length=1000,         # Limit response length to avoid extra text\n",
        "    temperature=0.7,        # Lower temperature to reduce randomness\n",
        "    do_sample=True,        # Disable sampling for deterministic output\n",
        "    pad_token_id=tokenizer.eos_token_id  # Ensure the model doesn't go beyond the end token\n",
        "\n",
        ")\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "r1 = response.split(\"summary\")[-1].strip()\n",
        "print(r1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv0RPc-8mreD"
      },
      "source": [
        "### Adding a System Prompt\n",
        "\n",
        "Llama was trained with a system message that set the context and persona to assume when solving a task. One of the unsung advantages of open-access models is that you have full control over the system prompt in chat applications. This is essential to specify the behavior of your chat assistant –and even imbue it with some personality–, but it's unreachable in models served behind APIs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp5DiYHNmreD",
        "outputId": "59134bca-ce96-4c00-d440-8977bd349a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Milan is an Italian football club that has won 29 Serie A titles, 5 Coppa Italia titles, \n",
            "7 Supercoppa Italiana titles, 7 European Cup titles, 5 Intercontinental Cups, 2 Latin Cups, a \n",
            "joint record of two UEFA Super Cups and one FIFA Club World Cup. The club has won seven UEFA \n",
            "Champions League titles, making it the competition's second-most successful team behind Real Madrid. \n",
            "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
            "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
            "Club Association.\n"
          ]
        }
      ],
      "source": [
        "#default standard system message from the Hugging Face blog to the prompt from above\n",
        "system_prompt = \"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
        "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
        "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
        "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
        "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
        "    please don't share false information. <</SYS>>\"\n",
        "\n",
        "#concatenate the system prompt with your pront and get the response\n",
        "p2 = system_prompt + \"\\n\" + p1\n",
        "\n",
        "inputs = tokenizer(p2, return_tensors='pt').to(device)\n",
        "output = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    max_length=1000,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "r2 = output.split(\"summary:\")[-1].strip()\n",
        "print(r2)\n",
        "\n",
        "#what changes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mejoL-TcmreE"
      },
      "source": [
        "### Customizing the System prompt\n",
        "\n",
        "With Llama we have full control over the system prompt. The following experiment will instruct Llama to assume the persona of a researcher tasked with writing a concise brief.\n",
        "\n",
        "Apply the following changes the original system prompt:\n",
        "- Use the researcher persona and specify the tasks to summarize articles.\n",
        "- Remove safety instructions; they are unnecessary since we ask Llama to be truthful to the article.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryZn4eRImreE",
        "outputId": "cbedfbc4-9518-45cb-b4a2-f028387c7a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Milan is a professional football club based in Milan, Lombardy, Italy. Founded in 1899, the club competes in the Serie A, the top tier of Italian football. In its early history, Milan played its home games in different grounds around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, with whom they contest the Derby della Madonnina, one of the most followed derbies in football. Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the 1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In international competitions, Milan is\n"
          ]
        }
      ],
      "source": [
        "new_system_prompt = \"<<SYS>> You are a helpful and honest assistant. \\\n",
        "    Always answer as helpfully as possible, while being safe. If a question does not make any sense, or is not factually \\\n",
        "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
        "    please don't share false information. You are an expert of text summary. Respond with only the text summary.<</SYS>>\"\n",
        "\n",
        "p3 = new_system_prompt + \"\\n\" + p1\n",
        "\n",
        "inputs = tokenizer(p3, return_tensors='pt').to(device)\n",
        "output = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    max_length=1000,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "r3 = output.split(\"summary:\")[-1].strip()\n",
        "print(r3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRWkFfj7mreE"
      },
      "source": [
        "### Chain-of-Thought prompting\n",
        "\n",
        "Chain-of-thought is when a prompt is being constructed using a previous prompt answer. For our use case to extract information from text, we will first ask Llama what the article is about and then use the response to ask a second question: what problem does [what the article is about] solve?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5lPiKbvmreE"
      },
      "outputs": [],
      "source": [
        "#define a prompt to ask what the article is about\n",
        "\n",
        "\n",
        "r4 = r3\n",
        "\n",
        "#now embed the result of the previous prompt in a new prompt to ask what that solves\n",
        "\n",
        "p5 = \"\"\n",
        "\n",
        "r5 = \"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78C4hENUmreE"
      },
      "source": [
        "### Generating JSONs with Llama\n",
        "\n",
        "Llama needs precise instructions when asking it to generate JSON. In essence, here is what works for me to get valid JSON consistently:\n",
        "\n",
        "- Explicitly state — “ All output must be in valid JSON. Don’t add explanation beyond the JSON” in the system prompt.\n",
        "- Add an “explanation” variable to the JSON example. Llama enjoys explaining its answers. Give it an outlet.\n",
        "- Use the JSON as part of the instruction. See the “in_less_than_ten_words” example below.\n",
        "Change “write the answer” to “output the answer.”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XSxRENtmreE",
        "outputId": "2a93d601-1c63-4842-bfbd-9cdd2c7d40ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"context\": \"Short summary of the text\",\n",
            "  \"explanation\": \"Brief explanation of the main idea or purpose of the text\"\n",
            "}\n",
            "\n",
            "Input:\n",
            "{\n",
            "  \"article\": {\n",
            "    \"title\": \"This is the title of the article\",\n",
            "    \"url\": \"https://en.wikipedia.org/wiki/This_is_the_title_of_the_article\",\n",
            "    \"excerpt\": \"This is the excerpt of the article\",\n",
            "    \"image\": \"https://en.wikipedia.org/wiki/This_is_the_image_of_the_article\",\n",
            "    \"text\": \"This is the text of the article\"\n",
            "  }\n",
            "}\n",
            "\n",
            "Output:\n",
            "{\n",
            "  \"context\": \"Short summary of the text\",\n",
            "  \"explanation\": \"Brief explanation of the main idea or purpose of the text\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#example addition to a prompt to deal with jsons\n",
        "json_prompt_addition = \"Output must be in valid JSON like the following example {{\\\"topic\\\": topic, \\\"explanation\\\": [in_less_than_ten_words]}}. Output must include only JSON.\"\n",
        "\n",
        "#now generate a prompt by correctly concatenating the system prompt, the json prompt instruction, and an article\n",
        "p6 = \"\"\" You are a summarization model.\n",
        "You will summarize the article.\n",
        "Your task is to generate a concise summary of the article and explain its main idea.\n",
        "\n",
        "Return your answer strictly as a JSON object with the following structure:\n",
        "{\n",
        "  \"context\": \"Short summary of the text\",\n",
        "  \"explanation\": \"Brief explanation of the main idea or purpose of the text\"\n",
        "}\n",
        "\n",
        "Do not include any text, comments, or formatting outside the JSON.\n",
        "\n",
        "article:\n",
        "\"\"\"\n",
        "p6 = p6 + text +\"\\n Output the answer:\"\n",
        "\n",
        "inputs = tokenizer(p6, return_tensors='pt').to(device)\n",
        "output = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    max_length=1000,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "r6 = output.split(\"Output the answer:\")[-1].strip()\n",
        "print(r6)\n",
        "\n",
        "#compare the difference between the prompt with the formatting instruction and a regular prompt without formatting instructions. is there any difference?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4v0BCH0mreE"
      },
      "source": [
        "### One-to-Many Shot Learning Prompting\n",
        "\n",
        "One-to-Many Shot Learning is a term that refers to a type of machine learning problem where the goal is to learn to recognize many different classes of objects from only one or a few examples of each class. For example, if you have only one image of a cat and one image of a dog, can you train a model to distinguish between cats and dogs in new images? This is a challenging problem because the model has to generalize well from minimal data (source)\n",
        "\n",
        "Important points about the prompts:\n",
        "\n",
        "- The system prompt includes the instructions to output the answer in JSON.\n",
        "- The prompt consists of an one-to-many shot learning section that starts after ```<</SYS>>``` and ends with ```</s>```.  See the prompt template below will make it easier to understand.\n",
        "- The examples are given in JSON because the answers need to be JSON.\n",
        "- The JSON allows defining the response with name, type, and explanation.\n",
        "- The prompt question start with the second ```<s>[INST]``` and end with the last ```[/INST]```\n",
        "\n",
        "```\n",
        "<s>[INST] <<SYS>>\n",
        "SYSTEM MESSAGE\n",
        "<</SYS>>\n",
        "EXAMPLE QUESTION [/INST]\n",
        "EXAMPLE ANSWER(S)\n",
        "</s>\n",
        "<s>[INST]  \n",
        "QUESTION\n",
        "[/INST]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSJptcN8mreE"
      },
      "outputs": [],
      "source": [
        "#describe all the main nouns in the example.pdf article\n",
        "\n",
        "#use the following addition for one-to-many prompting exampling\n",
        "nouns = \"\"\"[\\\n",
        "{{\"name\": \"semiconductor\", \"type\": \"industry\", \"explanation\": \"Companies engaged in the design and fabrication of semiconductors and semiconductor devices\"}},\\\n",
        "{{\"name\": \"NBA\", \"type\": \"sport league\", \"explanation\": \"NBA is the national basketball league\"}},\\\n",
        "{{\"name\": \"Ford F150\", \"type\": \"vehicle\", \"explanation\": \"Article talks about the Ford F150 truck\"}},\\\n",
        "{{\"name\": \"Ford\", \"type\": \"company\", \"explanation\": \"Ford is a company that built vehicles\"}},\\\n",
        "{{\"name\": \"John Smith\", \"type\": \"person\", \"explanation\": \"Mentioned in the article\"}},\\\n",
        "]\"\"\"\n",
        "\n",
        "#now build the prompt following the template described above\n",
        "p7 = \"\"\n",
        "\n",
        "r7 = \"\"\n",
        "\n",
        "#compare the response of the prompt described above and a zero-shot prompt. Are there any differences?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5dMRR9HmreE"
      },
      "source": [
        "## Exercise 2: RAG (Retrieval-Augmented-Generation)\n",
        "\n",
        "RAG (Retrieval-Augmented Generation) is a powerful framework in Natural Language Processing (NLP) that enhances the performance of language models by combining traditional generative models with external knowledge retrieval. This hybrid approach allows models to retrieve relevant information from a large corpus (like a database or document collection) and incorporate this information into the generation process. It is particularly useful when a model needs to answer questions, generate content, or provide explanations based on real-time or domain-specific data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RiGZLSJ7mreE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "15066af9-6366-47ca-e1a2-1f118cd475af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "pivotal workload in various applications. Today, LLM inference\n",
            "clusters receive a large number of queries with strict Service\n",
            "Level Objectives (SLOs). To achieve the desired performance,\n",
            "these models execute on power-hungry GPUs causing the in-\n",
            "ference clusters to consume large amount of energy and, conse-\n",
            "quently, result in excessive carbon emissions. Fortunately, we find\n",
            "that there is a great opportunity to exploit the heterogeneity in\n",
            "inference compute properties and fluctuations in inference work-\n",
            "loads, to significantly improve energy-efficiency. However, such\n",
            "a diverse and dynamic environment creates a large search-space\n",
            "where different system configurations (e.g., number of instances,\n",
            "model parallelism, and GPU frequency) translate into different\n",
            "energy-performance trade-offs. To address these challenges, we\n",
            "propose DynamoLLM, the first energy-management framework\n",
            "for LLM inference environments. DynamoLLM automatically\n",
            "and dynamically reconfigures the inference cluster to optimize for\n",
            "energy and cost of LLM serving under the service’s performance\n",
            "SLOs. We show that at a service-level, DynamoLLM conserves\n",
            "53% energy and 38% operational carbon emissions, and reduces\n",
            "61% cost to the customer, while meeting the latency SLOs.\n",
            "I. INTRODUCTION\n",
            "The exponential growth in the adoption of generative large\n",
            "language models (LLMs) has positioned them at the core\n",
            "of numerous technological advancements and applications.\n",
            "Today, we see use-cases of LLMs in various domains, such\n",
            "as healthcare [52], developer productivity [13], data analyt-\n",
            "ics [68], education [5] and other. As the popularity of LLMs\n",
            "increases among users, the inference clusters receive millions\n",
            "of queries per day [27] resulting in large infrastructures with\n",
            "sophisticated software and expensive hardware systems.\n",
            "To meet these ever increasing computing demands, re-\n",
            "searchers proposed various software [9], [26], [35], [73], [81]\n",
            "and hardware [4], [50], [78] techniques. Such techniques\n",
            "improve the performance efficiency of LLM inference clusters.\n",
            "However, one aspect that has been largely overlooked is the\n",
            "energy consumption of these environments [58], [60]. The\n",
            "substantial energy requirements of serving LLMs running on\n",
            "power-hungry GPUs have emerged as a significant concern. As\n",
            "these models become integral to various services, minimizing\n",
            "their energy consumption and, consequently, carbon emissions\n",
            "while maintaining high performance is paramount.\n",
            "To address this gap, this paper starts by characterizing the\n",
            "energy-efficiency properties of LLM inference workloads. Our\n",
            "characterization underscores that such environments present\n",
            "a distinct set of challenges, divergent from existing energy\n",
            "management schemes tailored for traditional datacenters appli-\n",
            "cations [7], [17], [21], [31], [61], [80]. Specifically, we observe\n",
            "that heterogeneity in LLM inference compute properties and\n",
            "fluctuations in LLM inference workloads create a dynamic en-\n",
            "vironment with large variations. Such variations arise from: (1)\n",
            "requests with varying input/output token lengths, (2) distinct\n",
            "compute properties of different LLMs, and (3) different SLOs\n",
            "required by the services using an LLM.\n",
            "Requests with a large number of input tokens are compute\n",
            "intensive, thus, sensitive to GPU frequency. Conversely, re-\n",
            "quests with a few input tokens and many output tokens have\n",
            "low compute, but high memory requirements. Reducing their\n",
            "GPU frequency would save the energy without significantly\n",
            "impacting the performance. Moreover, the number of model\n",
            "parameters also affects the LLM’s sensitivity to the number of\n",
            "GPUs and GPU frequency. Finally, depending on the service\n",
            "currently using the LLM, the SLO requirements can be strict\n",
            "requiring high-performance configurations, or loose allowing\n",
            "for lower-performance but more energy-efficient configura-\n",
            "tions. Importantly, these characteristics rapidly change due to\n",
            "load fluctuations and dynamic distributions of requests. Such\n",
            "dynamic changes cause a system configuration that is energy-\n",
            "efficient at a given point, to quickly become sub-optimal. This\n",
            "requires a dynamic approach to resource management.\n",
            "To pave the way towards energy-efficient and sustainable\n",
            "LLM inference clusters, this paper introduces DynamoLLM,\n",
            "the first energy-management framework for LLM inference\n",
            "environments. DynamoLLM exploits the unique properties of\n",
            "LLM inference workloads to reduce their energy consump-\n",
            "tion while meeting the performance SLOs. The system uses\n",
            "energy-performance profiles of models and their workloads\n",
            "to automatically and dynamically select the energy-efficient\n",
            "configuration. It leverages multiple knobs, including scaling\n",
            "in/out the number of server instances, model parallelism across\n",
            "GPUs, and GPU frequency scaling.\n",
            "To handle workload heterogeneity, DynamoLLM maintains\n",
            "differently configured pools of LLM instances that are op-\n",
            "timal for different types of incoming requests. For instance,\n",
            "compared to a request with many input and output tokens,\n",
            "a request that processes and outputs fewer tokens runs more\n",
            "efficiently on a model parallelized across fewer GPUs running\n",
            "at a lower frequency. As request distribution varies over\n",
            "time, DynamoLLM dynamically sizes the pools. These pools\n",
            "can be merged into fewer pools or divided into multiple\n",
            "pools over time, providing a balance between right-sizing\n",
            "and fragmentation of resources. To efficiently manage the\n",
            "1\n",
            "arXiv:2408.00741v1  [cs.AI]  1 Aug 2024\n",
            "resources, DynamoLLM uses a hierarchy of controllers that\n",
            "reduces computation complexity and eliminates centralized\n",
            "bottlenecks. The controller at each level operates under the\n",
            "conditions imposed by the upper level, computes its dedicated\n",
            "knob, and forwards further constraints to the controllers at a\n",
            "lower level. Finally, to enable frequent and smooth transition\n",
            "across different configurations, DynamoLLM includes tech-\n",
            "niques to minimize or hide the reconfiguration overheads. As\n",
            "a result, the system maintains high levels of efficiency and\n",
            "service quality under changing workload demands.\n",
            "An evaluation of DynamoLLM with a large GPU cluster\n",
            "running production-level traces from a major cloud provider\n",
            "shows that DynamoLLM is very effective: it conserves 53%\n",
            "energy, 38% operational carbon emissions, and reduces 61%\n",
            "cost to the customer, while meeting the latency SLOs.\n",
            "The contributions of this paper are as follows:\n",
            "• An analysis of the opportunities for energy-efficient LLM\n",
            "serving, rooting in the heterogeneity and fluctuations within\n",
            "the inference workloads.\n",
            "• Design and implementation of DynamoLLM, a high perfor-\n",
            "mance and energy-optimized framework for LLM inference.\n",
            "• An evaluation of DynamoLLM on a large-scale platform\n",
            "using production-level traces.\n",
            "II. BACKGROUND\n",
            "Computational phases of LLMs Generative LLMs [34], [38],\n",
            "[56], [67], [80] are auto-regressive: they process the whole\n",
            "input in parallel, and serially generate the output tokens. This\n",
            "property leads to two computationally distinct phases [49],\n",
            "[50]. First is the prefill phase, where the input tokens are\n",
            "computed in parallel. This is a compute-intensive phase and\n",
            "scales with the number of input tokens. Second is the decode\n",
            "phase, where each output token is generated serially, based on\n",
            "all the tokens seen so far. This is a memory-intensive phase,\n",
            "and scales with the number of output tokens.\n",
            "Performance metrics for LLMs To evaluate the performance,\n",
            "we use: time to first token (TTFT), time between tokens\n",
            "(TBT), and throughput\n",
            "[50], [63]. TTFT is the latency of\n",
            "generating the first output token; while TBT is the latency\n",
            "to generate each new output token. To quantify the energy\n",
            "efficiency, we measure the energy consumption in Watt-hours\n",
            "(Wh) while meeting certain latency SLOs. The SLOs vary\n",
            "depending on their use cases for different tasks. For latency-\n",
            "sensitive tasks, both TTFT and TBT are important metrics\n",
            "with strict SLOs. We define SLOs for TTFT and TBT based\n",
            "on maximum achievable performance, described in Table IV.\n",
            "LLM parallelism A single model can be divided across GPUs\n",
            "to improve performance and allow larger memory footprints.\n",
            "LLM inference typically uses pipeline and tensor parallelism.\n",
            "Pipeline parallelism (PP) partitions the LLM layers among\n",
            "GPUs, while keeping all the operators/tensors of a layer on\n",
            "the GPU. GPUs then communicate only in between two\n",
            "consecutive stages. Tensor parallelism (TP) allocates a slice\n",
            "of each layer to each GPU. This requires aggregation across\n",
            "all the GPU for each layer, in turn needing high bandwidth\n",
            "communication. TP performs better for GPUs within the same\n",
            "server, connected with high bandwidth interconnects (e.g.,\n",
            "NVLink [45]), while PP is preferred across servers. Since most\n",
            "open source models [34], [38], [67] fit on 8 GPUs in a single\n",
            "server, we consider only TP in the rest of the paper; the ideas\n",
            "can easily extend to PP. We denote tensor parallelism across\n",
            "2, 4 and 8 GPUs as TP2, TP4 and TP8, respectively.\n",
            "Power and energy in datacenters A rich body of work\n",
            "explored power/energy efficiency in traditional datacenters [7],\n",
            "[29], [31], [62]. However, the rapid growth of LLMs has posed\n",
            "new challenges that have not yet been extensively studied.\n",
            "LLM inference workloads comprise a swiftly increasing per-\n",
            "centage of datacenter load [49]. This, coupled with the power-\n",
            "dense hardware like DGX A100s and H100s being deployed\n",
            "to serve these workloads makes them power, energy, and\n",
            "carbon-intensive [12], [49], [58]. To effectively address this\n",
            "challenge, it is important to have a comprehensive framework\n",
            "for managing energy in these systems.\n",
            "III. OPPORTUNITIES FOR ENERGY EFFICIENCY\n",
            "To understand the energy-efficiency properties of LLM\n",
            "inference environments, we characterize open-source mod-\n",
            "els [33], [38], [39], [66] on an NVIDIA DGX H100 server [44]\n",
            "using vLLM [26] inference engine. We analyze the energy\n",
            "properties of LLMs by varying the request lengths, request\n",
            "load, model, and service SLO. Additionally, we analyze how\n",
            "the profiled variables change over time in a real-production\n",
            "environment using the invocation traces of two LLM services\n",
            "from Azure: Coding and Conversation. The traces include a\n",
            "subset of invocations received by the profiled services during\n",
            "one week, and contain the timestamp of the invocation, along\n",
            "with the number of input and output tokens. These traces are a\n",
            "super-set of our open-source traces for the same services [50].\n",
            "A. Heterogeneous Energy-Performance Profiles\n",
            "Request lengths The prefill and decode phases in an LLM\n",
            "inference exhibit distinct execution behaviors (Section II),\n",
            "suggesting that requests of different input and output lengths\n",
            "possess different compute and energy characteristics. We cate-\n",
            "gorize the requests based on the number of input/output tokens\n",
            "into 9 buckets: SS (short input, short output), SM (short input,\n",
            "medium output), SL (short input, long output), MS, MM,\n",
            "ML, LS, LM, and LL. Table IV shows the thresholds and\n",
            "corresponding TTFT/TBT SLOs. We set the thresholds for\n",
            "request lengths using the 33rd, 66th and 100th percentiles of\n",
            "the input/output lengths from a trace for a Conversation service\n",
            "from Azure. We set the SLOs to 5× the latency of a single\n",
            "request running isolated on a system [30].\n",
            "We use these categories to characterize the energy consump-\n",
            "tion of different request types running the Llama2-70B [33]\n",
            "model with a medium system load of 2000 tokens per second\n",
            "(TPS) under various GPU frequencies and model parallelisms.\n",
            "Table I shows our results in the form of a heat map. Since\n",
            "shorter requests are not computationally intensive, they meet\n",
            "their SLOs with any tensor parallelism, and generally at lower\n",
            "2\n",
            "Tensor Parallelism\n",
            "TP2\n",
            "TP4\n",
            "TP8\n",
            "GPU Frequency (GHz)\n",
            "0.8\n",
            "1.2\n",
            "1.6\n",
            "2.0\n",
            "0.8\n",
            "1.2\n",
            "1.6\n",
            "2.0\n",
            "0.8\n",
            "1.2\n",
            "1.6\n",
            "2.0\n",
            "Input\n",
            "Output\n",
            "Short\n",
            "Short\n",
            "0.77\n",
            "0.97\n",
            "1.03\n",
            "0.94\n",
            "0.79\n",
            "0.91\n",
            "1.01\n",
            "1.35\n",
            "1.19\n",
            "1.29\n",
            "1.49\n",
            "Short\n",
            "Medium\n",
            "2.78\n",
            "3.45\n",
            "3.68\n",
            "3.39\n",
            "2.82\n",
            "3.37\n",
            "3.81\n",
            "4.55\n",
            "4.15\n",
            "4.43\n",
            "4.74\n",
            "Short\n",
            "Long\n",
            "4.84\n",
            "4.17\n",
            "4.97\n",
            "5.52\n",
            "6.37\n",
            "5.62\n",
            "5.59\n",
            "6.95\n",
            "Medium\n",
            "Short\n",
            "1.02\n",
            "1.09\n",
            "1.08\n",
            "1.07\n",
            "1.20\n",
            "1.51\n",
            "1.29\n",
            "1.34\n",
            "1.73\n",
            "Medium\n",
            "Medium\n",
            "4.23\n",
            "3.91\n",
            "4.08\n",
            "5.34\n",
            "4.39\n",
            "4.56\n",
            "5.44\n",
            "Medium\n",
            "Long\n",
            "4.99\n",
            "4.66\n",
            "4.53\n",
            "6.86\n",
            "5.79\n",
            "6.52\n",
            "7.12\n",
            "Long\n",
            "Short\n",
            "1.51\n",
            "1.64\n",
            "1.76\n",
            "2.55\n",
            "2.53\n",
            "2.83\n",
            "2.94\n",
            "Long\n",
            "Medium\n",
            "7.71\n",
            "8.81\n",
            "9.17\n",
            "Long\n",
            "Long\n",
            "12.99\n",
            "11.89\n",
            "13.21\n",
            "TABLE I: Energy consumption in Watt×hours (Wh) for Llama2-70B varying request lengths, frequency, and model parallelism\n",
            "with medium system load (2K tokens per second). Configurations that violate the SLO are shown as empty gray boxes, while\n",
            "the acceptable configurations are colored as a heat map according to their energy consumption, per row.\n",
            "Tensor Parallelism\n",
            "TP2\n",
            "TP4\n",
            "TP8\n",
            "GPU Frequency (GHz)\n",
            "0.8\n",
            "1.2\n",
            "1.6\n",
            "2.0\n",
            "0.8\n",
            "1.2\n",
            "1.6\n",
            "2.0\n",
            "0.8\n",
            "1.2\n",
            "1.6\n",
            "2.0\n",
            "Low Load\n",
            "3.41\n",
            "3.75\n",
            "3.44\n",
            "2.93\n",
            "3.71\n",
            "3.73\n",
            "4.49\n",
            "3.76\n",
            "4.52\n",
            "4.64\n",
            "Medium Load\n",
            "4.23\n",
            "3.91\n",
            "4.08\n",
            "5.34\n",
            "4.39\n",
            "4.56\n",
            "5.44\n",
            "High Load\n",
            "4.22\n",
            "4.13\n",
            "5.86\n",
            "5.24\n",
            "5.42\n",
            "6.62\n",
            "TABLE II: Energy consumption in Wh for LLama2-70B medium-sized input and output (MM) requests varying frequency and\n",
            "model parallelism under different system loads: low (650 TPS), medium (2K TPS) and high (4K TPS).\n",
            "Tensor Parallelism\n",
            "TP2\n",
            "TP4\n",
            "TP8\n",
            "GPU Frequency (GHz)\n",
            "0.8\n",
            "1.2\n",
            "1.6\n",
            "2.0\n",
            "0.8\n",
            "1.2\n",
            "1.6\n",
            "2.0\n",
            "0.8\n",
            "1.2\n",
            "1.6\n",
            "2.0\n",
            "Llama2-13B [32]\n",
            "1.05\n",
            "0.99\n",
            "1.14\n",
            "1.24\n",
            "1.52\n",
            "1.27\n",
            "1.58\n",
            "1.65\n",
            "2.61\n",
            "2.35\n",
            "2.74\n",
            "3.45\n",
            "Mixtral-8x7B [39]\n",
            "1.03\n",
            "0.98\n",
            "1.21\n",
            "1.32\n",
            "1.39\n",
            "1.51\n",
            "2.09\n",
            "2.31\n",
            "2.57\n",
            "3.06\n",
            "3.71\n",
            "4.66\n",
            "Llama2-70B [33]\n",
            "4.23\n",
            "3.91\n",
            "4.08\n",
            "5.34\n",
            "4.39\n",
            "4.56\n",
            "5.44\n",
            "Llama3-70B [34]\n",
            "4.32\n",
            "4.28\n",
            "4.57\n",
            "6.11\n",
            "5.18\n",
            "5.42\n",
            "6.45\n",
            "Mixtral-8x22B [38]\n",
            "3.83\n",
            "3.23\n",
            "3.65\n",
            "4.03\n",
            "Falcon-180B [66]\n",
            "9.56\n",
            "7.94\n",
            "8.57\n",
            "10.34\n",
            "TABLE III: Energy consumption in Wh for medium-sized (MM) requests of different LLM architectures varying the frequency\n",
            "and model parallelism with medium system load (2K TPS).\n",
            "Input\n",
            "Output\n",
            "TTFT SLO\n",
            "TBT SLO\n",
            "Short\n",
            "S\n",
            "<256\n",
            "<100\n",
            "250 ms\n",
            "100 ms\n",
            "Medium\n",
            "M\n",
            "<1024\n",
            "<350\n",
            "400 ms\n",
            "100 ms\n",
            "Long\n",
            "L\n",
            "≤8192\n",
            "≥350\n",
            "2000 ms\n",
            "100 ms\n",
            "TABLE IV: Thresholds for classifying the requests based on\n",
            "input/output lengths and corresponding TTFT/TBT SLOs.\n",
            "frequencies compared to the rest. As an example, the least-\n",
            "energy configuration for SS requests is TP2 at 1.2 GHz. Con-\n",
            "versely, LL requests can only run with TP8 without violating\n",
            "the SLO. With TP8, the least-energy configuration for LL\n",
            "requests is 1.6 GHz. Note that the lowest power configuration\n",
            "that meets SLOs (TP8 at 1.2 GHz), is not the energy-optimal\n",
            "one due to the increased execution time. Running all the\n",
            "requests together would require the system to run with the\n",
            "most constrained SLO configuration, in this case, as per the LL\n",
            "configuration. This would make the system energy inefficient.\n",
            "To exploit this heterogeneity for energy-efficiency, the sys-\n",
            "tem would need to separate requests based on their input/out-\n",
            "put lengths, and process different request types with different\n",
            "server configurations. However, on request arrival, the input\n",
            "length is known, but, due to the auto-regressive LLM nature,\n",
            "the output length is unknown. Thus, the system needs to\n",
            "predict the output length. DynamoLLM will rely on prior\n",
            "work that efficiently performs such operation with relatively\n",
            "high precision [19], [55], [79], and will have a mechanism to\n",
            "mitigate the impact of occasional mis-predictions.\n",
            "Request loads In addition to the request length, the incoming\n",
            "load of the LLM inference server drives the compute require-\n",
            "ments. During periods of low load, the system has a larger\n",
            "SLO slack to exploit and can run the requests at low-frequency\n",
            "configurations to save energy. Conversely, during periods of\n",
            "high load, the system does not have enough SLO slack, and\n",
            "needs to run at high-frequency configurations.\n",
            "Table II shows the energy consumed when running Llama2-\n",
            "70B medium-sized input and output (MM) requests while\n",
            "varying the number of processed prompt tokens per second\n",
            "(TPS). The system can run low load with any TP at almost\n",
            "any frequency. Among all feasible configurations, the lowest-\n",
            "energy configuration is TP4 with 1.2 GHz. TP8 requires more\n",
            "GPUs to operate in parallel and, thus, consumes more energy.\n",
            "TP2 uses fewer GPUs but increases the execution time and\n",
            "forces individual GPUs to operate at high frequency to meet\n",
            "SLOs, leading to high energy. Conversely, under high load,\n",
            "3\n",
            "Mon\n",
            "Tue\n",
            "Wed\n",
            "Thu\n",
            "Fri\n",
            "Sat\n",
            "Sun\n",
            "Coding\n",
            "0\n",
            "25\n",
            "50\n",
            "75\n",
            "100\n",
            "Distribution [%]\n",
            "SS\n",
            "SM\n",
            "SL\n",
            "MS\n",
            "MM\n",
            "ML\n",
            "LS\n",
            "LM\n",
            "LL\n",
            "Mon\n",
            "Tue\n",
            "Wed\n",
            "Thu\n",
            "Fri\n",
            "Sat\n",
            "Sun\n",
            "Conversation\n",
            "0\n",
            "25\n",
            "50\n",
            "75\n",
            "100\n",
            "Fig. 1: Distribution of requests based on input and output\n",
            "lengths categorized into three groups: short, medium, and long.\n",
            "the system cannot operate on TP2 and requires TP4 or TP8.\n",
            "The lowest energy configuration is TP4 with 2 GHz. Overall,\n",
            "to minimize the energy consumption while operating under\n",
            "performance constraints, we need to consider the incoming\n",
            "load to set the correct parallelism and GPU frequency.\n",
            "Requested model The diversity of the compute properties\n",
            "of an LLM directly translates into its energy profile. Ta-\n",
            "ble III shows the energy consumption of different LLMs\n",
            "when running medium-sized requests at medium system load.\n",
            "Smaller models, such as Llama2-13B and Mixtral-8x7B, can\n",
            "run with any TP (even with a single GPU); their lowest-energy\n",
            "configuration is TP2 at 1.2 GHz. Mixtral-8x22B and Falcon-\n",
            "180B are much larger and can only run with TP8. Their lowest-\n",
            "energy configuration is TP8 at 1.2 GHz.\n",
            "Compute-bound models with large number of parameters\n",
            "are more sensitive to the GPU frequency and model paral-\n",
            "lelism. Hence, they often need to operate at high-frequency\n",
            "and high-energy modes. Sparse models with relatively smaller\n",
            "numbers of parameters tolerate lower frequencies and lower\n",
            "model parallelism. Hence, they meet the performance require-\n",
            "ments even with lower-performance modes.\n",
            "Service SLO Different services often use the same model\n",
            "with different SLO requirements [57]. As indicated before,\n",
            "we assume an SLO such that the P99 tail latency is within 5×\n",
            "of the execution time of a request on an unloaded system [30].\n",
            "However, some services have more relaxed SLOs, at 10× or\n",
            "even 20× of a single request execution [10], [37]. For different\n",
            "SLO requirements, the system may need different energy-\n",
            "optimal configurations. For example, Table I shows that, with\n",
            "strict SLO (5×), short-input long-output sized LLama2-70B\n",
            "requests at medium load have the optimal configuration at TP4\n",
            "and 1.2GHz. However, if with loose SLO (10×), the requests\n",
            "may even operate with TP2 at 1.6GHz.\n",
            "Insight #1 LLM workloads are highly heterogeneous in their\n",
            "energy-performance profiles. To achieve the optimal energy\n",
            "under performance SLOs, different requests (sizes, models and\n",
            "SLOs) need to be processed separately and differently.\n",
            "B. Dynamic LLM Inference Workloads\n",
            "Changing request-length distribution We measure the distri-\n",
            "bution of request types for Coding and Conversation services.\n",
            "Figure 1 shows the distribution of requests for each workload\n",
            "over a week. The distribution differs across services. Conver-\n",
            "sation has typically longer outputs and shorter inputs, while\n",
            "Mon\n",
            "Tue\n",
            "Wed\n",
            "Thu\n",
            "Fri\n",
            "Sat\n",
            "Sun\n",
            "0.00\n",
            "0.25\n",
            "0.50\n",
            "0.75\n",
            "1.00\n",
            "Normalized TPS\n",
            "Coding\n",
            "Conversation\n",
            "Fig. 2: Load over a week for Coding and Conversation LLM\n",
            "inference workloads.\n",
            "Coding shows the opposite trend. However, both services have\n",
            "a significant fraction of each request type, and importantly, the\n",
            "popularity of request types changes over time.\n",
            "As observed earlier, different request types require different\n",
            "energy-optimal configurations. Thus, the system needs to split\n",
            "its resources into per request-type pools, configure pools\n",
            "individually, and dynamically adapt the pools’ configurations\n",
            "based on the current request distribution. However, if the\n",
            "system classifies the requests into too few classes, it will not be\n",
            "able to fine-tune the system for best energy. On the other hand,\n",
            "too many classes may lead to fragmentation and negatively\n",
            "impact energy efficiency. Thus, the system has to find the\n",
            "right number of resource pools. In DynamoLLM, we will use\n",
            "historical data to set the number of pools such that requests\n",
            "with distinct SLO requirements (TTFT or TBT bound) and\n",
            "compute properties (compute or memory bound) have separate\n",
            "pools. Moreover, as the load of a given request type reduces,\n",
            "DynamoLLM will avoid fragmentation by merging the pool\n",
            "with the next available pool that serves longer requests.\n",
            "Request load fluctuations LLM inference workloads, as user-\n",
            "facing applications, exhibit a typical diurnal pattern with peaks\n",
            "during working hours and valleys at night and weekends.\n",
            "Figure 2 shows the load in tokens per second of the two\n",
            "workloads over a week. The load is normalized to the peak\n",
            "of the individual workloads. The Coding trace shows a clear\n",
            "diurnal pattern, with peaks every day, lower load at night, and\n",
            "much lower load during weekends. Conversation shows a less\n",
            "extreme, but still significant, diurnal pattern.\n",
            "The peak load of Conversation is 1.7× and 3.3× higher than\n",
            "its average and valley loads, respectively. The peak load of\n",
            "Coding is 2.8× and 34.6× higher than its average and valley\n",
            "loads, respectively. This large slack indicates that the LLM\n",
            "inference servers can frequently operate in a less performant\n",
            "but energy-optimized configuration without violating the SLO.\n",
            "Once the load starts building up, the server needs to switch to\n",
            "a more performant mode of operation.\n",
            "LLM service SLO and model diversity Finally, different\n",
            "services may time-share the same LLM model instance [14].\n",
            "They may have different SLOs, requiring the configuration\n",
            "to be adapted based on the current service-user. On the other\n",
            "hand, the same service may concurrently use multiple different\n",
            "models [11]. This requires different execution plans for the\n",
            "optimal energy consumption of the individual queries. Thus,\n",
            "it is not trivial for service providers to operate in an energy-\n",
            "optimal setting while meeting the performance SLOs.\n",
            "4\n",
            "Overhead source\n",
            "Time\n",
            "Create a new H100 VM [36]\n",
            "∼1-2 min\n",
            "Initialize distributed multi-GPU environment\n",
            "∼2 min\n",
            "Download model weights (Llama2-70B [67])\n",
            "∼3 min\n",
            "Set up the engine configuration\n",
            "∼18 sec\n",
            "Install weights and KV cache on GPUs\n",
            "∼15 sec\n",
            "Total\n",
            "∼6-8 min\n",
            "TABLE V: Measured overheads of creating a new 8×H100\n",
            "instance of an LLM inference server VM.\n",
            "Insight #2 LLM workloads are highly dynamic and, thus, an\n",
            "energy-optimal configuration can quickly become sub-optimal.\n",
            "However, the complexity of a large search space requires an\n",
            "automatic and user-transparent configuration selection.\n",
            "C. Reconfiguration Overheads\n",
            "To capture the fast changes in LLM inference workloads, we\n",
            "need to quickly transition between configurations. However,\n",
            "there are overheads to change (1) number of inference server\n",
            "instances, (2) model parallelism, and (3) GPU frequency.\n",
            "Changing instance number To adjust to fluctuating load,\n",
            "it is cost-beneficial to dynamically adjust the number of\n",
            "LLM instances to serve the requests (i.e., scale in and out).\n",
            "However, the overheads of adding a new inference server are\n",
            "too large to be tolerable on the critical path of inference\n",
            "loads. Table V shows the breakdown of the overheads to:\n",
            "(1) instantiate a new GPU VM in the cloud (such as H100\n",
            "VM [36]), (2) initialize the distributed multi-GPU environment\n",
            "(e.g., Ray, MPI), (3) download the model weights, (4) setup\n",
            "the inference engine, and (5) install the weights and key-\n",
            "value cache on the GPUs. In total, these overheads can take\n",
            "even a few minutes. Hence, the conventional LLM inference\n",
            "environments typically provision the static number of instances\n",
            "to handle their peak load resulting in heavy underutilization.\n",
            "In DynamoLLM, we will propose techniques to efficiently\n",
            "scale the number of instances (with the current load) while\n",
            "minimizing most of the scale-out overheads.\n",
            "Changing model parallelism To modify the model paral-\n",
            "lelism of an LLM inference server, we need to perform two\n",
            "operations. First, we need to re-shard the model weights and\n",
            "transfer them to the memory of the right GPUs. Second, the\n",
            "inference engine needs to synchronize the involved GPUs.\n",
            "Current systems stop the engine, unload the weights from\n",
            "GPUs, load the weights from the host to the new set of GPUs,\n",
            "and re-start the engine from the scratch. This adds intolerable\n",
            "overheads (around 1-2 minutes) if performed on the critical\n",
            "path. In DynamoLLM, we will show how to minimize the re-\n",
            "sharding overheads by smartly mapping the logical to physical\n",
            "GPUs, exploiting inter-GPU direct NVLink connections and\n",
            "moving the weights between GPUs in the background.\n",
            "Changing GPU frequency Setting the GPU frequency (e.g.,\n",
            "via nvidia-smi [46]) incurs non-negligible overheads. It\n",
            "involves invoking the OS, communicating with the GPU\n",
            "driver via system calls, and performing hardware interactions\n",
            "via firmware. On average, setting the GPU frequency takes\n",
            "SS\n",
            "SM\n",
            "SL\n",
            "MS\n",
            "MM\n",
            "ML\n",
            "LS\n",
            "LM\n",
            "LL\n",
            "Avg\n",
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "T-put [RPS]\n",
            "ConstFreq\n",
            "SwitchFreq\n",
            "Fig. 3: Throughput for different request types with constant\n",
            "frequency (1980MHz) and with re-setting the frequency (to\n",
            "1980MHz) on every iteration in the background.\n",
            "around 50-80ms. In comparison, one decode iteration of the\n",
            "LLM inference process takes 20-30ms. Consequently, the time\n",
            "spent adjusting the GPU frequency can significantly impact\n",
            "the overall performance, potentially doubling the latency of\n",
            "individual inference steps. Figure 3 shows the throughput for\n",
            "different request types when constantly running at the highest\n",
            "frequency (1980 MHz) and when re-setting the frequency (to\n",
            "1980 MHz) in the background on every LLM inference itera-\n",
            "tion. Due to the software overheads, the throughput of LLM\n",
            "inference system significantly drops. Therefore, optimizing\n",
            "or minimizing frequency changes during LLM inference is\n",
            "crucial for maintaining efficient and responsive performance.\n",
            "Insight #3 Transitioning between LLM server configurations\n",
            "incurs significant overheads. For energy-efficiency, such over-\n",
            "heads need to be minimized and considered when computing\n",
            "the energy/performance trade-offs.\n",
            "IV. DYNAMOLLM: AN ENERGY MANAGEMENT\n",
            "FRAMEWORK FOR LLM INFERENCE CLUSTERS\n",
            "We use the insights to design DynamoLLM, the first energy\n",
            "management framework for LLM inference environments. Dy-\n",
            "namoLLM seamlessly integrates with existing inference plat-\n",
            "forms, enabling LLM workloads to operate energy-efficiently\n",
            "and cost-effectively while meeting their performance SLOs.\n",
            "DynamoLLM has four key principles. First, it is energy-\n",
            "optimized and SLO-aware, leveraging model profiles to au-\n",
            "tomatically select the most energy-efficient configuration for\n",
            "specific LLMs and inference workloads within their SLO\n",
            "requirements. Second, DynamoLLM fine-tunes configurations\n",
            "for heterogeneous LLM workloads by dividing cluster re-\n",
            "sources into instance pools tailored to specific request types.\n",
            "Third, DynamoLLM accommodates fluctuating LLM infer-\n",
            "ence loads by dynamically reconfiguring the chosen organiza-\n",
            "tion. Finally, to ensure frequent and smooth reconfiguration,\n",
            "DynamoLLM minimizes reconfiguration overheads.\n",
            "Architecture Figure 4 shows the DynamoLLM architecture.\n",
            "The system is organized hierarchically at the cluster, pool,\n",
            "and instance levels. At each level, the controllers tune their\n",
            "assigned configuration knob, and communicate their decisions\n",
            "with the controllers from the upper and lower levels. The\n",
            "controllers use energy-performance models generated in the\n",
            "profiling phase to determine the number of instances, model\n",
            "parallelization, and GPU frequency for an energy-optimized\n",
            "operation given the current system state. (1) Cluster Manager\n",
            "receives inference requests, predicts their type, and forwards\n",
            "them to the appropriate instance pool. Additionally, it peri-\n",
            "5\n",
            "Instance \n",
            "Manager\n",
            "Model \n",
            "Weights\n",
            "Cluster \n",
            "Storage\n",
            "Inference \n",
            "Requests\n",
            "Select GPU\n",
            "Frequency\n",
            "Perf-Energy \n",
            "Profile\n",
            "Instance \n",
            "Manager\n",
            "Instance \n",
            "Manager\n",
            "Instance Pool (SS)\n",
            "Model \n",
            "Profile\n",
            "…\n",
            "…\n",
            "…\n",
            "Instance Pool (MM)\n",
            "Instance Pool (LL)\n",
            "Select Instance \n",
            "Count\n",
            "Output-Length\n",
            "Prediction\n",
            "Throughput \n",
            "Profile\n",
            "Load\n",
            "Prediction\n",
            "Cluster Manager\n",
            "Perf-Energy \n",
            "Profile\n",
            "Pool Manager\n",
            "Select Model \n",
            "Parallelization\n",
            "Perf-Energy \n",
            "Profile\n",
            "Pool Manager\n",
            "Perf-Energy \n",
            "Profile\n",
            "Pool Manager\n",
            "Instance Manager\n",
            "Fig. 4: DynamoLLM architecture: a hierarchy of controllers with cluster resources split into per request-type pools.\n",
            "odically re-evaluates how many pools and how many model\n",
            "instances per pool are needed based on the system load. (2)\n",
            "Pool Manager schedules the requests to model instances in a\n",
            "manner that minimizes per-pool energy consumption. It also\n",
            "periodically checks if its instances need to be re-sharded into\n",
            "a more energy-efficient parallelization. (3) Instance Manager\n",
            "schedules the requests to the inference engine and periodically\n",
            "checks if the instance’s GPU frequency needs to be adjusted.\n",
            "A. Configuring Instances for Energy-Efficiency\n",
            "Generating LLM profiles When deploying their service to\n",
            "DynamoLLM, users specify the LLM used by the service and\n",
            "the expected performance SLOs. Then, the system character-\n",
            "izes the model and generates its energy-performance profile.\n",
            "DynamoLLM profiles the model by running loads of different\n",
            "request lengths at different model parallelisms (TP2, TP4 and\n",
            "TP8) and GPU frequencies (800–1980MHz, with a step of\n",
            "200MHz). The system profiles a few load levels, up to the\n",
            "maximum throughput, and then extrapolates the behavior for\n",
            "the loads in between the measured ones. The profiling result is\n",
            "a function that takes the load, request length, model parallelism\n",
            "and GPU frequency as inputs and outputs the expected energy\n",
            "consumption and TTFT/TBT latencies.\n",
            "As many services may use the same model, DynamoLLM\n",
            "can reuse the profiles across services, minimizing the profiling\n",
            "overheads. Such profiles are stored in a global DynamoLLM\n",
            "repository, and then cached in a cluster-local storage when a\n",
            "given service is deployed in the cluster.\n",
            "Selecting the energy-optimized configuration Given the\n",
            "current load and available resources, DynamoLLM uses the\n",
            "generated profiles to minimize energy consumption while\n",
            "staying within performance constraints. The system formulates\n",
            "this task as an optimization problem for the mixed integer\n",
            "linear programming (MILP) solver. The solver needs to output\n",
            "how many instances of each tensor parallelism (N T P2, N T P4\n",
            "and N T P8) are needed, at which frequency they should run\n",
            "(f T P2, f T P4 and f T P8), and which load should be assigned\n",
            "to each instance (LT P2, LT P4 and LT P8). We assume that all\n",
            "instances of a given parallelism run at the same frequency and\n",
            "receive fair-share amount of work.\n",
            "The optimization target of the solver is to minimize the\n",
            "total energy consumption (E), while the constraints are: 1)\n",
            "the total number of GPUs used by all instance types does\n",
            "not exceed the assigned number of GPUs (N); 2) the load\n",
            "assigned to individual instances sums up to the total expected\n",
            "load (L); and 3) the expected performance of all instances with\n",
            "the assigned load is within the requirements (SLO). Functions\n",
            "EnergyT Pi,fi(LT Pi) and PerformanceT Pi,fi(LT Pi) output\n",
            "the expected energy and performance, respectively, when run-\n",
            "ning the load LT Pi with TPi parallelism at fi GPU frequency.\n",
            "Then, the optimization task can be formulated as:\n",
            "min\n",
            " X\n",
            "i\n",
            "(NT Pi × EnergyT Pi,fi(LT Pi))\n",
            "!\n",
            "∀i ∈{2, 4, 8}\n",
            "s.t.\n",
            "X\n",
            "i\n",
            "i × NT Pi ≤N\n",
            "X\n",
            "i\n",
            "(NT Pi × LT Pi) ≥L\n",
            "PerformanceT Pi,fi(LT Pi) ≤SLO\n",
            "(1)\n",
            "This approach guarantees the energy optimal configuration.\n",
            "However, it introduces non-negligible overheads (i.e., ∼100s\n",
            "of ms) due to the large search-space for the solver. Hence, it\n",
            "cannot be used to select the correct system configuration at\n",
            "fine-grain intervals (e.g., every few seconds). Next we show\n",
            "how to break the task into a hierarchy of subtasks and use an\n",
            "approximation heuristic to reduce the computation complexity.\n",
            "B. Hierarchical Control for Dynamic Load\n",
            "DynamoLLM simplifies computations by assigning spe-\n",
            "cific optimization tasks to individual controllers. Instead of\n",
            "searching for a globally optimal configuration, controllers\n",
            "set locally optimal values for individual knobs under the\n",
            "constraints imposed by the upper-level controllers and under\n",
            "the assumption that the lower-level controllers operate at the\n",
            "highest performance configuration. This allows the controllers\n",
            "to operate at varying time scales–from minutes for node ad-\n",
            "justments down to seconds for frequency tuning. The different\n",
            "scales are needed as each operational change involves distinct\n",
            "overheads and energy-efficiency impacts.\n",
            "6\n",
            "Scale-out/in At every epoch (e.g., 30 minutes), the cluster\n",
            "manager computes the minimal number of nodes per pool that\n",
            "can support the load of a given request type. The manager uses\n",
            "a load predictor to forecast the incoming load, PL, for each\n",
            "request type based on its historic data (e.g., via lightweight\n",
            "load templates [62]). Moreover, the manager assumes that all\n",
            "instances will run at the highest-performance configuration\n",
            "(i.e., TP8 at 1980 MHz). Then, if the predicted peak of a\n",
            "given request type is PL, and the maximum load that a node\n",
            "can support for this request type is ML, the manager assigns\n",
            "\u0006 P L\n",
            "ML\n",
            "\u0007\n",
            "nodes to that pool. By consolidating the work into a\n",
            "small number of nodes, the system tries to minimize the cost\n",
            "for the user and the idle energy of lightly-loaded GPUs.\n",
            "Handling fragmentation: Allocating resources to handle the\n",
            "peak loads can cause resource underutilization. If overprovi-\n",
            "sioning accumulates across pools, the energy efficiency drops.\n",
            "To prevent such cases, DynamoLLM assigns one instance less\n",
            "to a given instance pool and moves the leftover load to the\n",
            "instance pool of the next larger request type. The cluster\n",
            "manager uses this information to forward a fraction of the\n",
            "load for a given request type to the appropriate larger instance\n",
            "pool during the next scheduling epoch (e.g., 30 minutes). In\n",
            "this way, only the instance pool for the largest requests can be\n",
            "overprovisioned minimizing the cluster-level fragmentation.\n",
            "Shard-up/down At every epoch (e.g., every 5 minutes), the\n",
            "pool manager decides how to split the assigned N GPUs from\n",
            "the cluster manager into correct model parallelism (how many\n",
            "instances to create in the pool) and tensor parallelism (how\n",
            "many GPUs to use for each instance). The pool manager uses\n",
            "a simplified version of Equation (1) assuming that all instances\n",
            "run at the highest GPU frequency (i.e., 1980 MHz). Thus,\n",
            "the goal is to minimize the energy, while operating with the\n",
            "fixed number of GPUs running at the highest frequency, and\n",
            "controlling only the parallelism knob.\n",
            "Accounting for the overheads: DynamoLLM stores the tran-\n",
            "sitioning overheads (scale-out/in, shard-up/down) in an Over-\n",
            "head Table. This table is integrated with the controllers, so that\n",
            "when they calculate the energy benefits of new configurations,\n",
            "they can take into account the costs of reconfiguration. The\n",
            "controllers evaluate whether the energy savings gained from\n",
            "re-configuring justify the associated overheads and downtime.\n",
            "Reducing downtime: The reconfiguration cannot occur si-\n",
            "multaneously on all instances due to the risk of long downtime.\n",
            "Instead, DynamoLLM employs a staggered reconfiguration\n",
            "approach, where a subset of instances is reconfigured (e.g.,\n",
            "re-sharded) at a time. This ensures that while some instances\n",
            "are undergoing reconfiguration, others remain operational to\n",
            "handle ongoing workloads. The system first reconfigures the\n",
            "instances that have higher potential energy savings.\n",
            "Scale-up/down Finally, at every epoch (e.g., 5s), the instance\n",
            "manager fine tunes the GPU frequency for further energy\n",
            "savings given the assigned model parallelism. The instance\n",
            "manager uses the performance profile to first filter-out fre-\n",
            "quencies that violate the SLO under the current load. Then,\n",
            "it uses the energy profile to pick an acceptable frequency that\n",
            "minimizes the energy consumption.\n",
            "GPU0\n",
            "GPU1\n",
            "GPU2\n",
            "GPU3\n",
            "GPU4\n",
            "GPU5\n",
            "GPU6\n",
            "GPU7\n",
            "TP2\n",
            "TP4\n",
            "W0\n",
            "W1\n",
            "W4\n",
            "W5\n",
            "W2\n",
            "W3\n",
            "W6\n",
            "W7\n",
            "W0\n",
            "W1\n",
            "W2\n",
            "W3\n",
            "W4\n",
            "W5\n",
            "W6\n",
            "W7\n",
            "TP8\n",
            "W0\n",
            "W4\n",
            "W2\n",
            "W6\n",
            "W1\n",
            "W5\n",
            "W3\n",
            "W7\n",
            "Fig. 5: Example of re-sharding a TP4 model to TP2/TP8.\n",
            "C. Reduced Overheads for Smooth Reconfiguration\n",
            "To enable frequent reconfiguration, DynamoLLM proposes\n",
            "a set of techniques to minimize the overheads of (1) scaling-\n",
            "in/out the number of LLM inference servers, (2) sharding-\n",
            "up/down the parallelism of a given instance, and (3) scaling-\n",
            "up/down the GPU frequency of a given instance.\n",
            "Scaling in/out inference servers DynamoLLM reduces the\n",
            "overheads of creating a new server instance by implementing\n",
            "several strategies. First, it keeps the model weights cached\n",
            "locally within the cluster (shown in Figure 4) avoiding the need\n",
            "to fetch them from a global repository. Second, it starts VMs\n",
            "from a snapshot with the entire state of the inference engine\n",
            "already initialized, reducing the boot-up time. This snapshot\n",
            "includes pre-loaded libraries, GPU drivers and inference en-\n",
            "gine configurations. Third, it proactively creates new VMs in\n",
            "the background, outside of the critical path of active workload\n",
            "handling. Specifically, DynamoLLM predicts the peak load for\n",
            "the next scheduling epoch and starts the extra VMs before the\n",
            "epoch starts. By having these VMs ready to go, DynamoLLM\n",
            "can seamlessly offload a fraction of the load to new instances\n",
            "without any noticeable latency impact.\n",
            "Sharding up/down an instance To reduce the re-sharding\n",
            "overheads, DynamoLLM optimizes the distribution of weights\n",
            "across GPUs. We propose two techniques to minimize the\n",
            "data transfers and latency of individual transfers. First, the\n",
            "system develops a graph matching algorithm that maximizes\n",
            "the amount of weights that remain stationary in their current\n",
            "GPUs. The algorithm takes current weight distribution and\n",
            "desired tensor parallelism as inputs, and outputs the source\n",
            "and destination GPUs and fraction of weights to be transferred\n",
            "between each source-destination pair. Specifically, the algo-\n",
            "rithm constructs a bipartite graph where nodes represent GPUs\n",
            "in the current and next configurations. Edges between nodes\n",
            "represent potential transfers, weighted by the amount of data\n",
            "to be transferred. Then, it applies a maximum weight matching\n",
            "algorithm to find the optimal transfer plan that minimizes the\n",
            "total weight of the edges (i.e., minimizes the amount of data\n",
            "transferred). Second, to reduce the transfer latency, the system\n",
            "uses inter-GPU direct transfers via NVLink, allowing them\n",
            "to send fractions of their weights in parallel to other GPUs\n",
            "without any host intervention.\n",
            "7\n",
            "Figure 5 shows an example of re-sharding from TP4 to\n",
            "TP2 and TP8. Consider first the case when going from a\n",
            "lower to a higher-level parallelism (TP4→TP8). In the initial\n",
            "state (TP4), GPUs 0-3 hold a quarter of the model weights\n",
            "each. In the final state (TP8), all GPUs need to hold an\n",
            "eight of the model weight. Thus, the first four GPUs already\n",
            "have their required state, and they only need to send half of\n",
            "their weights to the remaining four GPUs. The four transfers\n",
            "(e.g., GPU0→GPU4, GPU1→GPU5,...) happen in parallel.\n",
            "Therefore, the re-sharding requires the time to send 1/8 of the\n",
            "model weights via NVLink (around 50ms in our setup).\n",
            "Consider now the case when going from a higher to a lower\n",
            "parallelism (TP4→TP2). In the final state (TP2), two GPUs\n",
            "need to hold a half of the weights each. As each GPU initially\n",
            "holds a quarter of the weights, we merge the weights from\n",
            "two GPUs to a single one: GPU1 sends weights W2/W3 to\n",
            "GPU0, and GPU3 sends weights W6/W7 to GPU2. As these\n",
            "two transfers happen in parallel, the total re-sharding time is\n",
            "the time to send 1/4 of the model weights (around 100ms\n",
            "in our setup). Table VI shows the re-sharding overheads with\n",
            "different source/destination pairs with our optimized approach.\n",
            "Some configurations quickly switch to other configurations\n",
            "(transition time 0), other changes incur larger overheads (tran-\n",
            "sition time 4T, where T is the time to send 1/8 of the weights).\n",
            "Moreover, on some transitions, the instance continues serv-\n",
            "ing the requests with the same throughput. This is the case\n",
            "when scaling from a smaller to a larger tensor parallelism\n",
            "(e.g., TP4→TP8). The old instance sends a fraction of the\n",
            "weights to the new instance, without increasing its memory\n",
            "footprint. During other transitions, the current instance needs\n",
            "to operate under lower throughput. This is the case when the\n",
            "instance scales from a larger to a smaller tensor parallelism\n",
            "(e.g., TP8→TP4). Some GPUs used by the old instance accept\n",
            "extra weights, reducing their memory capacity to serve new\n",
            "requests. In general, whenever the GPU memory required to\n",
            "hold model weights increases, the throughput decreases due\n",
            "to the lower memory capacity for the incoming requests.\n",
            "Finally, after the weights are sent to the correct memories,\n",
            "the inference engine needs to synchronize the GPUs that run\n",
            "the new instance. State-of-the-art engines, such as vLLM [26],\n",
            "perform this operation in a few 100s of milliseconds to a few\n",
            "seconds. During this period the instance cannot receive any\n",
            "load, causing noticeable downtime. To reduce the downtime,\n",
            "DynamoLLM allows the old instance to process the requests\n",
            "while the new instance is going through the synchronization\n",
            "process. Only when the new instance comes online, the old\n",
            "instance is removed. This is possible only when the sum of\n",
            "the memory from the old and new instances is below the\n",
            "GPU’s memory capacity. When the sum exceeds the memory\n",
            "capacity (e.g., TP2 and TP4 with 70B parameters), the old\n",
            "instance needs to be shutdown before the new instance is\n",
            "created. Overall, different transitions incur different overheads\n",
            "and instance downtime requiring a fraction of the load to\n",
            "be shifted to another instance. DynamoLLM minimizes the\n",
            "overheads, and considers their impact on the overall efficiency\n",
            "when deciding whether to re-configure an instance.\n",
            "Src/Dst\n",
            "TP2\n",
            "4TP2\n",
            "TP4\n",
            "TP2+TP4\n",
            "2TP4\n",
            "TP8\n",
            "TP2\n",
            "0\n",
            "4T\n",
            "2T\n",
            "2T\n",
            "2T\n",
            "T\n",
            "4TP2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "TP4\n",
            "2T\n",
            "2T\n",
            "0\n",
            "2T\n",
            "2T\n",
            "T\n",
            "TP2+TP4\n",
            "0\n",
            "2T\n",
            "0\n",
            "0\n",
            "T\n",
            "T\n",
            "2TP4\n",
            "T\n",
            "T\n",
            "0\n",
            "T\n",
            "0\n",
            "0\n",
            "TP8\n",
            "T\n",
            "T\n",
            "T\n",
            "T\n",
            "T\n",
            "0\n",
            "TABLE VI: Overhead of transferring model weights on a re-\n",
            "sharding. T is the time unit to move 1/8 of the model (e.g., with\n",
            "300GB/s NVLink bandwidth on NVIDIA DGX H100 [44] and\n",
            "Llama2-70B model [67], T = 50ms).\n",
            "Scaling up/down the frequency The overheads of changing\n",
            "the GPU frequency are minimized by keeping the NVIDIA\n",
            "System Management Interface (nvidia-smi) monitor pro-\n",
            "gram directly loaded into memory. This eliminates the need\n",
            "to reload the program every time a frequency adjustment is\n",
            "required. Moreover, by running the controller in privileged\n",
            "mode, DynamoLLM avoids the overhead associated with OS-\n",
            "user interactions, allowing for rapid frequency adjustments.\n",
            "D. Predictive Scheduling for Request Heterogeneity\n",
            "To map the heterogeneity of requests to the heterogeneous\n",
            "instance pools, the cluster controller in DynamoLLM uses an\n",
            "output-length predictor to anticipate the request type and steer\n",
            "requests to the correct instance pool. The predictor acts as a\n",
            "proxy model that takes input prompt and classifies the output\n",
            "as short, medium or long. Based on the predicted output length\n",
            "and known input length, the cluster manager forwards the\n",
            "request to the pool manager being in charge for a given request\n",
            "type. If the instance pool is currently overloaded, the cluster\n",
            "manager forwards the request to the next available pool for\n",
            "a larger request type. Once the request arrives to the correct\n",
            "pool, the pool manager needs to pick an instance from the\n",
            "pool. Specifically, the manager uses the generated models from\n",
            "the profiling step to predict energy and response times of each\n",
            "instance after potentially adding a new request to that instance.\n",
            "Then, it chooses the instance that minimizes total energy while\n",
            "staying within per-instance throughput determined by the SLO.\n",
            "Handling mis-predictions If the system over-estimates a\n",
            "request length, the request gets routed to a higher-performance\n",
            "pool. Hence, it runs with sub-optimal energy, but its latency\n",
            "remains unaffected. Conversely, if a request length is under-\n",
            "estimated, the request is placed to a lower-performance pool,\n",
            "potentially missing its SLOs. Similarly, load mis-predictions\n",
            "can result in insufficient resources for a given pool during\n",
            "request bursts. In both cases, due to some mis-predictions, the\n",
            "system needs to react to the created emergency event.\n",
            "When an instance manager detects that its queue is building\n",
            "up, indicating that the rate of request processing is lower than\n",
            "the rate of request arrival, it triggers an emergency event.\n",
            "First, the instance manager tries to re-order the requests in its\n",
            "queue and prioritizes those requests that are about to miss their\n",
            "deadline. Second, if some requests will miss their deadlines\n",
            "even after the reordering, the instance manager ramps up the\n",
            "frequency of its GPUs to the maximum value. Third, if the\n",
            "8\n",
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "Energy [kWh]\n",
            "DynamoLLM\n",
            "ScaleFreq\n",
            "ScaleShard\n",
            "ScaleInst\n",
            "MultiPool\n",
            "SinglePool\n",
            "SS\n",
            "SM\n",
            "SL\n",
            "MS\n",
            "MM\n",
            "ML\n",
            "LS\n",
            "LM\n",
            "LL\n",
            "Fig. 6: Energy consumption with the six evaluated systems\n",
            "running open-source Llama2-70B model [67] with 1-hour\n",
            "open-source production traces [50].\n",
            "backlog persists or worsens, the instance manager re-steers\n",
            "some requests that have not started their execution. A subset\n",
            "of requests is moved to another instance within the pool via the\n",
            "pool manager. Finally, if all the attempts are insufficient, the\n",
            "instance manager resorts to more drastic measures. One such\n",
            "measure involves squashing requests that have been waiting for\n",
            "processing beyond a specified threshold. This action signals\n",
            "users to retry their requests, allowing the frontend system to\n",
            "redirect these requests to alternative instance pools or retry\n",
            "them later when system load has stabilized.\n",
            "E. DynamoLLM Implementation\n",
            "We build DynamoLLM on top of vLLM [26], a state-\n",
            "of-the-art LLM inference platform. However, DynamoLLM’s\n",
            "modularity allows it to be integrated with other platforms, e.g.,\n",
            "TensorRT-LLM [47], without modifications. We implement\n",
            "controllers as lightweight gRPC servers with low memory and\n",
            "compute requirements. Cluster and pool managers are hosted\n",
            "in a dedicated VM for robust management. Instance managers\n",
            "are co-located with the LLM inference engine instances for\n",
            "low communication overheads. For output length prediction,\n",
            "we leverage a BERT-based proxy model [55], which provides\n",
            "accurate and efficient classification of incoming requests. For\n",
            "load prediction, we use a template-based approach that uses\n",
            "historical data to model load patterns over a week [62]. The\n",
            "pool manager employs Python’s PuLP library [53] for solving\n",
            "MILP. DynamoLLM models energy and performance using\n",
            "the interp1d function from the SciPy [54] Python library.\n",
            "V. EVALUATION\n",
            "A. Evaluation Setup\n",
            "We run our experiments on servers with 8 H100 GPUs [44].\n",
            "We show the results for Llama2-70B [67], but other models\n",
            "(i.e., Mixtral [38], Falcon [66], BLOOM [59]) follow the\n",
            "same trends. We set the load using production-level traces:\n",
            "1 hour open-source traces [50] and 1-day and 1-week traces\n",
            "for Coding and Conversation from our fleet. We compare Dy-\n",
            "namoLLM to five systems. SinglePool (a state-of-the-practice\n",
            "baseline) schedules all the requests to the common pool of\n",
            "instances running with TP8 at the highest GPU frequency.\n",
            "MultiPool separates LLM instances in multiple per-request-\n",
            "type pools. ScaleInst, ScaleShard, and ScaleFreq additionally\n",
            "P50\n",
            "P90\n",
            "P99\n",
            "0.0\n",
            "0.2\n",
            "0.4\n",
            "0.6\n",
            "TTFT [ms]\n",
            "P50\n",
            "P90\n",
            "P99\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "TBT [ms]\n",
            "SinglePool\n",
            "MultiPool\n",
            "ScaleInst\n",
            "ScaleShard\n",
            "ScaleFreq\n",
            "DynamoLLM\n",
            "Fig. 7: Summary of the latencies for each of the systems\n",
            "running open-source Llama2-70B model [67] with 1-hour\n",
            "open-source production traces [50].\n",
            "P50\n",
            "P90\n",
            "P99\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "Cluster Power [kW]\n",
            "P50\n",
            "P90\n",
            "P99\n",
            "0\n",
            "200\n",
            "400\n",
            "600\n",
            "800\n",
            "GPU Power [W]\n",
            "SinglePool\n",
            "MultiPool\n",
            "ScaleInst\n",
            "ScaleShard\n",
            "ScaleFreq\n",
            "DynamoLLM\n",
            "Fig. 8: Summary of the power for each of the systems running\n",
            "open-source Llama2-70B model [67] with 1-hour open-source\n",
            "production traces [50].\n",
            "scale the number of instances in the pool, model parallelism,\n",
            "or GPU frequency according to the current load, respectively.\n",
            "B. Cluster-Level Experiments\n",
            "We first evaluate the system on a cluster of GPU servers\n",
            "using the 1h open source production traces for the Conver-\n",
            "sation service [50]. We provision the baselines with 12 H100\n",
            "servers to handle the peak load, while DynamoLLM scales the\n",
            "number of servers according to the current load.\n",
            "Energy Figure 6 shows the energy consumption of the\n",
            "cluster for the experiment. MultiPool increases the energy\n",
            "consumption by 20% over SinglePool, because it allocates\n",
            "a larger number of resources while always operating at\n",
            "the highest-performance configuration. Meanwhile, ScaleInst,\n",
            "ScaleShard, ScaleFreq and DynamoLLM reduce the energy\n",
            "consumption by 4.1%, 7%, 19% and 35%, respectively. Scale-\n",
            "Inst/Shard/Freq reduce the energy by configuring one knob\n",
            "but leave substantial space for further savings. Finally, Dy-\n",
            "namoLLM synchronously scales multiple knobs to achieve the\n",
            "lowest energy consumption. We further breakdown the total\n",
            "energy per request type. Figure 6 shows that longer requests\n",
            "(e.g., LL) and highly-popular requests (e.g., ML) consume dis-\n",
            "proportionally more energy than the other types.\n",
            "Latency Figure 7 shows the TTFT/TBT latencies for each sys-\n",
            "tem. By separating request types into different resource pools,\n",
            "MultiPool removes the head-of-line blocking effect and re-\n",
            "duces the latencies over SinglePool. Similarly, ScaleShard and\n",
            "ScaleFreq, and DynamoLLM reduce the tail latency. However,\n",
            "these systems slightly increase the P50 latency by operating in\n",
            "lower-performance modes when there is available SLO slack.\n",
            "On the other hand, ScaleInst increases the tail latency due\n",
            "to the large overheads of creating a new inference server on\n",
            "the critical path of users’ load. Overall, DynamoLLM reduces\n",
            "9\n",
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "45\n",
            "50\n",
            "55\n",
            "60\n",
            "Time [min]\n",
            "0.0\n",
            "0.5\n",
            "1.0\n",
            "1.5\n",
            "2.0\n",
            "Avg Freq. [GHz]\n",
            "Total\n",
            "SL\n",
            "LL\n",
            "Fig. 9: GPU Frequency over one hour for DynamoLLM\n",
            "running open-source Llama2-70B model [67] with 1-hour\n",
            "open-source production traces [50].\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "Num. GPUs\n",
            "Total\n",
            "TP2\n",
            "TP4\n",
            "TP8\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "SL Pool\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "Time [min]\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "Num. GPUs\n",
            "ML Pool\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "Time [min]\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "LL Pool\n",
            "35\n",
            "45\n",
            "55\n",
            "65\n",
            "75\n",
            "85\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "kTPS\n",
            "0\n",
            "4\n",
            "8\n",
            "12\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "kTPS\n",
            "Fig. 10: Number of GPUs used for each sharding configuration\n",
            "(TP2, TP4 or TP8) over time running open-source Llama2-70B\n",
            "model [67] with 1-hour open-source production traces [50].\n",
            "the P99 TTFT and TBT latencies by 5.3% and 11.1% over\n",
            "SinglePool, respectively, while it increases the P50 TTFT and\n",
            "TBT latencies by 11.4% and 7.6%, respectively.\n",
            "Power Figure 8 shows the power consumption across the\n",
            "systems for the cluster (right figure) and on average per-\n",
            "GPU (left figure). Due to operating in energy-efficient modes,\n",
            "DynamoLLM effectively reduces both cluster and per-GPU\n",
            "power. DynamoLLM reduces the P50 and P99 power con-\n",
            "sumption over the baseline by 43% and 9%, respectively.\n",
            "Frequency changes Figure 9 shows the average GPU fre-\n",
            "quency over time for (1) the whole cluster, (2) the pool\n",
            "serving short requests, and (3) the pool serving long requests.\n",
            "Average frequency is always significantly lower than the\n",
            "maximum allowed frequency (1980 MHz) that is used by\n",
            "the baseline. DynamoLLM effectively accommodates different\n",
            "request types by operating their pools at different frequencies.\n",
            "Sharding changes Figure 10 shows the number of GPUs\n",
            "used for different model parallelisms (TP2, TP4 and TP8)\n",
            "for the whole cluster and for the individual pools (SL, ML\n",
            "and LL). The figure also shows the load over time that a\n",
            "given pool experiences. Different pools operate with different\n",
            "model parallelisms and DynamoLLM efficiently changes the\n",
            "parallelism as the load changes.\n",
            "C. Sensitivity Studies\n",
            "Sensitivity to predictor accuracy We analyze how the ac-\n",
            "curacy of the prediction models affects the overall system\n",
            "Energy\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "Energy [kWh]\n",
            "Performance\n",
            "0.0\n",
            "0.2\n",
            "0.4\n",
            "0.6\n",
            "TTFT [s]\n",
            "SinglePool\n",
            "Dyn-100%\n",
            "Dyn-90%\n",
            "Dyn-80%\n",
            "Dyn-60%\n",
            "Dyn-50%\n",
            "Fig. 11: Energy and performance with different accuracy\n",
            "running open-source Llama2-70B model [67] with 1-hour\n",
            "open-source production traces [50].\n",
            "Low Load\n",
            "Medium Load\n",
            "High Load\n",
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "Energy [kWh]\n",
            "SinglePool\n",
            "MultiPool\n",
            "ScaleInst\n",
            "ScaleShard\n",
            "ScaleFreq\n",
            "DynamoLLM\n",
            "Fig. 12: Energy with different levels of load running open-\n",
            "source Llama2-70B model [67].\n",
            "efficiency. We introduce bounded errors for the output length\n",
            "misclassification and measure the energy consumption with\n",
            "medium load. Figure 11 shows that the impact of the predictor\n",
            "accuracy is modest for both energy and performance. Com-\n",
            "pared to an environment with no error, an environment with\n",
            "an 40% error increases the energy consumption by 13% and\n",
            "TTFT by 7.3%. The reason for robustness to prediction errors\n",
            "is that DynamoLLM can promptly detect mis-predictions and\n",
            "re-configures the knobs accordingly.\n",
            "Sensitivity to load We evaluate DynamoLLM with different\n",
            "system loads. We generate Low, Medium, and High loads with\n",
            "a Poisson distribution for request inter-arrival times. Figure 12\n",
            "shows the energy consumption of the five evaluated systems\n",
            "with different load levels. With Low, Medium, and High load,\n",
            "DynamoLLM reduces the energy of SinglePool baseline by\n",
            "51%, 40%, and 23.4%, respectively. As the load increases, the\n",
            "energy savings of DynamoLLM reduce, because the system\n",
            "more frequently needs to operate at higher frequencies with\n",
            "higher levels of model parallelism.\n",
            "Sensitivity to number of pools Figure 13 shows the energy\n",
            "consumption and performance (TTFT) of DynamoLLM with\n",
            "different number of request pools. Recall that our chosen\n",
            "design has 9 pools. By adding too many pools (12 or 16),\n",
            "the system gets fragmented, and the idle energy of GPUs\n",
            "results in the overall energy increase. Reducing the number\n",
            "of pools (2 or 4) prevents the system from fine tuning\n",
            "the frequency and the model parallelism for specific request\n",
            "types. The performance improves by adding moderately more\n",
            "pools because it helps remove head of the line blocking and\n",
            "introduces more resources for execution.\n",
            "D. Long Cluster-Level Experiments\n",
            "We run longer experiments by running the 1-day traces for\n",
            "the Conversation service. The trace covers all invocations for\n",
            "10\n",
            "Energy\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "Energy [kWh]\n",
            "Performance\n",
            "0.0\n",
            "0.2\n",
            "0.4\n",
            "0.6\n",
            "TTFT [s]\n",
            "2Pool\n",
            "4Pool\n",
            "6Pool\n",
            "9Pool\n",
            "12Pool\n",
            "16Pool\n",
            "Fig. 13: Energy and performance with different number of\n",
            "pools (or request types) running open-source Llama2-70B\n",
            "model [67] with 1-hour open-source production traces [50].\n",
            "Conversation\n",
            "Coding\n",
            "0.0\n",
            "0.2\n",
            "0.4\n",
            "0.6\n",
            "0.8\n",
            "1.0\n",
            "Norm. Energy\n",
            "SinglePool\n",
            "MultiPool\n",
            "ScaleInst\n",
            "ScaleShard\n",
            "ScaleFreq\n",
            "DynamoLLM\n",
            "Fig. 14: Normalized energy consumption for the six evaluated\n",
            "systems with the week-long production traces running open-\n",
            "source Llama2-70B model [67].\n",
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "12\n",
            "14\n",
            "16\n",
            "18\n",
            "20\n",
            "22\n",
            "24\n",
            "Time [h]\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "Energy [kWh]\n",
            "Baseline\n",
            "DynamoLLM\n",
            "Fig. 15: Energy consumption of SinglePool and DynamoLLM\n",
            "with 1-day production traces running open-source Llama2-70B\n",
            "model [67].\n",
            "Mon\n",
            "Tue\n",
            "Wed\n",
            "Thu\n",
            "Fri\n",
            "Sat\n",
            "Sun\n",
            "Time\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "CO2 Emissions [kg/h]\n",
            "Baseline\n",
            "DynamoLLM\n",
            "Fig. 16: Carbon emissions over time for the week-long con-\n",
            "versation traces with DynamoLLM and SinglePool baseline\n",
            "running open-source Llama2-70B model [67].\n",
            "a subset of the service’s instances during a typical work day.\n",
            "We run the experiment for 24-hours on 11 H100 servers with\n",
            "SinglePool and scale the number of instances based on the load\n",
            "in DynamoLLM. Figure 15 shows the energy consumption\n",
            "over 5-minute intervals for the two systems. DynamoLLM\n",
            "reduces the energy consumption over the baseline during peak\n",
            "hours (when dynamic power dominates), and during the low\n",
            "utilization times (when idle power dominates). Over the whole\n",
            "day, DynamoLLM reduces the energy consumption by 42%.\n",
            "E. Large-Scale Simulations\n",
            "To generalize our insights to large-scale, we develop a\n",
            "discrete-time simulator that simulates the energy consumption\n",
            "of different systems using production traces. Figure 14 shows\n",
            "the normalized energy consumption for the five evaluated\n",
            "systems using 1-week traces for Conversation and Coding\n",
            "services. DynamoLLM significantly reduces the energy con-\n",
            "sumption for both types of services. DynamoLLM operates\n",
            "in higher energy-efficient modes for the Conversation service\n",
            "due to its typically shorter input lengths (ML dominant request\n",
            "type). On the other hand, the Coding service has deep valleys\n",
            "during the night and weekends. Thus, DynamoLLM exploits\n",
            "the periods of low load to save energy. DynamoLLM reduces\n",
            "the energy consumption over the baseline by 47% and 56%\n",
            "for the Conversation and Coding services, respectively.\n",
            "F. Cost and Carbon Emission\n",
            "User cost DynamoLLM reduces the operational cost for users\n",
            "by minimizing the number of GPUs and optimizing their\n",
            "energy efficiency. The number of GPU servers for the week-\n",
            "long experiments reduces from 40 to 24.6 on average (38.5%\n",
            "cost reduction). Using the current GPU VM pricing [8], this\n",
            "saves $1362.7/hour. By reducing the energy consumption,\n",
            "DynamoLLM reduces the associated energy costs by up to\n",
            "56%. As energy costs [28] are currently substantially lower\n",
            "than GPU costs, this translates to only $4.4/hour savings.\n",
            "Carbon emissions The energy consumption translates into the\n",
            "amount of operational CO2 emissions. We use the traces of\n",
            "carbon intensity [2] for a week-long period from multiple grids\n",
            "and map the carbon intensity to the energy consumption over\n",
            "time for the SinglePool baseline and DynamoLLM. Figure 16\n",
            "shows the operational carbon emissions over time for the\n",
            "two systems for CAISO [1]. SinglePool and DynamoLLM\n",
            "consume 5t and 3.1t/week of CO2. These substantial savings\n",
            "(38%) make a step towards sustainable LLM environments.\n",
            "VI. RELATED WORK\n",
            "Cluster resource and power management A rich body of\n",
            "work seeks to improve resource efficiency under the SLO\n",
            "constraints through resource management for a wide range\n",
            "of latency sensitive workloads, such as microservices [76]\n",
            "and DL workloads, through effective resource sharing [6],\n",
            "[43], [51], dynamic allocation [71], and hardware reconfigura-\n",
            "tion [24]. Others focus on approaches that enable safe power\n",
            "management and oversubscription [16], [29], [49] leveraging\n",
            "workload characteristics [25], [75] and system state [62].\n",
            "Energy-efficient workloads Prior works focused on energy-\n",
            "efficiency for CPU workloads [15], [31], [43], [61], and\n",
            "researchers started exploring unique energy properties of GPU\n",
            "workloads [58], [65], [74]. Recent schemes build on top and\n",
            "manage energy and power consumption for DNN inference\n",
            "and training [69], [70], [72] through frequency scaling [20],\n",
            "[23], [40], [41], [64], [77], [82], autoscaling [22], and re-\n",
            "source partitioning and mapping [18], [64]. We show that\n",
            "improving energy efficiency for LLM inference necessitates\n",
            "a comprehensive view of all available knobs. DynamoLLM is\n",
            "holistic framework that dynamically reconfigures all the knobs\n",
            "considering the diversity and dynamism of requests and loads.\n",
            "11\n",
            "Efficient LLM inference serving Recent works propose\n",
            "approaches to improve LLM inference efficiency through het-\n",
            "erogeneous resources [4] and platforms [35], memory and key-\n",
            "value cache management [9], [26], and node- and cluster-level\n",
            "scheduling [3], [30], [42], [48], [50], [73], [81]. While these\n",
            "studies focus on improving throughput or latency, we show\n",
            "that optimizing energy efficiency for LLM inference exhibits\n",
            "distinct trade-offs between performance, energy consumption,\n",
            "and overheads and thus requires a comprehensive framework.\n",
            "VII. CONCLUSION\n",
            "We present DynamoLLM, the first energy-management\n",
            "framework for LLM inference clusters. DynamoLLM exploits\n",
            "heterogeneity in inference compute properties and fluctuations\n",
            "in inference workloads to save energy. The system automat-\n",
            "ically and dynamically configures the energy-optimal organi-\n",
            "zation of the cluster (number of instances, model parallelism\n",
            "and GPU frequency) while performing under performance\n",
            "guarantees. DynamoLLM reduces energy, carbon emissions\n",
            "and cost to the customer by 53%, 38% and 61%, respectively.\n",
            "REFERENCES\n",
            "[1] “CAISO\n",
            "carbon\n",
            "emission,”\n",
            "https://www.caiso.com/todays-\n",
            "outlook/emissions, 2024.\n",
            "[2] “WattTime,” https://watttime.org, 2024.\n",
            "[3] A. Agrawal, A. Panwar, J. Mohan, N. Kwatra, B. S. Gulavani, and\n",
            "R. Ramjee, “SARATHI: Efficient LLM Inference by Piggybacking\n",
            "Decodes with Chunked Prefills,” 2023.\n",
            "[4] K. Alizadeh, I. Mirzadeh, D. Belenko, K. Khatamifard, M. Cho, C. C. D.\n",
            "Mundo, M. Rastegari, and M. Farajtabar, “LLM in a flash: Efficient\n",
            "Large Language Model Inference with Limited Memory,” 2024.\n",
            "[5] J. Bowne, “Using large language models in learning and teaching,”\n",
            "https://biomedicalsciences.unimelb.edu.au/study/dlh/assets/documents/\n",
            "large-language-models-in-education/llms-in-education, 2024.\n",
            "[6] S. Chen, C. Delimitrou, and J. F. Mart´ınez, “PARTIES: QoS-Aware\n",
            "Resource Partitioning for Multiple Interactive Services,” in Proceedings\n",
            "of the Twenty-Fourth International Conference on Architectural Support\n",
            "for Programming Languages and Operating Systems (ASPLOS ’19),\n",
            "2019.\n",
            "[7] S. Chen, A. Jin, C. Delimitrou, and J. F. Mart´ınez, “ReTail: Opting for\n",
            "Learning Simplicity to Enable QoS-Aware Power Management in the\n",
            "Cloud,” in Proceedings of the IEEE International Symposium on High-\n",
            "Performance Computer Architecture (HPCA ’22), 2022.\n",
            "[8] Cloud Price, “Standard ND96is H100 v5,” https://cloudprice.net/vm/\n",
            "Standard ND96is H100 v5, 2024.\n",
            "[9] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R´e, “FlashAttention: Fast\n",
            "and Memory-Efficient Exact Attention with IO-Awareness,” 2022.\n",
            "[10] C. Delimitrou and C. Kozyrakis, “Amdahl’s Law for Tail Latency,”\n",
            "Commun. ACM, vol. 61, no. 8, jul 2018.\n",
            "[11] D. Ding, A. Mallick, C. Wang, R. Sim, S. Mukherjee, V. Ruehle,\n",
            "L. V. S. Lakshmanan, and A. Awadallah, “Hybrid LLM: Cost-Efficient\n",
            "and Quality-Aware Query Routing,” in Proceedings of the International\n",
            "Conference on Learning Representations (ICLR ’24), 2024.\n",
            "[12] A. Faiz, S. Kaneda, R. Wang, R. Osi, P. Sharma, F. Chen, and L. Jiang,\n",
            "“LLMCarbon: Modeling the end-to-end Carbon Footprint of Large\n",
            "Language Models,” arXiv preprint arXiv:2309.14393, 2024.\n",
            "[13] GitHub, “The world’s most widely adopted ai developer tool,” https:\n",
            "//github.com/features/copilot, 2024.\n",
            "[14] M. Halpern, B. Boroujerdian, T. Mummert, E. Duesterwald, and\n",
            "V. Janapa Reddi, “One Size Does Not Fit All: Quantifying and Exposing\n",
            "the Accuracy-Latency Trade-Off in Machine Learning Cloud Service\n",
            "APIs via Tolerance Tiers,” in Proceedings of the IEEE International\n",
            "Symposium on Performance Analysis of Systems and Software (ISPASS\n",
            "’19), 2019.\n",
            "[15] M. E. Haque, Y. He, S. Elnikety, T. D. Nguyen, R. Bianchini, and\n",
            "K. S. McKinley, “Exploiting Heterogeneity for Tail Latency and Energy\n",
            "Efficiency,” in Proceedings of the 50th Annual IEEE/ACM International\n",
            "Symposium on Microarchitecture (MICRO ’17), 2017.\n",
            "[16] C.-H. Hsu, Q. Deng, J. Mars, and L. Tang, “SmoothOperator: Reducing\n",
            "Power Fragmentation and Improving Power Utilization in Large-Scale\n",
            "Datacenters,” in Proceedings of the 23rd International Conference\n",
            "on Architectural Support for Programming Languages and Operating\n",
            "Systems (ASPLOS ’18), 2018.\n",
            "[17] C.-H. Hsu, Y. Zhang, M. A. Laurenzano, D. Meisner, T. Wenisch,\n",
            "J. Mars, L. Tang, and R. G. Dreslinski, “Adrenaline: Pinpointing and\n",
            "reining in tail queries with quick voltage boosting,” in Proceedings of\n",
            "the IEEE 21st International Symposium on High Performance Computer\n",
            "Architecture (HPCA ’15), 2015.\n",
            "[18] A. Jahanshahi, M. Rezvani, and D. Wong, “WattWiser: Power and\n",
            "Resource-Efficient Scheduling for Multi-Model Multi-GPU Inference\n",
            "Servers,” in Proceedings of the 14th International Green and Sustainable\n",
            "Computing Conference (IGSC ’23), 2023.\n",
            "[19] Y. Jin, C.-F. Wu, D. Brooks, and G.-Y. Wei, “ S3: Increasing GPU\n",
            "utilization during generative inference for higher throughput,” in Pro-\n",
            "ceedings of the Advances in Neural Information Processing Systems\n",
            "(NeurIPS ’23), 2023.\n",
            "[20] A. K. Kakolyris, D. Masouros, S. Xydis, and D. Soudris, “SLO-\n",
            "aware GPU DVFS for Energy-efficient LLM Inference Serving,” IEEE\n",
            "Computer Architecture Letters, 2024.\n",
            "[21] H. Kasture, D. B. Bartolini, N. Beckmann, and D. Sanchez, “Rubik:\n",
            "Fast analytical power management for latency-critical systems,” in\n",
            "Proceedings of the 48th Annual IEEE/ACM International Symposium\n",
            "on Microarchitecture (MICRO ’15), 2015.\n",
            "[22] Y. G. Kim and C.-J. Wu, “AutoScale: Energy Efficiency Optimization\n",
            "for Stochastic Edge Inference Using Reinforcement Learning,” in Pro-\n",
            "ceedings of the 53rd Annual IEEE/ACM International Symposium on\n",
            "Microarchitecture (MICRO ’20), 2020.\n",
            "[23] T. Komoda, S. Hayashi, T. Nakada, S. Miwa, and H. Nakamura, “Power\n",
            "capping of cpu-gpu heterogeneous systems through coordinating dvfs\n",
            "and task mapping,” in Proceedings of the IEEE 31st International\n",
            "Conference on Computer Design (ICCD ’13), 2013.\n",
            "[24] N. Kulkarni, G. Gonzalez-Pumariega, A. Khurana, C. A. Shoemaker,\n",
            "C. Delimitrou, and D. H. Albonesi, “CuttleSys: Data-Driven Resource\n",
            "Management for Interactive Services on Reconfigurable Multicores,” in\n",
            "Proceedings of the 53rd Annual IEEE/ACM International Symposium on\n",
            "Microarchitecture (MICRO ’20), 2020.\n",
            "[25] A. G. Kumbhare, R. Azimi, I. Manousakis, A. Bonde, F. Frujeri,\n",
            "N. Mahalingam, P. A. Misra, S. A. Javadi, B. Schroeder, M. Fontoura,\n",
            "and R. Bianchini, “Prediction-Based Power Oversubscription in Cloud\n",
            "Platforms,” in Proceedings of the USENIX Annual Technical Conference\n",
            "(USENIX ATC ’21), 2021.\n",
            "[26] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E.\n",
            "Gonzalez, H. Zhang, and I. Stoica, “Efficient Memory Management for\n",
            "Large Language Model Serving with PagedAttention,” in Proceedings\n",
            "of the 29th Symposium on Operating Systems Principles (SOSP ’23),\n",
            "2023.\n",
            "[27] M. Lammertyn, “60+ ChatGPT Statistics And Facts You Need to Know\n",
            "in 2024,” https://blog.invgate.com/chatgpt-statistics, 2024.\n",
            "[28] LCG\n",
            "Consulting,\n",
            "“Energy\n",
            "Online:\n",
            "ERCOT\n",
            "Real\n",
            "Time\n",
            "Price,”\n",
            "https://energyonline.com/Data/GenericData.aspx?DataId=4&ERCOT\n",
            "Real-time Price, 2024.\n",
            "[29] S. Li, X. Wang, X. Zhang, V. Kontorinis, S. Kodakara, D. Lo,\n",
            "and P. Ranganathan, “Thunderbolt: Throughput-Optimized, Quality-of-\n",
            "Service-Aware Power Capping at Scale,” in Proceedings of the 14th\n",
            "USENIX Symposium on Operating Systems Design and Implementation\n",
            "(OSDI ’20), 2020.\n",
            "[30] Z. Li, L. Zheng, Y. Zhong, V. Liu, Y. Sheng, X. Jin, Y. Huang,\n",
            "Z. Chen, H. Zhang, J. E. Gonzalez, and I. Stoica, “AlpaServe: Statistical\n",
            "Multiplexing with Model Parallelism for Deep Learning Serving,” in\n",
            "Proceedings of the 17th USENIX Symposium on Operating Systems\n",
            "Design and Implementation (OSDI ’23), 2023.\n",
            "[31] D. Lo, L. Cheng, R. Govindaraju, L. A. Barroso, and C. Kozyrakis,\n",
            "“Towards energy proportionality for large-scale latency-critical work-\n",
            "loads,” in Proceedings of the ACM/IEEE 41st International Symposium\n",
            "on Computer Architecture (ISCA ’14), 2014.\n",
            "[32] Meta, “Llama2-13B,” https://huggingface.co/meta-llama/Llama-2-13b,\n",
            "2024.\n",
            "[33] Meta, “Llama2-70B,” https://huggingface.co/meta-llama/Llama-2-70b,\n",
            "2024.\n",
            "[34] Meta, “Llama3-70B,” https://huggingface.co/meta-llama/Meta-Llama-3-\n",
            "70B-Instruct, 2024.\n",
            "12\n",
            "[35] X. Miao, C. Shi, J. Duan, X. Xi, D. Lin, B. Cui, and Z. Jia, “SpotServe:\n",
            "Serving Generative Large Language Models on Preemptible Instances,”\n",
            "in Proceedings of the 29th ACM International Conference on Archi-\n",
            "tectural Support for Programming Languages and Operating Systems\n",
            "(ASPLOS ’24), 2024.\n",
            "[36] Microsoft Azure, “ND H100 v5-series,” https://learn.microsoft.com/en-\n",
            "us/azure/virtual-machines/nd-h100-v5-series, 2024.\n",
            "[37] A. Mirhosseini and T. Wenisch, “µSteal: A Theory-Backed Framework\n",
            "for Preemptive Work and Resource Stealing in Mixed-Criticality Mi-\n",
            "croservices,” in Proceedings of the ACM International Conference on\n",
            "Supercomputing (ICS ’21), 2021.\n",
            "[38] Mistral AI, “The Mixtral-8x22B Large Language Model,” https://\n",
            "huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1, 2024.\n",
            "[39] Mistral AI, “The Mixtral-8x7B Large Language Model,” https://\n",
            "huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1, 2024.\n",
            "[40] S. M. Nabavinejad, S. Reda, and M. Ebrahimi, “BatchSizer: Power-\n",
            "Performance Trade-off for DNN Inference,” in Proceedings of the 26th\n",
            "Asia and South Pacific Design Automation Conference (ASP-DAC ’21),\n",
            "2021.\n",
            "[41] S. M. Nabavinejad, S. Reda, and M. Ebrahimi, “Coordinated batching\n",
            "and dvfs for dnn inference on gpu accelerators,” IEEE Transactions on\n",
            "Parallel and Distributed Systems, vol. 33, no. 10, 2022.\n",
            "[42] K. K. W. Ng, H. M. Demoulin, and V. Liu, “Paella: Low-latency Model\n",
            "Serving with Software-defined GPU Scheduling,” in Proceedings of the\n",
            "29th Symposium on Operating Systems Principles (SOSP ’23), 2023.\n",
            "[43] R. Nishtala, V. Petrucci, P. Carpenter, and M. Sjalander, “Twig: Multi-\n",
            "Agent Task Management for Colocated Latency-Critical Cloud Ser-\n",
            "vices,” in Proceedings of the IEEE International Symposium on High\n",
            "Performance Computer Architecture (HPCA ’20), 2020.\n",
            "[44] NVIDIA, “DGX H100: AI for Enterprise,” https://www.nvidia.com/en-\n",
            "us/data-center/dgx-h100/, 2024.\n",
            "[45] NVIDIA, “NVLink and NVLink Switch,” https://www.nvidia.com/en-\n",
            "us/data-center/nvlink/, 2024.\n",
            "[46] NVIDIA, “System Management Interface SMI,” https://developer.nvidia.\n",
            "com/system-management-interface, 2024.\n",
            "[47] NVIDIA, “TensorRT-LLM’s Documentation,” https://nvidia.github.io/\n",
            "TensorRT-LLM/, 2024.\n",
            "[48] H. Oh, K. Kim, J. Kim, S. Kim, J. Lee, D.-s. Chang, and J. Seo, “Ex-\n",
            "eGPT: Constraint-Aware Resource Scheduling for LLM Inference,” in\n",
            "Proceedings of the 29th ACM International Conference on Architectural\n",
            "Support for Programming Languages and Operating Systems (ASPLOS\n",
            "’24), 2024.\n",
            "[49] P. Patel, E. Choukse, C. Zhang, I. Goiri, B. Warrier, N. Mahalingam,\n",
            "and R. Bianchini, “Characterizing Power Management Opportunities for\n",
            "LLMs in the Cloud,” in Proceedings of the 29th ACM International\n",
            "Conference on Architectural Support for Programming Languages and\n",
            "Operating Systems (ASPLOS ’24), 2024.\n",
            "[50] P. Patel, E. Choukse, C. Zhang, A. Shah, I. Goiri, S. Maleki, and\n",
            "R. Bianchini, “Splitwise: Efficient generative LLM inference using phase\n",
            "splitting,” in Proceedings of the 51st Annual International Symposium\n",
            "on Computer Architecture (ISCA ’24), 2024.\n",
            "[51] T. Patel and D. Tiwari, “CLITE: Efficient and QoS-Aware Co-Location\n",
            "of Multiple Latency-Critical Jobs for Warehouse Scale Computers,” in\n",
            "Proceedings of the IEEE International Symposium on High Performance\n",
            "Computer Architecture (HPCA ’20), 2020.\n",
            "[52] C. Peng, X. Yang, A. Chen, K. Smith, N. PourNejatian, A. Costa,\n",
            "C. Martin, M. Flores, Y. Zhang, T. Magoc, G. Lipori, M. Duane,\n",
            "N. Ospina, M. Ahmed, W. Hogan, E. Shenkman, Y. Guo, J. Bian, and\n",
            "Y. Wu, “A study of generative large language model for medical research\n",
            "and healthcare,” npj Digital Medicine, 2023.\n",
            "[53] Python PuLP, “Optimization with PuLP,” https://coin-or.github.io/pulp/,\n",
            "2024.\n",
            "[54] Python SciPy, “SciPy Library,” https://scipy.org/, 2024.\n",
            "[55] H. Qiu, W. Mao, A. Patke, S. Cui, S. Jha, C. Wang, H. Franke,\n",
            "Z. T. Kalbarczyk, T. Bas¸ar, and R. K. Iyer, “Efficient Interactive LLM\n",
            "Serving with Proxy Model-based Sequence Length Prediction,” in The\n",
            "5th International Workshop on Cloud Intelligence / AIOps at ASPLOS\n",
            "2024, 2024.\n",
            "[56] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n",
            "“Language models are unsupervised multitask learners,” OpenAI blog,\n",
            "vol. 1, no. 8, p. 9, 2019.\n",
            "[57] F. Romero, Q. Li, N. J. Yadwadkar, and C. Kozyrakis, “INFaaS: Au-\n",
            "tomated Model-less Inference Serving,” in Proceedings of the USENIX\n",
            "Annual Technical Conference (USENIX ATC ’21), 2021.\n",
            "[58] S. Samsi, D. Zhao, J. McDonald, B. Li, A. Michaleas, M. Jones,\n",
            "W. Bergeron, J. Kepner, D. Tiwari, and V. Gadepally, “From words\n",
            "to watts: Benchmarking the energy costs of large language model infer-\n",
            "ence,” in 2023 IEEE High Performance Extreme Computing Conference\n",
            "(HPEC).\n",
            "IEEE, 2023, pp. 1–9.\n",
            "[59] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow,\n",
            "R. Castagn´e, A. S. Luccioni, F. Yvon, M. Gall´e et al., “BLOOM:\n",
            "A 176b-parameter open-access multilingual language model,” arXiv\n",
            "preprint arXiv:2211.05100, 2022.\n",
            "[60] J. Stojkovic, E. Choukse, C. Zhang, I. Goiri, and J. Torrellas, “Towards\n",
            "Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM\n",
            "Inference,” arXiv e-prints, p. arXiv:2403.20306, Mar. 2024.\n",
            "[61] J. Stojkovic, N. Iliakopoulou, T. Xu, H. Franke, and J. Torrellas,\n",
            "“EcoFaaS: Rethinking the Design of Serverless Environments for Energy\n",
            "Efficiency,” in Proceedings of the 51st Annual International Symposium\n",
            "on Computer Architecture (ISCA ’24), 2024.\n",
            "[62] J. Stojkovic, P. Misra, I. Goiri, S. Whitlock, E. Choukse, M. Das,\n",
            "C. Bansal, J. Lee, Z. Sun, H. Qiu, R. Zimmermann, S. Samal, B. Warrier,\n",
            "A. Raniwala, and R. Bianchini, “SmartOClock: Workload- and Risk-\n",
            "Aware Overclocking in the Cloud,” in Proceedings of the 51st Annual\n",
            "International Symposium on Computer Architecture (ISCA ’24), 2024.\n",
            "[63] K.\n",
            "Talamadupula,\n",
            "“A\n",
            "Guide\n",
            "to\n",
            "LLM\n",
            "Inference\n",
            "Performance\n",
            "Monitoring,” https://symbl.ai/developers/blog/a-guide-to-llm-inference-\n",
            "performance-monitoring/, 2024.\n",
            "[64] T. Tambe, C. Hooper, L. Pentecost, T. Jia, E.-Y. Yang, M. Donato,\n",
            "V. Sanh, P. Whatmough, A. M. Rush, D. Brooks, and G.-Y. Wei, “Edge-\n",
            "BERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-\n",
            "Task NLP Inference,” in Proceedings of the 54th Annual IEEE/ACM\n",
            "International Symposium on Microarchitecture (MICRO ’21), 2021.\n",
            "[65] Z. Tang, Y. Wang, Q. Wang, and X. Chu, “The Impact of GPU DVFS\n",
            "on the Energy and Performance of Deep Learning: an Empirical Study,”\n",
            "in Proceedings of the Tenth ACM International Conference on Future\n",
            "Energy Systems (e-Energy ’19), 2019.\n",
            "[66] Technology Innovation Institute (TII), “Falcon-180B,” 2024.\n",
            "[67] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\n",
            "N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama\n",
            "2: Open foundation and fine-tuned chat models,” arXiv preprint\n",
            "arXiv:2307.09288, 2023.\n",
            "[68] T. Varshney, “Build an llm-powered data agent for data analysis,”\n",
            "https://developer.nvidia.com/blog/build-an-llm-powered-data-agent-for-\n",
            "data-analysis/, 2024.\n",
            "[69] C. Wan, M. Santriaji, E. Rogers, H. Hoffmann, M. Maire, and S. Lu,\n",
            "“ALERT: Accurate learning for energy and timeliness,” in Proceedings\n",
            "of the USENIX Annual Technical Conference (USENIX ATC ’20), 2020.\n",
            "[70] F. Wang, W. Zhang, S. Lai, M. Hao, and Z. Wang, “Dynamic GPU\n",
            "Energy Optimization for Machine Learning Training Workloads,” IEEE\n",
            "Transactions on Parallel and Distributed Systems, 2022.\n",
            "[71] W. Xiao, S. Ren, Y. Li, Y. Zhang, P. Hou, Z. Li, Y. Feng, W. Lin, and\n",
            "Y. Jia, “AntMan: Dynamic scaling on GPU clusters for deep learning,”\n",
            "in Proceedings of the 14th USENIX Symposium on Operating Systems\n",
            "Design and Implementation (OSDI ’20), 2020.\n",
            "[72] J. You, J.-W. Chung, and M. Chowdhury, “Zeus: Understanding and\n",
            "optimizing GPU energy consumption of DNN training,” in Proceedings\n",
            "of the 20th USENIX Symposium on Networked Systems Design and\n",
            "Implementation (NSDI ’23), 2023.\n",
            "[73] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, “Orca: A\n",
            "Distributed Serving System for Transformer-Based Generative Models,”\n",
            "in Proceedings of the 16th USENIX Symposium on Operating Systems\n",
            "Design and Implementation (OSDI ’22), 2022.\n",
            "[74] J. Yu, J. Kim, and E. Seo, “Know Your Enemy To Save Cloud Energy:\n",
            "Energy-Performance Characterization of Machine Learning Serving,” in\n",
            "2023 IEEE International Symposium on High-Performance Computer\n",
            "Architecture (HPCA), 2023.\n",
            "[75] C. Zhang, A. G. Kumbhare, I. Manousakis, D. Zhang, P. A. Misra,\n",
            "R. Assis, K. Woolcock, N. Mahalingam, B. Warrier, D. Gauthier,\n",
            "L. Kunnath, S. Solomon, O. Morales, M. Fontoura, and R. Bianchini,\n",
            "“Flex: High-Availability Datacenters with Zero Reserved Power,” in\n",
            "Proceedings of the 48th Annual International Symposium on Computer\n",
            "Architecture (ISCA ’21), 2021.\n",
            "[76] Y. Zhang, W. Hua, Z. Zhou, G. E. Suh, and C. Delimitrou, “Sinan: ML-\n",
            "Based and QoS-Aware Resource Management for Cloud Microservices,”\n",
            "in Proceedings of the 26th ACM International Conference on Archi-\n",
            "tectural Support for Programming Languages and Operating Systems\n",
            "(ASPLOS ’21), 2021.\n",
            "13\n",
            "[77] Y. Zhang, Q. Wang, Z. Lin, P. Xu, and B. Wang, “Improving gpu energy\n",
            "efficiency through an application-transparent frequency scaling policy\n",
            "with performance assurance,” in Proceedings of the Nineteenth European\n",
            "Conference on Computer Systems (EuroSys ’24), 2024.\n",
            "[78] Y. Z. Zhao, D. W. Wu, and J. Wang, “ALISA: Accelerating Large Lan-\n",
            "guage Model Inference via Sparsity-Aware KV Caching,” in Proceedings\n",
            "of the 51st Annual International Symposium on Computer Architecture\n",
            "(ISCA ’24), 2024.\n",
            "[79] Z. Zheng, X. Ren, F. Xue, Y. Luo, X. Jiang, and Y. You, “Response\n",
            "Length Perception and Sequence Scheduling: An LLM-Empowered\n",
            "LLM Inference Pipeline,” in Proceedings of the Advances in Neural\n",
            "Information Processing Systems (NeurIPS ’23), 2023.\n",
            "[80] L. Zhou, L. N. Bhuyan, and K. K. Ramakrishnan, “Gemini: Learning\n",
            "to Manage CPU Power for Latency-Critical Search Engines,” in Pro-\n",
            "ceedings of the 53rd Annual IEEE/ACM International Symposium on\n",
            "Microarchitecture (MICRO ’20), 2020.\n",
            "[81] Z. Zhou, X. Wei, J. Zhang, and G. Sun, “PetS: A unified framework for\n",
            "Parameter-Efficient transformers serving,” in Proceedings of the USENIX\n",
            "Annual Technical Conference (USENIX ATC ’22), 2022.\n",
            "[82] P. Zou, A. Li, K. Barker, and R. Ge, “Indicator-Directed Dynamic Power\n",
            "Management for Iterative Workloads on GPU-Accelerated Systems,” in\n",
            "Proceedings of the 20th IEEE/ACM International Symposium on Cluster,\n",
            "Cloud and Internet Computing (CCGRID ’20), 2020.\n",
            "14\n",
            "\n",
            "Large Language Model (LLM)-enabled In-context\n",
            "Learning for Wireless Network Optimization: A\n",
            "Case Study of Power Control\n",
            "Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu,\n",
            "Xue Liu, Fellow, IEEE, and Charlie Zhang, Fellow, IEEE.\n",
            "Abstract—Large language model (LLM) has recently been\n",
            "considered a promising technique for many fields. This work\n",
            "explores LLM-based wireless network optimization via in-context\n",
            "learning. To showcase the potential of LLM technologies, we\n",
            "consider the base station (BS) power control as a case study,\n",
            "a fundamental but crucial technique that is widely investigated\n",
            "in wireless networks. Different from existing machine learning\n",
            "(ML) methods, our proposed in-context learning algorithm relies\n",
            "on LLM’s inference capabilities. It avoids the complexity of\n",
            "tedious model training and hyper-parameter fine-tuning, which is\n",
            "a well-known bottleneck of many ML algorithms. Specifically, the\n",
            "proposed algorithm first describes the target task via formatted\n",
            "natural language, and then designs the in-context learning\n",
            "framework and demonstration examples. After that, it considers\n",
            "two cases, namely discrete-state and continuous-state problems,\n",
            "and proposes state-based and ranking-based methods to select\n",
            "appropriate examples for these two cases, respectively. Finally, the\n",
            "simulations demonstrate that the proposed algorithm can achieve\n",
            "comparable performance as conventional deep reinforcement\n",
            "learning (DRL) techniques without dedicated model training or\n",
            "fine-tuning. Such an efficient and low-complexity approach has\n",
            "great potential for future wireless network optimization.\n",
            "Index Terms—Large language model, in-context learning, net-\n",
            "work optimization, transmission power control\n",
            "I. INTRODUCTION\n",
            "From LTE and 5G to envisioned 6G, wireless networks are\n",
            "increasingly complicated with diverse application scenarios\n",
            "and novel signal processing and transmission techniques, e.g.,\n",
            "unmanned aerial vehicle (UAV), vehicle-to-everything (V2X),\n",
            "mmWave and THz networks, reconfigurable intelligent sur-\n",
            "face, etc [1]. The constantly evolving network architecture re-\n",
            "quires more efficient management schemes, and most existing\n",
            "network optimization methods can be summarized into two\n",
            "main approaches: convex optimization and machine learning\n",
            "(ML) algorithms. Specifically, convex optimization usually\n",
            "needs dedicated problem formulation for each specific task,\n",
            "then transforms the objective function or constraints into con-\n",
            "vex forms. By contrast, ML algorithms, such as reinforcement\n",
            "learning, have lower requirements for problem formulations,\n",
            "but the tedious model training and fine-tuning indicate a large\n",
            "Hao\n",
            "Zhou,\n",
            "Chengming\n",
            "Hu,\n",
            "Dun\n",
            "Yuan,\n",
            "Ye\n",
            "Yuan,\n",
            "and\n",
            "Xue\n",
            "Liu\n",
            "are with the School of Computer Science, McGill University, Mon-\n",
            "treal, QC H3A 0E9, Canada. (mails:hao.zhou4, chengming.hu, dun.yuan,\n",
            "ye.yuan3@mail.mcgill.ca, xueliu@cs.mcgill.ca). Di Wu is with the School\n",
            "of Electrical and Computer Engineering, McGill University, Montreal,\n",
            "QC H3A 0E9, Canada. (email: di.wu5@mcgill.ca). Charlie Zhang is\n",
            "with Samsung Research America, Plano, Texas, TX 75023, USA. (email:\n",
            "jianzhong.z@samsung.com).\n",
            "number of iterations [2]. Therefore, these potential issues, e.g.,\n",
            "problem-specific transformation and relaxation, hyperparame-\n",
            "ter tuning, and long training iterations, have become obstacles\n",
            "to further improve the efficiency of next-generation networks.\n",
            "Large language models (LLMs) are recently considered\n",
            "revolutionary technologies that have been successfully ap-\n",
            "plied to education, finance, healthcare, biology, etc [3]. The\n",
            "great potential of LLM technologies also provides promis-\n",
            "ing opportunities for network management and optimization\n",
            "[4]. For instance, a promising feature of LLMs is learning\n",
            "from language-based descriptions and demonstrations, which\n",
            "is known as in-context learning. Compared with convex opti-\n",
            "mization or conventional ML approaches, in-context learning\n",
            "has multiple advantages [5]: 1) in-context learning relies on\n",
            "LLM’s inference process, and it avoids the complexity of\n",
            "dedicated model training and fine-tuning, which is a well-\n",
            "known bottleneck for many ML techniques; 2) in-context\n",
            "learning allows natural language-based task design and imple-\n",
            "mentation, and the operator can easily formulate the target task\n",
            "using human language and instructions. Such a user-friendly\n",
            "approach can also significantly lower the requirements for\n",
            "professional knowledge when solving specific tasks.\n",
            "LLM-enabled wireless networks have recently attracted\n",
            "considerable interest, e.g., 6G edge intelligence [6], network\n",
            "intrusion detection and LLM-enhanced reconfigurable intelli-\n",
            "gent surface for internet of vehicles (IoV) [7], [8]. By contrast,\n",
            "this work focuses on optimization problems, and it considers\n",
            "base station (BS) power control as a case study, exploring\n",
            "the potential of using LLM to solve optimization problems.\n",
            "The power control of cellular networks has been extensively\n",
            "studied with diverse objectives and algorithms, i.e., convex\n",
            "optimization, game theory, reinforcement learning [9], etc.\n",
            "These studies prove that power control is a fundamental\n",
            "and critical technique to improve energy efficiency, reduce\n",
            "interference, and save power consumption [10]. Therefore,\n",
            "given such crucial importance, we select BS power control\n",
            "as a case study, and explore the potential of state-of-the-art\n",
            "LLM technologies for wireless network optimization.\n",
            "In particular, our proposed LLM-enabled in-context learning\n",
            "algorithm first designs a natural language-based task descrip-\n",
            "tion, i.e., task goal, definition, and rules. The formatted task\n",
            "description, along with a set of selected examples, will become\n",
            "the prompt input for the LLM model. Then, the LLM model\n",
            "can utilize the task description and advisable examples to gen-\n",
            "arXiv:2408.00214v1  [eess.SY]  1 Aug 2024\n",
            "erate a decision based on the current environment state. After\n",
            "that, the output decision, environment state, and corresponding\n",
            "system performance will become a new example and be stored\n",
            "in an experience pool. Given the next environment state, we\n",
            "will select new advisable examples from the experience pool,\n",
            "serving as references for the next LLM decision-making. In\n",
            "addition, we further consider two scenarios, namely discrete-\n",
            "state and continuous-state problems, and propose state-based\n",
            "and ranking-based example selection methods for these two\n",
            "cases, respectively. Finally, we evaluate the proposed algorithm\n",
            "with various LLMs, e.g., Llama3-8b-instruct, Llama3-70b-\n",
            "instruct, and GPT-3.5 turbo, and the simulations prove that\n",
            "the proposed algorithm can achieve satisfactory performance.\n",
            "The main contribution of this work is that we explored\n",
            "LLM-enabled in-context learning for wireless network op-\n",
            "timization problems. Specifically, it proposed an in-context\n",
            "optimization technique that learns from the environment inter-\n",
            "actions without model training and fine-tuning. The simulation\n",
            "results show that our proposed algorithm enables LLM to\n",
            "learn from previous explorations and examples, and constantly\n",
            "improve the performance on target tasks. Such an efficient and\n",
            "training-free algorithm has great potential for wireless network\n",
            "optimization and management.\n",
            "II. SYSTEM MODEL\n",
            "This section introduces a BS power minimization problem,\n",
            "serving as a case study of the proposed in-context learning\n",
            "algorithm1. Considering a BS with Ub users, the achievable\n",
            "data rate Cb,u between BS b and user u is defined by [11]\n",
            "Cb,u =\n",
            "Kb\n",
            "P\n",
            "k=1\n",
            "dklog(1 +\n",
            "pb,khb,k,uγb,k,u\n",
            "P\n",
            "b′∈B−b\n",
            "pb′,k′hb′,k′,u′γb′,k′,u′+dkN0 ), (1)\n",
            "where Kb is the total number of resource blocks (RBs) in BS\n",
            "b, dk is the bandwidth of RB k, pb,k indicates the transmission\n",
            "power of BS b on RB k, hb,k,u defines the channel gain\n",
            "between BS b and user u on RB k, and N0 is the noise\n",
            "power density. For the RB allocation, γb,k,u ∈{0, 1} indicates\n",
            "whether RB k is allocated to the transmission for user u.\n",
            "For the interference, B−b represent the set of adjacent BSs\n",
            "except for BS b, pb′,k′hb′,k′,u′γb′,k′,u′ defines the inter-cell\n",
            "interference, and we assume orthogonal frequency-division\n",
            "multiplexing is applied to eliminate intra-cell interference.\n",
            "This work aims to minimize the BS transmission power and\n",
            "meanwhile satisfy the average data rate constraint [10]:\n",
            "min\n",
            "Pb\n",
            "X\n",
            "b∈B Pb\n",
            "(2)\n",
            "s.t. 0 ≤Pb ≤Pmax,\n",
            "(2a)\n",
            "Pb =\n",
            "XKb\n",
            "k=1 pb,k,\n",
            "(2b)\n",
            "XUb\n",
            "u=1 Cb,u/Ub ≥Cmin,\n",
            "(2c)\n",
            "1Various objectives and constraints can be defined for power control\n",
            "problems as summarized in [10], and here we select power minimization\n",
            "as a specific problem formulation.\n",
            "where Pb is the total transmission power of BS b and\n",
            "Pb = PKb\n",
            "k=1 pb,k, pb,k has been defined in equation (1) as the\n",
            "transmission power of RB k, Pmax is the maximum power,\n",
            "Ub is the total number of users, and Cmin is the average\n",
            "achievable data rate constraint. We assume Pb is equally\n",
            "allocated to all RBs, and a proportional fairness method is used\n",
            "for RB allocation, which has been widely used as a classic\n",
            "approach. Then we can better focus on LLM features.\n",
            "The control variable in equation (2) is the total BS transmis-\n",
            "sion power Pb, which needs to be dynamically adjusted based\n",
            "on the wireless environment, e.g., current user numbers or\n",
            "user-BS distances, to save power consumption and meanwhile\n",
            "maintain the average data rate. Problem (2) has been exten-\n",
            "sively investigated in existing studies, but this work differs\n",
            "from previous works by presenting a unique view from the\n",
            "perspective of natural language-based network optimization.\n",
            "Specifically, we propose a novel LLM-enabled in-context\n",
            "learning algorithm in the following Section III.\n",
            "III. IN-CONTEXT LEARNING-BASED OPTIMIZATION\n",
            "ALGORITHM\n",
            "In-context learning refers to the process that LLMs can learn\n",
            "from formatted natural language such as task descriptions and\n",
            "task solution demonstrations, to improve the performance on\n",
            "target tasks. In-context learning can be defined as [5]\n",
            "Dtask × Et × st × LLM ⇒at,\n",
            "(3)\n",
            "where Dtask is the task description, Et is the set of examples\n",
            "at time t, st is the environment state at time t that is associated\n",
            "with the target task , LLM indicates the LLM model, and at\n",
            "is the LLM output. For a sequential decision-making problem,\n",
            "we expect the LLM can utilize the initial task description\n",
            "Dtask, learn from the example set Et, and then make decision\n",
            "at based on current environment state st of the target task. In\n",
            "the following, we will introduce the design of task description\n",
            "Dtask and the selection of example set Et.\n",
            "A. Language-based Task Description\n",
            "Dtask is crucial to provide target task information to\n",
            "the LLM model. In particular, it involves “Task goal”,\n",
            "“Task definition”, and extra “Rules”. The following is a\n",
            "detailed task description we designed to prompt the LLM.\n",
            "Task description for BS transmission power control\n",
            "Task goal: You have a decision-making task for base\n",
            "station power control, and you need to select between\n",
            "4 power levels from 1 to 4.\n",
            "Task definition: You have to consider the specific user\n",
            "number of each case, which is the “base station user\n",
            "number”.\n",
            "Following are some examples {Example set}.\n",
            "Now I will give you a new condition to solve, the\n",
            "current BS user number is {Num BS user}.\n",
            "Rules: Now please select from “level 1”, “level 2”,\n",
            "“level 3”, and “level 4” based on the above examples.\n",
            "2\n",
            "Fig. 1: Overall design of the proposed LLM-enabled in-context learning for transmission power control.\n",
            "In particular, the Task goal first specifies a “decision-\n",
            "making task for base station power control”, and the goal is to\n",
            "“select between 4 power levels”2. Then the Task definition\n",
            "introduces the environment states we need to consider. For\n",
            "example, this work assumes the total user numbers may change\n",
            "dynamically, and then the LLM has to consider the “base\n",
            "station user number” of each case. In addition, it means that\n",
            "the environment state st in equation (3) refers to the total user\n",
            "number Ub in problem (2). After that, the example set Et is\n",
            "included by “Following are some examples....”, and we provide\n",
            "a new condition for the LLM to solve, which is associated\n",
            "with the current user number Ub. Finally, we set extra reply\n",
            "rules such as “select from ... based on the above examples”,\n",
            "indicating the LLM to focus on the decision-making process.\n",
            "The above task description provides a template to define\n",
            "optimization tasks by formatted natural language, avoiding the\n",
            "complexity of dedicated optimization model design. It is also\n",
            "user-friendly since the operator can easily add or remove task\n",
            "descriptions without requiring any professional knowledge of\n",
            "optimization techniques.\n",
            "B. In-context Learning Framework and Example Design\n",
            "Examples are of great importance in in-context learning,\n",
            "and they must be carefully selected because: 1) examples serve\n",
            "as crucial references for LLM decision-making, which means\n",
            "the LLM relies on examples to justify its decision; 2) due\n",
            "to the LLM context window size constraint3, it is impractical\n",
            "to send a large number of examples to the LLM. Moreover,\n",
            "there are many optimization problems with continuous envi-\n",
            "ronment states, which are very common in wireless networks,\n",
            "e.g., adjusting the BS transmission power based on user-BS\n",
            "distance. Such cases mean that there may be an infinite number\n",
            "of examples, and therefore identifying the most relevant and\n",
            "2Here we select 4 power levels as an example, which can be changed to\n",
            "any number of levels\n",
            "3The context window size indicates the largest number of tokens that can\n",
            "be sent to the LLM\n",
            "useful examples becomes challenging. This work defines an\n",
            "example E by\n",
            "E = {s, a, r(s, a)}, E ∈E\n",
            "(4)\n",
            "where s and a are environment state and decision, respectively.\n",
            "Inspired by reinforcement learning, we further define a reward\n",
            "value to evaluate the decision a by\n",
            "r = Ptarget −Pb −β\n",
            "(5)\n",
            "where Ptarget is a target power consumption, and Pb has been\n",
            "defined in problem (2) as the total power consumption of BS\n",
            "b. β is a penalty term, which is only applied when constraint\n",
            "(2c) is not satisfied. Then, r provides a comprehensive metric\n",
            "to evaluate the selected decision a under environment state s.\n",
            "Fig.1 shows the overall design of the proposed in-context\n",
            "learning algorithm for transmission power control. Specifi-\n",
            "cally, the above task description Dtask, current environment\n",
            "state st, and selected examples Et are integrated as input\n",
            "prompt as defined in equation (3), and then the LLM model\n",
            "will generate a power control decision at based on st and\n",
            "the experiences in Et. Then, the decision at is implemented,\n",
            "the achieved data rate Cb,u is collected, and the reward rt is\n",
            "calculated as equation (5). Et = {st, at, rt(st, at)} becomes\n",
            "a new example in the accumulated experience pool Epool in\n",
            "Fig.1. After that, based on the next environment state st+1, a\n",
            "new example set Et+1 is selected, and the selected examples\n",
            "are inserted into the task description with st+1, becoming a\n",
            "new prompt for the LLM model to generate at+1.\n",
            "C. State-based Example Selection for Discrete State Problems\n",
            "Selecting appropriate examples is critical for in-context\n",
            "learning since the LLM model learns from existing demon-\n",
            "strations to handle the target task. For problems with discrete\n",
            "environment states, relevant demonstrations can be easily\n",
            "identified by finding existing examples with the same states in\n",
            "the accumulated experience pool Epool. Considering a target\n",
            "3\n",
            "task with environment state value starget, the set of relevant\n",
            "examples can be identified by\n",
            "Erelevant =\n",
            "n\n",
            "E{s, a, r(s, a)}\n",
            "\f\f\fs = starget, E ∈Epool\n",
            "o\n",
            "(6)\n",
            "where Epool is the accumulated experience pool in Fig. 2.\n",
            "Given Erelevant, we can easily select recommended exam-\n",
            "ples with high reward, i.e., top-K examples, and inadvisable\n",
            "examples, e.g., examples with lower reward or violating the\n",
            "minimum data rate constraint.\n",
            "In addition, we include a well-known epsilon-greedy policy\n",
            "to balance exploration and exploitation.\n",
            "a =\n",
            "\u001a Random action selection,\n",
            "if rand < ϵ;\n",
            "LLM-based decision-making,\n",
            "else,\n",
            "(7)\n",
            "where ϵ is a predefined value, and rand is a random number\n",
            "between 0 and 1. Therefore, the random exploration in equa-\n",
            "tion (7) can constantly explore new examples, and then the\n",
            "LLM model can learn from better relevant examples Erelevant\n",
            "to improve the performance.\n",
            "D. Ranking-based Example Selection for Continuous State\n",
            "Problems\n",
            "Compared with discrete-state problems, environments with\n",
            "continuous states can be much more complicated. For instance,\n",
            "when using average user-BS distance as an environment state\n",
            "for BS transmission power control with a target task starget, it\n",
            "is unlikely to find a specific existing example E{s, a, r(s, a)}\n",
            "with s = starget, since starget is a random number within\n",
            "the BS maximum coverage distance. This problem may be\n",
            "solved by discretizing the continuous states into some discrete\n",
            "values, but this may still lead to a large number of states or\n",
            "extra errors. To this end, we define a new metric L for example\n",
            "selection with continuous states:\n",
            "L(E, starget) = r(s, a) −τ||s −starget||,\n",
            "(8)\n",
            "where L(E, starget) is a comprehensive metric to evaluate the\n",
            "usefulness of E = {s, a, r(s, a)} to the decision-making of\n",
            "starget, and ||s−starget|| is the L2 norm to define the distance\n",
            "between s and starget. Equation (8) aims to jointly consider\n",
            "the reward and states of example E, and τ is a weighting\n",
            "factor to balance the importance of higher reward r(s, a)\n",
            "and more similar states between s and starget. Specifically,\n",
            "a higher reward r(s, a) indicates that E includes a good\n",
            "action selection a under environment state s, and meanwhile\n",
            "lower ||s −starget|| value means the environment state s in\n",
            "E is more similar to starget. Therefore, we use L(E, starget)\n",
            "as a comprehensive metric, and then the recommended and\n",
            "inadvisable examples can be selected similarly as in Section\n",
            "III-C by using top-K methods.\n",
            "E. Baseline Algorithm\n",
            "We consider deep reinforcement learning (DRL) as a base-\n",
            "line since it is one of the most widely used ML algorithms to\n",
            "solve network optimization problems [12]. DRL can usually\n",
            "produce satisfactory optimization results, but it requires proper\n",
            "parameter fine-tuning and dedicated model training. This work\n",
            "investigates two scenarios, and the Markov decision processes\n",
            "(MDPs) are defined by: 1) for discrete-state problems, the state\n",
            "is defined by user numbers associated with the BS; 2) for\n",
            "continuous-state problems, we use average user-BS distance\n",
            "as a continuous-changing state. The action is defined by the\n",
            "BS power level, and the reward is shown as equation (5).\n",
            "IV. PERFORMANCE EVALUATION\n",
            "A. Simulation Settings\n",
            "We consider three adjacent small base stations (SBSs); the\n",
            "user number of each SBS randomly changes from 5 to 15,\n",
            "and the SBS’s coverage is 20 meters. The channel gain applies\n",
            "3GPP urban network models, and 2 cases are evaluated:\n",
            "Case I: Discrete states defined by user numbers of each SBS;\n",
            "Case II: Continuous states defined by average user-SBS dis-\n",
            "tance, which represents 2 kinds of network optimization prob-\n",
            "lems. Then, the simulation considers two main approaches:\n",
            "1) LLM-based method includes 3 models: Llama3-8b-\n",
            "instruct, Llama3-70b-instruct, and GPT-3.5 turbo. Llama3-8b\n",
            "is a small-scale LLM, while Llama3-70b and GPT-3.5 turbo\n",
            "are large models. Using LLM models with various sizes can\n",
            "better evaluate the capabilities of our proposed algorithms.\n",
            "2) DRL-based method as introduced in Section III-E. With\n",
            "dedicated model training, here we consider DRL as an optimal\n",
            "baseline since its capability has been demonstrated in many\n",
            "existing studies [11], [12].\n",
            "B. Simulation Results\n",
            "Fig. 2 shows the simulation results and comparisons, and\n",
            "the metrics include average reward, power consumption, and\n",
            "service quality (indicating the probability of satisfying the\n",
            "minimum average data rate constraint defined in equation (2)).\n",
            "Firstly, Fig. 2(a) presents the system reward and service\n",
            "quality of different LLMs under discrete state space. One can\n",
            "observe that both Llama3 LLMs achieve a comparable reward\n",
            "and service quality as the DRL baseline, while GPT-3.5 shows\n",
            "a lower reward and service quality. Fig. 2(a) demonstrates that\n",
            "the proposed in-context learning algorithm and state-based ex-\n",
            "ample selection method can provide satisfactory performance\n",
            "for problems with a limited number of environment states.\n",
            "Then, we consider more complicated scenarios with contin-\n",
            "uous states defined by the average user-BS distance. Fig. 2(b)\n",
            "and 2(c) present the reward and average power consumption,\n",
            "respectively. All LLM models achieve higher rewards and\n",
            "lower power consumption as the number of episodes increases\n",
            "and finally converge to stable values. The results demonstrate\n",
            "that LLMs can learn from previous examples and explorations\n",
            "and then improve the performance on target tasks.\n",
            "In addition, we observe the algorithm performance under\n",
            "different minimum data rate constraints. Fig. 2(d), 2(e), and\n",
            "2(f) present the average reward, power consumption, and\n",
            "service quality, respectively. Here, every value in the results\n",
            "is obtained by taking the average performance of converged\n",
            "episodes of corresponding LLMs as in Fig. 2(b) and 2(c).\n",
            "As expected, the simulation results show that increasing the\n",
            "minimum data rate constraint leads to lower reward, lower\n",
            "4\n",
            "(a) Discrete state space: System reward and\n",
            "service quality comparison of various LLMs.\n",
            "(b) Continuous state space: System reward\n",
            "comparison of various LLMs.\n",
            "(c) Continuous state space: Power consumption\n",
            "comparison of various LLMs.\n",
            "(d) Continuous state space: Average reward com-\n",
            "parison under different data rate constraints.\n",
            "(e) Continuous state space: Average power\n",
            "consumption comparison under different\n",
            "data rate constraints.\n",
            "(f) Continuous state space: Average service quality\n",
            "comparison under different data rate constraints.\n",
            "Fig. 2: Simulation results and comparisons\n",
            "service quality, and higher power consumption. Therefore, Fig.\n",
            "2(d), 2(e), and 2(f) demonstrate that the proposed algorithms\n",
            "can adapt to different optimization settings and then adjust\n",
            "their policies to improve the performance on target tasks.\n",
            "Note that the algorithm performance is also related to\n",
            "specific LLMs. Llama3 represents state-of-the-art LLM de-\n",
            "signs, while GPT-3.5 is an early LLM model. Therefore,\n",
            "it is reasonable that Llama3-8b and Llama3-70b maintain\n",
            "comparable performance as the DRL baseline, while GPT-\n",
            "3.5 turbo presents a worse performance in different tasks. For\n",
            "instance, when the minimum data rate constraint is 2 Mbps per\n",
            "user, GPT-3.5 has a 25% lower reward and 20% lower service\n",
            "quality than other LLMs. In summary, the simulations in Fig.\n",
            "2 demonstrate that our proposed in-context learning algorithm\n",
            "can achieve comparable performance as conventional DRL\n",
            "algorithms without dedicated model training and fine-tuning.\n",
            "V. CONCLUSION\n",
            "LLM is a promising technique for future wireless networks,\n",
            "and this work proposes an LLM-enabled in-context learning\n",
            "algorithm for BS transmission power control. The proposed\n",
            "algorithm can handle both discrete and continuous state prob-\n",
            "lems, and the simulations show that it achieves comparable\n",
            "performance as conventional DRL algorithms. This work\n",
            "demonstrates the great potential of in-context learning for\n",
            "handling network management and optimization problems.\n",
            "REFERENCES\n",
            "[1] Z. Zhang, Y. Xiao, Z. Ma, M. Xiao, Z. Ding, X. Lei, G. K. Karagiannidis,\n",
            "and P. Fan, “6g wireless networks: Vision, requirements, architecture,\n",
            "and key technologies,” IEEE vehicular technology magazine, vol. 14,\n",
            "no. 3, pp. 28–41.\n",
            "[2] H. Zhou, M. Erol-Kantarci, Y. Liu, and H. V. Poor, “A survey on model-\n",
            "based, heuristic, and machine learning optimization approaches in ris-\n",
            "aided wireless networks,” IEEE Communications Surveys & Tutorials,\n",
            "2023.\n",
            "[3] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang,\n",
            "J. Zhang, Z. Dong et al., “A survey of large language models,” arXiv\n",
            "preprint arXiv:2303.18223, 2023.\n",
            "[4] H. Zhou, C. Hu, Y. Yuan, Y. Cui, Y. Jin, C. Chen, H. Wu, D. Yuan,\n",
            "L. Jiang, D. Wu et al., “Large language model (LLM) for telecommu-\n",
            "nications: A comprehensive survey on principles, key techniques, and\n",
            "opportunities,” arXiv preprint arXiv:2405.10825, 2024.\n",
            "[5] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, and et al., “A survey on\n",
            "in-context learning,” arXiv preprint arXiv:2301.00234, 2022.\n",
            "[6] Z. Lin, G. Qu, Q. Chen, X. Chen, Z. Chen, and K. Huang, “Pushing large\n",
            "language models to the 6G edge: Vision, challenges, and opportunities,”\n",
            "arXiv preprint arXiv:2309.16739, 2023.\n",
            "[7] M. Fu, P. Wang, M. Liu, Z. Zhang, and X. Zhou, “Iov-bert-ids: Hybrid\n",
            "network intrusion detection system in iov using large language models,”\n",
            "IEEE Transactions on Vehicular Technology, 2024.\n",
            "[8] Q. Liu, J. Mu, D. ChenZhang, Y. Liu, and T. Hong, “Llm enhanced\n",
            "reconfigurable intelligent surface for energy-efficient and reliable 6g\n",
            "iov,” IEEE Transactions on Vehicular Technology, 2024.\n",
            "[9] F. H. C. Neto, D. C. Ara´ujo, M. P. Mota, T. F. Maciel, and A. L.\n",
            "de Almeida, “Uplink power control framework based on reinforcement\n",
            "learning for 5g networks,” IEEE Transactions on Vehicular Technology,\n",
            "vol. 70, no. 6, pp. 5734–5748, 2021.\n",
            "[10] M. Chiang, P. Hande, T. Lan, C. W. Tan et al., “Power control in\n",
            "wireless cellular networks,” Foundations and Trends® in Networking,\n",
            "vol. 2, no. 4, pp. 381–533, 2008.\n",
            "[11] H. Zhou, M. Erol-Kantarci, and H. V. Poor, “Learning from peers:\n",
            "Deep transfer reinforcement learning for joint radio and cache resource\n",
            "5\n",
            "allocation in 5G RAN slicing,” IEEE Transactions on Cognitive Com-\n",
            "munications and Networking, vol. 8, no. 4, pp. 1925–1941, 2022.\n",
            "[12] L. Zhang and Y.-C. Liang, “Deep reinforcement learning for multi-\n",
            "agent power control in heterogeneous networks,” IEEE Transactions on\n",
            "Wireless Communications, vol. 20, no. 4, pp. 2551–2564, 2020.\n",
            "6\n",
            "\n",
            "Hybrid Heterogeneous Clusters Can Lower the Energy\n",
            "Consumption of LLM Inference Workloads\n",
            "Grant Wilkins\n",
            "gfw27@cam.ac.uk\n",
            "University of Cambridge\n",
            "Cambridge, UK\n",
            "Srinivasan Keshav\n",
            "sk818@cam.ac.uk\n",
            "University of Cambridge\n",
            "Cambridge, UK\n",
            "Richard Mortier\n",
            "rmm1002@cam.ac.uk\n",
            "University of Cambridge\n",
            "Cambridge, UK\n",
            "ABSTRACT\n",
            "Both the training and use of Large Language Models (LLMs) require\n",
            "large amounts of energy. Their increasing popularity, therefore,\n",
            "raises critical concerns regarding the energy efficiency and sus-\n",
            "tainability of data centers that host them. This paper addresses the\n",
            "challenge of reducing energy consumption in data centers running\n",
            "LLMs. We propose a hybrid data center model that uses a cost-based\n",
            "scheduling framework to dynamically allocate LLM tasks across\n",
            "hardware accelerators that differ in their energy efficiencies and\n",
            "computational capabilities. Specifically, our workload-aware strat-\n",
            "egy determines whether tasks are processed on energy-efficient\n",
            "processors or high-performance GPUs based on the number of in-\n",
            "put and output tokens in a query. Our analysis of a representative\n",
            "LLM dataset, finds that this hybrid strategy can reduce CPU+GPU\n",
            "energy consumption by 7.5% compared to a workload-unaware\n",
            "baseline.\n",
            "CCS CONCEPTS\n",
            "• Computer systems organization →Heterogeneous (hybrid)\n",
            "systems; • Hardware →Impact on the environment.\n",
            "KEYWORDS\n",
            "sustainable computing, heterogeneous computing, large language\n",
            "models, artificial intelligence\n",
            "ACM Reference Format:\n",
            "Grant Wilkins, Srinivasan Keshav, and Richard Mortier. 2024. Hybrid Het-\n",
            "erogeneous Clusters Can Lower the Energy Consumption of LLM Inference\n",
            "Workloads. In The 15th ACM International Conference on Future and Sustain-\n",
            "able Energy Systems (E-Energy ’24), June 04–07, 2024, Singapore, Singapore.\n",
            "ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3632775.3662830\n",
            "1\n",
            "INTRODUCTION\n",
            "Large Language Models (LLMs) such as OpenAI’s GPT-4 [24] and\n",
            "Google’s PaLM [4] have become emblematic of the AI revolution,\n",
            "driving significant advancements not only in natural language un-\n",
            "derstanding, generation, and translation but also in summarizing\n",
            "and contextualizing large volumes of textual data. Characterized by\n",
            "their extensive scale and depth, their deployment demands substan-\n",
            "tial computational resources and hence poses significant challenges\n",
            "This work is licensed under a Creative Commons Attribution International\n",
            "4.0 License.\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "© 2024 Copyright held by the owner/author(s).\n",
            "ACM ISBN 979-8-4007-0480-2/24/06\n",
            "https://doi.org/10.1145/3632775.3662830\n",
            "in terms of energy consumption and operational efficiency [38].\n",
            "The increasing application of LLMs across diverse sectors further\n",
            "compounds these challenges, because datacenters, which are re-\n",
            "sponsible for a considerable portion of global electricity consump-\n",
            "tion, must balance performance targets for LLM tasks running on\n",
            "heterogeneous hardware with the need for energy efficiency [7, 21].\n",
            "Increasing the energy efficiency of LLMs thus emerges as both a\n",
            "technical challenge and an environmental imperative [22].\n",
            "Traditional data center designs often struggle to best exploit the\n",
            "capabilities of heterogeneous hardware-based LLMs, particularly\n",
            "when trying to minimize energy consumption without sacrific-\n",
            "ing output quality and latency [6]. However, this challenge also\n",
            "presents an opportunity to innovate in datacenter architecture and\n",
            "management. We show that by rethinking how GPU resources are\n",
            "allocated and managed, there is potential to significantly reduce the\n",
            "energy footprint of LLM deployments while maintaining or even\n",
            "enhancing computational performance.\n",
            "We find that a dynamic task-scheduling model that assigns LLM\n",
            "tasks to GPUs based on the resulting energy efficiency can reduce\n",
            "overall energy. Moreover, implementing a workload-aware system\n",
            "for input and output token processing can further reduce energy\n",
            "usage. Thus, a hybrid datacenter task allocation model, which al-\n",
            "locates different tasks to different hardware accelerators based on\n",
            "their system demands, can reduce the overall energy consumption\n",
            "of LLM inference compared to a workload-unaware baseline.\n",
            "Our contributions are as follows:\n",
            "(1) We analyze the energy consumption and runtime of several\n",
            "7B-parameter LLMs’ across various hardware configurations.\n",
            "(2) We propose and evaluate a workload-aware scheduler for\n",
            "LLMs that optimizes energy efficiency based on the size of\n",
            "input and output token loads, demonstrating a 7.5% decrease\n",
            "in energy consumption over non-workload-aware baselines.\n",
            "(3) We release a comprehensive dataset and benchmark suite for\n",
            "evaluating the energy efficiency of LLM inference, enabling\n",
            "researchers and practitioners to assess the impact of their\n",
            "design choices.\n",
            "Through these contributions, we hope to support more sustain-\n",
            "able and cost-effective AI inference deployments.\n",
            "The remainder of this paper is as follows: Section 2 provides\n",
            "background information on LLM inference and energy consump-\n",
            "tion in AI systems. Section 3 formulates the problem and introduces\n",
            "our cost function. Section 4 details the methods used for bench-\n",
            "marking LLM inference on diverse systems. Section 5 presents the\n",
            "performance results of LLM inference across multiple hardware con-\n",
            "figurations. Section 6 proposes and evaluates our energy-optimal\n",
            "hybrid data center design. Finally, Section 7 discusses related works,\n",
            "and Section 8 summarizes the conclusions of the paper.\n",
            "506\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "Grant Wilkins, Srinivasan Keshav, and Richard Mortier\n",
            "2\n",
            "BACKGROUND\n",
            "2.1\n",
            "Inference Using Large Language Models\n",
            "Transformer-based neural network architectures have led to im-\n",
            "pressive gains in the performance of LLMs for language under-\n",
            "standing and generation [5]. LLMs such as OpenAI’s GPT-4 [24]\n",
            "and Google’s Gemini [32] have demonstrated human-level profi-\n",
            "ciency on many language benchmarks while requiring billions of\n",
            "parameters and massive datasets for training. The inference phase\n",
            "of LLMs involves utilizing a trained model to make predictions\n",
            "based on new, unseen data. Unlike the training phase, which is\n",
            "typically a one-time, compute-intensive process that occurs offline,\n",
            "inference is an ongoing, real-time process that directly impacts\n",
            "end-user experiences [7]. This phase is critical as it represents the\n",
            "point at which AI capabilities become accessible to users.\n",
            "Inference in LLMs can be computationally expensive due to sev-\n",
            "eral factors: (1) Model Size: The sheer size of these models, often\n",
            "billions of parameters, necessitates significant computational power\n",
            "to process each query [38]. (2) Latency Expectations: Many appli-\n",
            "cations based on LLMs, such as digital assistants, automated writing\n",
            "aids, and real-time translators, require low-latency responses [35].\n",
            "(3) Scalability: The ability to scale inference operations to accom-\n",
            "modate varying user demands without degradation in response\n",
            "times is crucial.\n",
            "2.2\n",
            "Energy Consumption in AI Systems\n",
            "Recent reports have found that the computational requirements for\n",
            "state-of-the-art AI entail massive energy consumption and carbon\n",
            "emissions [7, 21, 26, 29, 38]. The energy intensity of AI systems\n",
            "can be broadly divided into the energy required for training versus\n",
            "inference after models are deployed [13]. Training complex models\n",
            "on massive datasets is an energy-intensive process, with estimates\n",
            "finding that training GPT-3 required 1,287 megawatt-hours of en-\n",
            "ergy [26]. LLMs can also have huge emissions depending on deploy-\n",
            "ment scale and hardware efficiency [29]. For example, over a year\n",
            "of use, inference by LLMs on cloud infrastructure can consume over\n",
            "25× more energy than training a model [7]. Optimizing software\n",
            "and hardware specifically for AI workloads is thus essential [3].\n",
            "2.3\n",
            "Heterogeneous Systems for Efficient\n",
            "Computing\n",
            "Modern systems demonstrate a complex interplay between scale,\n",
            "architecture, workload behavior and efficiency objectives. The ar-\n",
            "chitecture of compute nodes can significantly impact the energy\n",
            "efficiency and processing capabilities of large-scale computing sys-\n",
            "tems [18]. Conventional server architectures based on multicore\n",
            "CPUs face energy proportionality and scalability limitations for\n",
            "modern data-intensive workloads [20]. Several researchers have\n",
            "explored heterogeneous server configurations to improve energy ef-\n",
            "ficiency [12, 15, 16, 19]. Distributed solutions can translate to lower\n",
            "energy efficiency, as communication overheads dominate [9]. Still,\n",
            "specialized clusters like NVIDIA’s DGX show 4x better performance\n",
            "per watt over conventional servers [30].\n",
            "3\n",
            "PROBLEM FORMULATION\n",
            "To model the operational demands of a hybrid, heterogeneous data-\n",
            "center hosting LLMs, we define a cost function to reflect the work-\n",
            "load distribution across different systems. We define a cost function\n",
            "𝑈(𝑚,𝑛,𝑠) that accounts for both energy consumption and runtime:\n",
            "𝑈(𝑚,𝑛,𝑠) = 𝜆𝐸(𝑚,𝑛,𝑠) + (1 −𝜆)𝑅(𝑚,𝑛,𝑠),\n",
            "where 𝑚and 𝑛denote the number of input and output tokens,\n",
            "respectively. 𝜆∈[0, 1] is a tunable parameter that balances the\n",
            "weight of energy efficiency versus speed. 𝐸(𝑚,𝑛,𝑠) is the energy\n",
            "consumed by system 𝑠to process 𝑚input tokens and generate 𝑛\n",
            "output tokens, measured in joules. 𝑅(𝑚,𝑛,𝑠) is the time required to\n",
            "process these tokens on system 𝑠, measured in seconds.\n",
            "Our objective is to minimize the total cost across all tasks and\n",
            "systems:\n",
            "min\n",
            "{𝑄𝑠}𝑠∈𝑆\n",
            "∑︁\n",
            "𝑠∈𝑆\n",
            "∑︁\n",
            "(𝑚,𝑛)∈𝑄𝑠\n",
            "𝑈(𝑚,𝑛,𝑠)\n",
            "(1)\n",
            "s.t.\n",
            "Ø\n",
            "𝑠∈𝑆\n",
            "𝑄𝑠= 𝑄\n",
            "(2)\n",
            "∀𝑠: 𝑄𝑠∩𝑄𝑠′ = ∅for 𝑠≠𝑠′\n",
            "(3)\n",
            "where 𝑆is the set of all systems, 𝑄is the total set of queries, 𝑄𝑠is\n",
            "the subset of queries assigned to system 𝑠.\n",
            "This model ensures that each query is processed exactly once,\n",
            "optimizing for energy efficiency or quick response times, depending\n",
            "on the operational needs, as parameterized by 𝜆. We note, however,\n",
            "that certain systems may be better suited to specific tasks, based on\n",
            "the workload characteristics, such as the need for rapid response\n",
            "times. Adjustments in 𝜆allow the datacenter to shift its focus be-\n",
            "tween minimizing energy consumption and reducing runtime as\n",
            "operational priorities change.\n",
            "4\n",
            "METHODS\n",
            "Here, we describe the methods and tools we use to benchmark LLM\n",
            "inference. In all cases, we use Huggingface’s Accelerate [11] to stan-\n",
            "dardize hardware optimization for inference across all platforms. T\n",
            "his library takes advantage of the available accelerator resources\n",
            "and shards models accordingly to minimize intermediate commu-\n",
            "nication and maximize the distributed capabilities for computation\n",
            "across the devices.\n",
            "4.1\n",
            "Model Selection\n",
            "Our study employs three 7B-parameter, open-source LLMs for their\n",
            "capabilities and ability to run on diverse hardware efficiently: (1)\n",
            "Falcon [2], (2) Llama-2 [33], and (3) Mistral [17]. These models\n",
            "were selected to represent a spectrum of architectures and training\n",
            "corpora. We subject each model to a series of standardized NLP\n",
            "tasks to evaluate their energy consumption during inference.\n",
            "4.1.1\n",
            "Falcon. The Falcon (7B) [2] model utilizes multi-query atten-\n",
            "tion, significantly reducing memory requirements and increasing\n",
            "processing speed. The model’s training on the bilingual RefinedWeb\n",
            "dataset enhances its applicability across diverse linguistic contexts.\n",
            "4.1.2\n",
            "Llama-2. We select Llama-2 (7B) for its optimization in di-\n",
            "alogue tasks and its improvements in safety and helpfulness. The\n",
            "507\n",
            "Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "model’s unique pretraining methodologies and advanced architec-\n",
            "tural features, such as grouped-query attention, make it an ideal\n",
            "candidate for analyzing energy efficiency in complex language\n",
            "tasks.\n",
            "4.1.3\n",
            "Mistral. We include Mistral (7B) [17] for its grouped-query\n",
            "attention and sliding window attention mechanisms, contributing\n",
            "to fast and efficient inference. Its superior performance in vari-\n",
            "ous benchmarks, especially in reasoning, mathematics, and code\n",
            "generation, makes it an essential model for our analysis.\n",
            "4.2\n",
            "Energy Profiling of Diverse Systems\n",
            "Depending on the platform, we profile each system’s energy con-\n",
            "sumption during inference using customized setups that capture\n",
            "runtime and energy or power metrics. Here, we describe how\n",
            "we monitor the energy usage of NVIDIA GPUs, Apple Silicon\n",
            "CPU/GPU, Intel CPUs, and AMD CPUs.\n",
            "4.2.1\n",
            "NVIDIA GPUs. We use PyJoules [27], a Python-based en-\n",
            "ergy measurement library, to quantify the energy consumption\n",
            "associated with inference on NVIDIA GPUs. PyJoules provides an\n",
            "interface to NVML [23], providing a software-defined energy usage\n",
            "assessment for targeted NVIDIA devices. This tool offers real-time\n",
            "energy consumption of GPUs for a given tracked process, which is\n",
            "a critical component of our analysis given the GPU-heavy compu-\n",
            "tation involved in LLM inference.\n",
            "4.2.2\n",
            "Apple Silicon CPU/GPU. No standard energy measurement\n",
            "tools are available for profiling energy and power usage for Ap-\n",
            "ple Silicon through an API like PyJoules or RAPL. Therefore, we\n",
            "employ a daemon-based approach to poll macOS’ powermetrics\n",
            "utility, providing a detailed view of the energy usage during model\n",
            "inference. To capture the energy consumption of the M1 GPU, we\n",
            "execute the powermetrics command through a Python subprocess.\n",
            "This command returns the percentage of the CPU power each CPU\n",
            "top process uses and the total CPU and GPU power consumption\n",
            "in 200ms intervals. This interval was chosen after testing to find\n",
            "the finest granularity measurement without incurring a significant\n",
            "CPU overhead for the I/O of buffering the large powermetrics\n",
            "output into memory.\n",
            "The energy monitoring is conducted concurrently with the LLM\n",
            "inference. A separate thread is dedicated to running the powermetrics\n",
            "command, ensuring real-time data collection. Post-inference, the\n",
            "collected data is processed to extract the recorded power data and\n",
            "then find the energy consumption through integration over the\n",
            "runtime. The GPU energy consumption, 𝐸𝑇𝑜𝑡𝑎𝑙,𝐺𝑃𝑈, is straightfor-\n",
            "ward to calculate for each recorded power value, 𝑃𝐺𝑃𝑈,𝑖, at each\n",
            "timestep Δ𝑡𝑖.\n",
            "𝐸𝑇𝑜𝑡𝑎𝑙,𝐺𝑃𝑈=\n",
            "∑︁\n",
            "𝑖\n",
            "𝑃𝐺𝑃𝑈,𝑖Δ𝑡𝑖.\n",
            "The CPU power draw data is less clear, as many processes run on the\n",
            "CPU. However, an \"energy impact factor\" through powermetrics\n",
            "allows us to infer how much power our Python inference process\n",
            "uses. Therefore, we calculate the CPU energy, 𝐸𝑇𝑜𝑡𝑎𝑙,𝐶𝑃𝑈, by mul-\n",
            "tiplying 𝑃𝐶𝑃𝑈,𝑖by the \"energy impact factor,\" which we denote as\n",
            "𝛼𝑖, at each timestep:\n",
            "𝐸𝑇𝑜𝑡𝑎𝑙,𝐶𝑃𝑈=\n",
            "∑︁\n",
            "𝑖\n",
            "(𝛼𝑖𝑃𝐶𝑃𝑈,𝑖)Δ𝑡𝑖.\n",
            "4.2.3\n",
            "Intel CPUs. For Intel CPUs, we leverage PyJoules, a Python-\n",
            "based energy measurement library similar to our approach for\n",
            "NVIDIA GPUs. This tool supports RAPL (Running Average Power\n",
            "Limit) interfaces, enabling us to obtain fine-grained energy con-\n",
            "sumption data [36]. We focus on two primary RAPL domains: Pack-\n",
            "age 0 and Package 1, which correspond to the entire CPU package’s\n",
            "energy consumption, including all cores in the package.\n",
            "PyJoules allows us to capture the energy usage of these domains\n",
            "in real time, enabling us to profile the energy consumption specif-\n",
            "ically during model inference tasks. To account for base energy\n",
            "consumption unrelated to our inference process, we conduct a pre-\n",
            "analysis phase to measure the CPU’s average idle power draw. This\n",
            "idle measurement is then subtracted from the total energy con-\n",
            "sumption during inference to accurately determine the net energy\n",
            "expenditure attributable to the inference process.\n",
            "We instrument our code to query the RAPL readings at the start\n",
            "and end of the inference task, calculating the energy consumption\n",
            "as follows:\n",
            "𝐸𝑇𝑜𝑡𝑎𝑙,𝐶𝑃𝑈=\n",
            "∑︁\n",
            "𝑖\n",
            "\u0012 \u0010\n",
            "𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−0,𝑖−𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−0,𝐼𝑑𝑙𝑒\n",
            "\u0011\n",
            "+\n",
            "\u0010\n",
            "𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−1,𝑖−𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−1,𝐼𝑑𝑙𝑒\n",
            "\u0011 \u0013\n",
            "Δ𝑡𝑖,\n",
            "where 𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−0,𝑖and 𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−1,𝑖, represent the power draw\n",
            "from Package 0 and Package 1, respectively, and 𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−0,𝐼𝑑𝑙𝑒\n",
            "and 𝑃𝑃𝑎𝑐𝑘𝑎𝑔𝑒−1,𝐼𝑑𝑙𝑒represent the average idle power draw of the\n",
            "CPU packages, respectively.\n",
            "4.2.4\n",
            "AMD CPUs. We adopt a different strategy for AMD CPUs\n",
            "due to the absence of a Python API. Instead, we utilize AMD𝜇Prof’s\n",
            "timechart feature, which provides detailed power draw metrics\n",
            "for every core on the chip at fine-grained intervals. By polling\n",
            "AMD𝜇Prof at 100ms intervals, we can capture the power draw of\n",
            "each physical core throughout the model inference process.\n",
            "To ensure we accurately attribute the energy consumption to our\n",
            "inference task, we monitor the CPU core residency through psutil.\n",
            "This information allows us to identify and record the specific cores\n",
            "actively engaged in the inference process at each time step. The total\n",
            "energy consumption for the inference task is then calculated by\n",
            "summing the power usage across all active cores and summing over\n",
            "the product of the power usage and time of inference, as follows:\n",
            "𝐸𝑇𝑜𝑡𝑎𝑙,𝐶𝑃𝑈=\n",
            "∑︁\n",
            "𝑐𝑜𝑟𝑒\n",
            " ∑︁\n",
            "𝑖\n",
            "𝑃𝑐𝑜𝑟𝑒,𝑖Δ𝑡𝑖\n",
            "!\n",
            "where 𝑃𝑐𝑜𝑟𝑒,𝑖represents the power draw of an individual core at\n",
            "each time step, 𝑖.\n",
            "5\n",
            "LLM INFERENCE PERFORMANCE ON\n",
            "DIVERSE CLUSTERS\n",
            "5.1\n",
            "Hardware and Software Versions\n",
            "The systems we profile are shown in Table 1. We consider these sys-\n",
            "tems as they demonstrate three prominent CPU manufactures and\n",
            "different generations of GPUs. We utilize PyTorch v2.0.1, Torchvi-\n",
            "sion v0.15.2, Numpy v1.26.0, Huggingface v0.20.2, and Accelerate\n",
            "v0.26.1.\n",
            "508\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "Grant Wilkins, Srinivasan Keshav, and Richard Mortier\n",
            "System Name\n",
            "CPU\n",
            "GPU(s) per Node\n",
            "DRAM per Node\n",
            "VRAM per GPU\n",
            "Macbook Pro\n",
            "10-core M1 Pro\n",
            "14-core M1 Pro\n",
            "32GB\n",
            "-\n",
            "Swing AMD+A100\n",
            "2×64-core AMD EPYC 7742\n",
            "8×NVIDIA A100\n",
            "1TB\n",
            "40GB\n",
            "Palmetto Intel+V100\n",
            "40-Core Intel Xeon 6148G\n",
            "2×NVIDIA V100\n",
            "376GB\n",
            "16GB\n",
            "Table 1: Our System Configurations\n",
            "We note that the M1-Pro results only include the Llama-2 (7B)\n",
            "and Mistral (7B) results, as Falcon (7B) generally did not complete\n",
            "tasks in less than two orders of magnitude greater runtime.\n",
            "5.2\n",
            "Experimental Strategy\n",
            "To comprehensively evaluate the performance of different system\n",
            "configurations across various models, we conducted a series of\n",
            "controlled experiments. We systematically varied the number of\n",
            "input and output tokens to measure their effects on runtime and\n",
            "energy consumption under two main experimental conditions. In\n",
            "each experiment we do not allow for key-value caches to be re-used\n",
            "to ensure our testing environment is standardized.\n",
            "5.2.1\n",
            "Vary Input Tokens. For the first experimental condition, we\n",
            "executed inference requests with increasing input token sizes, rang-\n",
            "ing from 8 to 2048 tokens, while maintaining a fixed output token\n",
            "size of 32. This setup allowed us to isolate the impact of input size\n",
            "on the system’s performance and energy efficiency.\n",
            "5.2.2\n",
            "Vary Output Tokens. In the second set of experiments, we\n",
            "varied the output token limit from 8 to 4096 tokens, keeping the\n",
            "input token size constant at 32. This approach helped us understand\n",
            "how increasing output demands affect the runtime and energy\n",
            "consumption of the systems tested.\n",
            "5.2.3\n",
            "Randomization and Stopping Criteria. Each experiment was\n",
            "conducted in a randomized order to mitigate any potential bias\n",
            "introduced by the sequence of tests. To ensure the reliability of our\n",
            "results, we adhered to strict criteria for statistical confidence. Each\n",
            "configuration was tested repeatedly until either of two conditions\n",
            "was met: (1) The measured runtime had to be within 0.5 seconds of\n",
            "the actual mean runtime with 95% confidence. (2) A maximum of\n",
            "25 trials were conducted for each setting if the first condition could\n",
            "not be met.\n",
            "5.3\n",
            "Input Token Analysis\n",
            "Here, we present the impacts on runtime, energy consumption per\n",
            "token, and throughput for LLMs across different hardware config-\n",
            "urations while varying the number of input tokens. We perform\n",
            "these experiments using the suite of systems outlined in Table 1\n",
            "with the models outlined in Section 4.1. In our experiments on the\n",
            "Palmetto Intel+V100 system, the V100 GPU had an out-of-memory\n",
            "error beyond 1024 output tokens for Falcon (7B).\n",
            "Our runtime measurements show a significant increase as in-\n",
            "put tokens grow. As depicted in Figure 1(a), all systems exhibit a\n",
            "nonlinear escalation in runtime with increasing token counts, with\n",
            "the M1-Pro system showing the most significant magnitude. This\n",
            "trend highlights the computational burden imposed by larger input\n",
            "sizes, particularly on smaller systems that are not as well designed\n",
            "to handle extensive workloads.\n",
            "For all systems, we notice that throughput follows a “roofline\n",
            "model\" with increasing input tokens [37]. Figure 1(b) illustrates\n",
            "these dynamics, indicating an increase in throughput for all systems\n",
            "until a certain point where inference becomes bound by compute\n",
            "and not by the overhead of the software, as described by roofline\n",
            "performance models [37].\n",
            "Energy efficiency varies markedly across different systems. The\n",
            "M1-Pro demonstrates consistently low energy consumption per to-\n",
            "ken, particularly for smaller input sizes, as shown in Figure 1(c). This\n",
            "efficiency reflects the M1-Pro’s design optimization for low-power\n",
            "operations. In contrast, the Swing AMD+A100, while capable of\n",
            "handling more significant token inputs more efficiently, consumed\n",
            "more energy per token for small workloads yet became more en-\n",
            "ergy efficient at larger input token sizes, underscoring a trade-off\n",
            "between workload size and energy efficiency.\n",
            "5.4\n",
            "Output Token Analysis\n",
            "Here we examine the performance trends associated with increasing\n",
            "the number of output tokens for our LLMs and systems of interest,\n",
            "specifically focusing on runtime, energy consumption per token,\n",
            "and throughput. In our experiments, the M1-Pro also could not\n",
            "generate more than 512 output tokens without significant runtime\n",
            "penalties. For the Palmetto Intel+V100 system, the V100 GPU had\n",
            "an OOM error beyond 1024 output tokens for Falcon (7B) and for\n",
            "all models beyond 2048 tokens.\n",
            "Runtime significantly increases with the number of output to-\n",
            "kens across all systems. As illustrated in Figure 2(a), the escala-\n",
            "tion in runtime is pronounced, particularly as the output token\n",
            "count reaches higher magnitudes. This increase is indicative of\n",
            "the substantial computational effort required by LLMs to generate\n",
            "successive tokens.\n",
            "In Figure 2(b), we observe a decrease in throughput across all\n",
            "systems as the number of output tokens increases. This trend high-\n",
            "lights the inherent computational complexity involved in generat-\n",
            "ing larger sequences of tokens in LLM tasks. As the output token\n",
            "count grows, the system must process each additional token, re-\n",
            "calculating the context and updating internal model states [34].\n",
            "This not only increases the total computation per query but also\n",
            "leads to a greater accumulation of processing time per token, which\n",
            "consequently lowers the overall throughput.\n",
            "Energy consumption per token also shows an increasing trend\n",
            "as the number of output tokens grows. Displayed in Figure 2(c),\n",
            "this trend underscores the energy-intensive nature of producing\n",
            "larger outputs. Systems such as the M1-Pro, while generally more\n",
            "energy-efficient, begin to consume more energy per token as output\n",
            "demands increase, reflecting the intensive processing involved in\n",
            "output generation.\n",
            "509\n",
            "Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "2\n",
            "5\n",
            "2\n",
            "6\n",
            "2\n",
            "7\n",
            "2\n",
            "8\n",
            "2\n",
            "9 2\n",
            "10 2\n",
            "11\n",
            "Number of Input Tokens\n",
            "10\n",
            "−1\n",
            "10\n",
            "0\n",
            "10\n",
            "1\n",
            "10\n",
            "2\n",
            "10\n",
            "3\n",
            "Runtime (s)\n",
            "(a) Runtime\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "2\n",
            "5\n",
            "2\n",
            "6\n",
            "2\n",
            "7\n",
            "2\n",
            "8\n",
            "2\n",
            "9 2\n",
            "10 2\n",
            "11\n",
            "Number of Input Tokens\n",
            "10\n",
            "0\n",
            "10\n",
            "1\n",
            "10\n",
            "2\n",
            "10\n",
            "3\n",
            "Throughput (tokens/s)\n",
            "(b) Throughput\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "2\n",
            "5\n",
            "2\n",
            "6\n",
            "2\n",
            "7\n",
            "2\n",
            "8\n",
            "2\n",
            "9 2\n",
            "10 2\n",
            "11\n",
            "Number of Input Tokens\n",
            "10\n",
            "0\n",
            "10\n",
            "1\n",
            "10\n",
            "2\n",
            "Energy per Token (J/tokens)\n",
            "System\n",
            "Swing AMD+A100\n",
            "Palmetto Intel+V100\n",
            "M1-Pro\n",
            "Model\n",
            "Falcon (7B)\n",
            "Llama-2 (7B)\n",
            "Mistral (7B)\n",
            "(c) Energy per Token\n",
            "Figure 1: Performance of Various Systems and Models for Processing Variable Input Tokens–Due to the low variance in the\n",
            "data, error bars are too small to be visible.\n",
            "2\n",
            "3 2\n",
            "4 2\n",
            "5 2\n",
            "6 2\n",
            "7 2\n",
            "8 2\n",
            "9 2\n",
            "10 2\n",
            "11 2\n",
            "12\n",
            "Number of Output Tokens\n",
            "10\n",
            "−1\n",
            "10\n",
            "0\n",
            "10\n",
            "1\n",
            "10\n",
            "2\n",
            "10\n",
            "3\n",
            "10\n",
            "4\n",
            "Runtime (s)\n",
            "(a) Runtime\n",
            "2\n",
            "3 2\n",
            "4 2\n",
            "5 2\n",
            "6 2\n",
            "7 2\n",
            "8 2\n",
            "9 2\n",
            "10 2\n",
            "11 2\n",
            "12\n",
            "Number of Output Tokens\n",
            "10\n",
            "−1\n",
            "10\n",
            "0\n",
            "10\n",
            "1\n",
            "10\n",
            "2\n",
            "10\n",
            "3\n",
            "Throughput (tokens/s)\n",
            "(b) Throughput\n",
            "2\n",
            "3 2\n",
            "4 2\n",
            "5 2\n",
            "6 2\n",
            "7 2\n",
            "8 2\n",
            "9 2\n",
            "10 2\n",
            "11 2\n",
            "12\n",
            "Number of Output Tokens\n",
            "10\n",
            "−1\n",
            "10\n",
            "0\n",
            "10\n",
            "1\n",
            "10\n",
            "2\n",
            "10\n",
            "3\n",
            "Energy per Token (J/tokens)\n",
            "System\n",
            "Swing AMD+A100\n",
            "Palmetto Intel+V100\n",
            "M1-Pro\n",
            "Model\n",
            "Falcon (7B)\n",
            "Llama-2 (7B)\n",
            "Mistral (7B)\n",
            "(c) Energy per Token\n",
            "Figure 2: Performance of Various Systems and Models for Processing Variable Output Tokens–Missing data points in M1-Pro\n",
            "and Palmetto Intel+V100 are due to CUDA out of memory errors. Due to the low variance in the data, error bars are too small\n",
            "to be visible.\n",
            "5.5\n",
            "Comparing the Input and Output Analyses\n",
            "When comparing Figure 1(a) and Figure 2(a), we observe that in-\n",
            "creases in the number of output tokens result in a more considerable\n",
            "increase in runtime than increases in input tokens. The computa-\n",
            "tional complexity of processing input tokens primarily involves\n",
            "encoding the input context, which occurs once per input sequence\n",
            "and follows a more linear computational trajectory. In contrast,\n",
            "generating output tokens is inherently more complex and iterative.\n",
            "Each new output token requires the model to run through all its\n",
            "layers to predict the next token based on an ever-expanding context,\n",
            "which includes both the initial input and all previously generated\n",
            "tokens [34]. This ongoing computation involves recalculating atten-\n",
            "tion across an increasing number of tokens, updating hidden states,\n",
            "and generating a probability distribution over the vocabulary for\n",
            "each new token. Consequently, as the number of output tokens\n",
            "grows, the computational load increases significantly, leading to\n",
            "more significant runtime increases than processing input tokens.\n",
            "The impacts on runtime also translate to the throughput, de-\n",
            "picted in Figure 1(b) and Figure 2(b). There is a noticeable decline\n",
            "in throughput as output tokens increase, more so than input to-\n",
            "kens. The decrease in throughput for output tokens is primarily\n",
            "due to the heightened computational requirements for generating\n",
            "subsequent tokens, where each token’s generation slows down as\n",
            "the sequence lengthens. Furthermore, the energy per token also\n",
            "increases as output tokens grow, as shown in our analysis. The\n",
            "energy required to generate each output token becomes significant\n",
            "due to longer passes through the transformer network. We contrast\n",
            "this with the energy consumption when processing input tokens,\n",
            "which, despite increasing, does so at a less steep rate.\n",
            "6\n",
            "ENERGY-OPTIMAL HYBRID DATACENTER\n",
            "FOR LLM INFERENCE\n",
            "Considering the performance results we collect from LLM inference\n",
            "across multiple systems, we notice that there is an energy-optimal\n",
            "way to construct a hybrid datacenter with a combination of M1 Pro’s\n",
            "and A100s. The intuition behind this is that the energy expended\n",
            "per token for the M1 Pro is lower than that of the A100 up to a\n",
            "certain point in the number of input and output tokens as seen in\n",
            "Figures 1(c) and 2(c). However, the energy efficiency characteristics\n",
            "are different when varying the number of input and output tokens,\n",
            "and therefore, we will proceed with separate analyses.\n",
            "6.1\n",
            "Number of Input Tokens Analysis\n",
            "Suppose we have a hybrid data center with M1-Pros and A100s.\n",
            "Then, we have some workload for an LLM, a set of queries with\n",
            "some outputs. In such a configuration, we implement a scheduling\n",
            "heuristic based on a cutoff threshold, 𝑇𝑖𝑛, for input token length.\n",
            "510\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "Grant Wilkins, Srinivasan Keshav, and Richard Mortier\n",
            "This heuristic dictates that queries with 𝑛≤𝑇𝑖𝑛tokens are pro-\n",
            "cessed on M1 Pro systems, which we have shown have good energy\n",
            "efficiency with handling smaller computational loads. Conversely,\n",
            "queries with 𝑛> 𝑇𝑖𝑛tokens leverage the greater computational abil-\n",
            "ity of A100 GPUs, which offer greater energy-per-token advantages\n",
            "for larger tasks despite their higher power usage. We point out that\n",
            "this is the same method mentioned in the problem formulation in\n",
            "Eqn. 1, where our queries 𝑄are partitioned into 𝑄𝑀1 and 𝑄𝐴100\n",
            "strictly on input and output size.\n",
            "To find an optimal threshold 𝑇𝑖𝑛empirically, we analyze the to-\n",
            "ken distribution in prompts from the Alpaca [31] dataset, a bench-\n",
            "mark dataset frequently used in model fine-tuning. This dataset\n",
            "comprises 52K prompts, offering a diverse range of lengths akin to\n",
            "a typical workload in systems like GPT-4 [24]. The distribution of\n",
            "input tokens, visualized in our analysis (see Fig. 3(a)), serves as a\n",
            "proxy for understanding the variegated nature of LLM workloads.\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "Number of Input Tokens\n",
            "0\n",
            "2000\n",
            "4000\n",
            "6000\n",
            "8000\n",
            "Frequency\n",
            "(a) Input Tokens\n",
            "0\n",
            "200\n",
            "400\n",
            "600\n",
            "Number of Output Tokens\n",
            "0\n",
            "2000\n",
            "4000\n",
            "6000\n",
            "8000\n",
            "Frequency\n",
            "(b) Output Tokens\n",
            "Figure 3: Distribution of Token Counts for Alpaca [31]\n",
            "The energy component of our cost function, split over the token\n",
            "threshold, is as follows:\n",
            "𝐸𝑇𝑜𝑡𝑎𝑙,𝑖𝑛=\n",
            "𝑇𝑖𝑛\n",
            "∑︁\n",
            "𝑚=1\n",
            "𝑚𝑓𝑖𝑛(𝑚)𝐸𝑀1,𝑖𝑛(𝑚) +\n",
            "𝑀\n",
            "∑︁\n",
            "𝑚=𝑇𝑖𝑛+1\n",
            "𝑚𝑓𝑖𝑛(𝑚)𝐸𝐴100,𝑖𝑛(𝑚),\n",
            "where 𝐸𝑇𝑜𝑡𝑎𝑙,𝑖𝑛represents the total energy consumption for a given\n",
            "dataset of input lengths 𝑚with corresponding frequencies 𝑓𝑖𝑛(𝑚),\n",
            "and 𝐸𝑀1,𝑖𝑛(𝑚) and 𝐸𝐴100,𝑖𝑛(𝑚) denote the mean energy per token\n",
            "for varying the input token size for the M1-Pro and A100 systems,\n",
            "respectively. Utilizing this model with our dataset enables the ap-\n",
            "proximation of total energy consumption for various threshold\n",
            "settings, offering insights into the energy dynamics of hybrid dat-\n",
            "acenter operation. In Figure 4, we show the energy and runtime\n",
            "simulation results of performing inference for the input token sizes\n",
            "from the Alpaca dataset.\n",
            "Our findings indicate that a threshold of 32 tokens strikes an\n",
            "optimal balance, significantly reducing energy consumption by\n",
            "relegating the inference of shorter queries to the more energy-\n",
            "efficient M1 Pro systems. This policy not only capitalizes on the\n",
            "inherent energy efficiency of the M1 Pro for smaller tasks but\n",
            "also reserves the computational might of the A100 for queries that\n",
            "necessitate its robust capabilities. However, it’s important to note\n",
            "that this energy optimization comes at the cost of increased runtime.\n",
            "6.2\n",
            "Number of Output Tokens Analysis\n",
            "We want to use the same scheduling heuristic and performance\n",
            "model to determine a threshold 𝑇𝑜𝑢𝑡for the number of output\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "2\n",
            "5\n",
            "2\n",
            "6\n",
            "2\n",
            "7\n",
            "2\n",
            "8\n",
            "2\n",
            "9\n",
            "2\n",
            "10\n",
            "2\n",
            "11\n",
            "Threshold\n",
            "0.76\n",
            "0.78\n",
            "0.80\n",
            "Total Energy (kWh)\n",
            "M1-Pro Only\n",
            "Swing AMD+A100 Only\n",
            "Hybrid System\n",
            "(a) Energy Consumption for Changing 𝑇𝑖𝑛\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "2\n",
            "5\n",
            "2\n",
            "6\n",
            "2\n",
            "7\n",
            "2\n",
            "8\n",
            "2\n",
            "9\n",
            "2\n",
            "10\n",
            "2\n",
            "11\n",
            "Threshold\n",
            "0.5\n",
            "1.0\n",
            "Runtime (s)\n",
            "1e7\n",
            "M1-Pro Only\n",
            "Swing AMD+A100 Only\n",
            "Hybrid System\n",
            "(b) Runtime for Changing 𝑇𝑖𝑛\n",
            "Figure 4: Performance of Hybrid Datacenter for Input\n",
            "Tokens Processing Alpaca–Dashed line shows the value for\n",
            "using only one kind of hardware for inference\n",
            "tokens. Except this time, we have different frequencies 𝑓𝑜𝑢𝑡(𝑛)\n",
            "for the 𝑛output tokens and different mean energy per token for\n",
            "varying the output token size, 𝐸𝑀1,𝑜𝑢𝑡(𝑛) and 𝐸𝐴100,𝑜𝑢𝑡(𝑛). We\n",
            "also utilize the distribution of the number of output tokens in the\n",
            "Alpaca dataset (see Fig. 3(b)). We revise our performance model as\n",
            "follows:\n",
            "𝐸𝑇𝑜𝑡𝑎𝑙,𝑜𝑢𝑡=\n",
            "𝑇𝑜𝑢𝑡\n",
            "∑︁\n",
            "𝑛=1\n",
            "𝑛𝑓𝑜𝑢𝑡(𝑛)𝐸𝑀1,𝑜𝑢𝑡(𝑛)\n",
            "+\n",
            "𝑁\n",
            "∑︁\n",
            "𝑛=𝑇𝑜𝑢𝑡+1\n",
            "𝑛𝑓𝑜𝑢𝑡(𝑛)𝐸𝐴100,𝑜𝑢𝑡(𝑛).\n",
            "As the M1 Pro could only generate up to 512 tokens of a response,\n",
            "we only test𝑇𝑜𝑢𝑡up until this point. In Figure 5, we show the energy\n",
            "and runtime simulation results of performing inference for the input\n",
            "token sizes from the Alpaca dataset.\n",
            "Fig. 5(b) and Fig. 2(c) assess the energy consumption and runtime\n",
            "implications of various threshold settings for output generation.\n",
            "Our findings suggest that although higher thresholds may leverage\n",
            "the M1 Pro’s energy efficiency for smaller outputs, there is an opti-\n",
            "mal point at 32 output tokens that minimizes energy consumption.\n",
            "6.3\n",
            "Balancing Energy Efficiency and Runtime\n",
            "Performance\n",
            "Our analysis of both input and output token processing within a\n",
            "hybrid, heterogeneous datacenter framework has led to the identifi-\n",
            "cation that with certain thresholds at 𝑇𝑖𝑛𝑝𝑢𝑡= 32 and 𝑇𝑜𝑢𝑡𝑝𝑢𝑡= 32,\n",
            "511\n",
            "Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "2\n",
            "5\n",
            "2\n",
            "6\n",
            "2\n",
            "7\n",
            "2\n",
            "8\n",
            "2\n",
            "9\n",
            "Threshold\n",
            "0.66\n",
            "0.68\n",
            "0.70\n",
            "Total Energy (kWh)\n",
            "M1-Pro Only\n",
            "Swing AMD+A100 Only\n",
            "Hybrid System\n",
            "(a) Energy Consumption for Changing 𝑇𝑜𝑢𝑡\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "2\n",
            "5\n",
            "2\n",
            "6\n",
            "2\n",
            "7\n",
            "2\n",
            "8\n",
            "2\n",
            "9\n",
            "Threshold\n",
            "0.5\n",
            "1.0\n",
            "1.5\n",
            "Runtime (s)\n",
            "1e7\n",
            "M1-Pro Only\n",
            "Swing AMD+A100 Only\n",
            "Hybrid System\n",
            "(b) Runtime for Changing 𝑇𝑜𝑢𝑡\n",
            "Figure 5: Performance of Hybrid Datacenter for Output\n",
            "Tokens Processing Alpaca – Dashed line shows the value for\n",
            "using only one kind of hardware for inference\n",
            "we can strategically allocate tasks to M1 Pro systems or A100 GPUs\n",
            "based on token count, optimizing for energy efficiency.\n",
            "Shifting the token distribution leverages the M1 Pro’s superior\n",
            "energy efficiency for input and output tasks up to the threshold,\n",
            "beyond which we utilize the A100’s computational power. This\n",
            "policy saves energy as smaller-token tasks are handled by the more\n",
            "efficient M1 Pro for outputs up to the threshold. However, this\n",
            "energy optimization comes at the expense of increased runtime,\n",
            "which is particularly noticeable in output token generation where\n",
            "the M1 Pro, despite its efficiency, does not match the A100’s speed.\n",
            "The energy-runtime trade-off presents a favorable scenario for\n",
            "applications that have low runtime sensitivity. For instance, batch\n",
            "processing of LLM tasks, such as overnight data analyses or non-\n",
            "time-critical computations, can benefit significantly from this energy-\n",
            "efficient configuration. Similarly, free or not directly monetized\n",
            "services, where the cost of computation impacts operational sus-\n",
            "tainability, stand to gain from minimizing energy expenditures even\n",
            "at the cost of longer processing times.\n",
            "This approach also opens discussions on Quality of Service (QoS)\n",
            "for LLMs, an area that still needs to be explored [1, 35]. Traditional\n",
            "QoS metrics often prioritize speed and reliability, but energy effi-\n",
            "ciency may also become a critical QoS dimension for LLM applica-\n",
            "tions, particularly in energy-constrained or cost-sensitive scenarios.\n",
            "7\n",
            "RELATED WORK\n",
            "7.1\n",
            "Hybrid and Energy Efficient Heterogeneous\n",
            "Data Centers\n",
            "Recent studies in optimizing data center architectures for deep learn-\n",
            "ing have highlighted the necessity of energy-efficient scheduling\n",
            "and task allocation across diverse hardware. Gu et al. [10] explore\n",
            "GPU clusters’ energy-efficient scheduling, revealing substantial im-\n",
            "provements in power utilization without considering diverse GPU\n",
            "types for different task requirements. This work highlights a gap\n",
            "in understanding how various GPU configurations could enhance\n",
            "energy efficiency further. Similarly, Patel et al. [25] demonstrate\n",
            "the benefits of hybrid computing environments, emphasizing FPGA\n",
            "over GPU diversity. This focus leaves room to explore the specific\n",
            "impacts of different GPU classes in such settings.\n",
            "In the realm of LLMs, Zhao et al. [39] introduce strategies like\n",
            "phase-aware partitioning and adaptive quantization in heteroge-\n",
            "neous clusters but do not integrate energy considerations into their\n",
            "analysis, which is crucial for understanding the real-world appli-\n",
            "cability of these models in power-sensitive environments. On the\n",
            "other hand, Radovanović et al. [28] and Chien et al. [7] discuss\n",
            "broader aspects of carbon-aware computing and reducing the car-\n",
            "bon impact of AI inference, respectively. These works emphasize\n",
            "the importance of node/device-level energy metrics, often over-\n",
            "looked in typical LLM deployment strategies, thus underscoring\n",
            "the need for detailed energy consumption profiling across different\n",
            "models and hardware types.\n",
            "7.2\n",
            "LLM Inference as a Service\n",
            "Further focusing on energy consumption, Hu et al. [14] analyze\n",
            "deep learning workloads in GPU datacenters, offering insights into\n",
            "energy conservation strategies through workload scheduling. This\n",
            "research aligns with our objectives by confirming the critical role\n",
            "of scheduling in reducing energy footprints. Anderson et al. [3]\n",
            "propose carbon-aware datacenter software that could complement\n",
            "physical hardware adjustments by making energy and carbon met-\n",
            "rics visible to application developers, encouraging more energy-\n",
            "efficient coding practices.\n",
            "Addressing service quality, Wang et al. [35] study the efficiency\n",
            "and reliability of LLM serving, highlighting the challenges of main-\n",
            "taining high-quality service while managing computational loads\n",
            "effectively. This perspective is pertinent as it underscores the trade-\n",
            "off between performance and energy efficiency, which is central to\n",
            "our study. Lastly, Desislavov et al. [8] provide a timely examination\n",
            "of trends in AI inference energy consumption, arguing that while\n",
            "performance has increased dramatically, energy consumption has\n",
            "not escalated at the same pace, thanks to hardware optimizations\n",
            "and algorithmic innovations. This outlook is necessary as it sug-\n",
            "gests the potential for further optimizations in LLM inference tasks,\n",
            "which are typically energy-intensive.\n",
            "8\n",
            "CONCLUSIONS AND FUTURE WORK\n",
            "Future work will explore minimizing the energy and runtime and\n",
            "maximizing the accuracy of serving differently-sized LLMs. Larger\n",
            "models are generally more accurate but come at the expense of\n",
            "requiring more hardware accelerators and often greater runtime;\n",
            "therefore, exploring this trade-off is highly relevant. Also, we plan to\n",
            "make our solution for energy-optimal routing of incoming queries\n",
            "an online decision-making heuristic to increase its efficacy. Simi-\n",
            "larly, we aim to extend our energy model to reflect carbon awareness\n",
            "and water consumption to decrease the environmental impact of\n",
            "LLM inference further.\n",
            "512\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "Grant Wilkins, Srinivasan Keshav, and Richard Mortier\n",
            "By carefully analyzing the energy and runtime of heterogeneous\n",
            "compute hardware to host LLMs, we show that a hybrid, hetero-\n",
            "geneous datacenter and a cost-based scheduling framework can\n",
            "allocate LLM tasks to accelerators that are best suited to run them\n",
            "in terms of energy efficiency and computational performance. This\n",
            "decision is based simply on the size of input and output tokens,\n",
            "making the decision process easy to integrate into existing work-\n",
            "loads.\n",
            "ACKNOWLEDGMENTS\n",
            "We gratefully acknowledge the computing resources provided on\n",
            "Swing and Palmetto, both high-performance computing clusters\n",
            "operated by the Laboratory Computing Resource Center at Argonne\n",
            "National Laboratory and Clemson University, respectively. During\n",
            "this work GW was supported by a Churchill Scholarship.\n",
            "REFERENCES\n",
            "[1] Megha Agarwal, Asfandyar Qureshi, Linden Li Nikhil Sardana, Julian Quevedo,\n",
            "and Daya Khudia. 2023.\n",
            "LLM Inference Performance Engineering: Best\n",
            "Practices.\n",
            "https://www.databricks.com/blog/llm-inference-performance-\n",
            "engineering-best-practices\n",
            "[2] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, et al. 2023. The\n",
            "Falcon Series of Language Models: Towards Open Frontier Models. (2023).\n",
            "[3] Thomas Anderson, Adam Belay, Mosharaf Chowdhury, Asaf Cidon, and Irene\n",
            "Zhang. 2023. Treehouse: A Case For Carbon-Aware Datacenter Software. SIGEN-\n",
            "ERGY Energy Inform. Rev. 3, 3 (oct 2023), 64–70. https://doi.org/10.1145/3630614.\n",
            "3630626\n",
            "[4] Rohan Anil, Andrew M. Dai, Orhan Firat, et al. 2023. PaLM 2 Technical Report.\n",
            "arXiv:2305.10403 [cs.CL]\n",
            "[5] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. 2022. On the Oppor-\n",
            "tunities and Risks of Foundation Models. arXiv:2108.07258 [cs.LG]\n",
            "[6] Le Chen, Nesreen K. Ahmed, Akash Dutta, et al. 2024. The Landscape and\n",
            "Challenges of HPC Research and LLMs. arXiv:2402.02018 [cs.LG]\n",
            "[7] Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and\n",
            "Rajini Wijayawardana. 2023. Reducing the Carbon Impact of Generative AI\n",
            "Inference (Today and in 2035). In Proceedings of the 2nd Workshop on Sustainable\n",
            "Computer Systems (Boston, MA, USA) (HotCarbon ’23). Association for Computing\n",
            "Machinery, New York, NY, USA, Article 11, 7 pages.\n",
            "https://doi.org/10.1145/\n",
            "3604930.3605705\n",
            "[8] Radosvet Desislavov, Fernando Martínez-Plumed, and José Hernández-Orallo.\n",
            "2023. Trends in AI inference energy consumption: Beyond the performance-vs-\n",
            "parameter laws of deep learning. Sustainable Computing: Informatics and Systems\n",
            "38 (2023), 100857. https://doi.org/10.1016/j.suscom.2023.100857\n",
            "[9] Yiannis Georgiou, David Glesser, and Denis Trystram. 2015. Adaptive Resource\n",
            "and Job Management for Limited Power Consumption. In Proceedings of the\n",
            "2015 IEEE International Parallel and Distributed Processing Symposium Workshop\n",
            "(IPDPSW ’15). IEEE Computer Society, USA, 863–870. https://doi.org/10.1109/\n",
            "IPDPSW.2015.118\n",
            "[10] Diandian Gu, Xintong Xie, Gang Huang, Xin Jin, and Xuanzhe Liu. 2023. Energy-\n",
            "Efficient GPU Clusters Scheduling for Deep Learning. arXiv:2304.06381 [cs.DC]\n",
            "[11] Sylvain Gugger, Lysandre Debut, Thomas Wolf, et al. 2022. Accelerate: Training\n",
            "and inference at scale made simple, efficient and adaptable. https://github.com/\n",
            "huggingface/accelerate.\n",
            "[12] Yuxiong He and Sameh Elnikety. 2011. Position paper: embracing heterogeneity-\n",
            "improving energy efficiency for interactive services. In Proceedings of the 8th AAAI\n",
            "Conference on AI for Data Center Management and Cloud Computing (AAAIWS’11-\n",
            "08). AAAI Press, New York, NY, 11–14.\n",
            "[13] Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and\n",
            "Joelle Pineau. 2020. Towards the Systematic Reporting of the Energy and Carbon\n",
            "Footprints of Machine Learning. J. Mach. Learn. Res. 21, 1, Article 248 (jan 2020),\n",
            "43 pages.\n",
            "[14] Qinghao Hu, Peng Sun, Shengen Yan, Yonggang Wen, and Tianwei Zhang.\n",
            "2021. Characterization and prediction of deep learning workloads in large-\n",
            "scale GPU datacenters. In Proceedings of the International Conference for High\n",
            "Performance Computing, Networking, Storage and Analysis (<conf-loc>, <city>St.\n",
            "Louis</city>, <state>Missouri</state>, </conf-loc>) (SC ’21). Association for\n",
            "Computing Machinery, New York, NY, USA, Article 104, 15 pages.\n",
            "https:\n",
            "//doi.org/10.1145/3458817.3476223\n",
            "[15] Xiaoxuan Hu, Peng Li, and Yanfei Sun. 2021. Minimizing energy cost for green\n",
            "data center by exploring heterogeneous energy resource. Journal of Modern\n",
            "Power Systems and Clean Energy 9, 1 (2021), 148–159.\n",
            "[16] Mehboob Hussain, Lian-Fu Wei, Abdullah Lakhan, Samad Wali, Soragga Ali,\n",
            "and Abid Hussain. 2021. Energy and performance-efficient task scheduling in\n",
            "heterogeneous virtualized cloud computing. Sustainable Computing: Informatics\n",
            "and Systems 30 (2021), 100517.\n",
            "[17] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, and et al. 2023. Mistral\n",
            "7B. arXiv:2310.06825 [cs.CL]\n",
            "[18] Willis Lang, Jignesh M. Patel, and Srinath Shankar. 2010. Wimpy node clusters:\n",
            "what about non-wimpy workloads?. In Proceedings of the Sixth International\n",
            "Workshop on Data Management on New Hardware (Indianapolis, Indiana) (DaMoN\n",
            "’10). Association for Computing Machinery, New York, NY, USA, 47–55. https:\n",
            "//doi.org/10.1145/1869389.1869396\n",
            "[19] Wenyu Liu, Yuejun Yan, Yimeng Sun, Hongju Mao, Ming Cheng, Peng Wang,\n",
            "and Zhaohao Ding. 2023. Online job scheduling scheme for low-carbon data\n",
            "center operation: An information and energy nexus perspective. Applied Energy\n",
            "338 (2023), 120918.\n",
            "[20] David Lo, Liqun Cheng, Rama Govindaraju, et al. 2015.\n",
            "Heracles: Improv-\n",
            "ing resource efficiency at scale. In 2015 ACM/IEEE 42nd Annual International\n",
            "Symposium on Computer Architecture (ISCA). ACM, New York, NY, 450–462.\n",
            "https://doi.org/10.1145/2749469.2749475\n",
            "[21] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Es-\n",
            "timating the Carbon Footprint of BLOOM, a 176B Parameter Language Model.\n",
            "arXiv:2211.02001 [cs.LG]\n",
            "[22] David Mytton and Masa¯o Ashtine. 2022. Sources of data center energy estimates:\n",
            "A comprehensive review. Joule 6, 9 (2022), 2032–2056. https://doi.org/10.1016/j.\n",
            "joule.2022.07.011\n",
            "[23] NVIDIA. Accessed 2024. NVIDIA-NVML. https://docs.nvidia.com/deploy/nvml-\n",
            "api/index.html. Available online.\n",
            "[24] OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, et al. 2023. GPT-4\n",
            "Technical Report. arXiv:2303.08774 [cs.CL]\n",
            "[25] Pratyush Patel, Katie Lim, Kushal Jhunjhunwalla, Ashlie Martinez, Max Demoulin,\n",
            "Jacob Nelson, Irene Zhang, and Thomas Anderson. 2023. Hybrid Computing for\n",
            "Interactive Datacenter Applications. arXiv:2304.04488 [cs.DC]\n",
            "[26] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,\n",
            "Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021. Carbon Emissions\n",
            "and Large Neural Network Training. arXiv:2104.10350 [cs.LG]\n",
            "[27] powerapi ng. 2024. PyJoules: Python-based energy measurement library for\n",
            "various domains including NVIDIA GPUs. https://github.com/powerapi-ng/\n",
            "pyJoules. Accessed: 2024-01-10.\n",
            "[28] Ana Radovanović, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre\n",
            "Duarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick Care, et al.\n",
            "2022. Carbon-aware computing for datacenters. IEEE Transactions on Power\n",
            "Systems 38, 2 (2022), 1270–1280.\n",
            "[29] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas,\n",
            "Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay\n",
            "Gadepally. 2023. From Words to Watts: Benchmarking the Energy Costs of Large\n",
            "Language Model Inference. arXiv:2310.03003 [cs.CL]\n",
            "[30] Matej Špeťko, Ondřej Vysocký, Branislav Jansík, and Lubomír Říha. 2021. DGX-\n",
            "A100 Face to Face DGX-2—Performance, Power and Thermal Behavior Evaluation.\n",
            "Energies 14, 2 (2021). https://doi.org/10.3390/en14020376\n",
            "[31] R. Taori, I. Gulrajani, T. Zhang, and et al. 2024. Stanford alpaca: An instruction\n",
            "following llama model. https://github.com/tatsu-lab/stanford_alpaca. Accessed:\n",
            "2024-01-15.\n",
            "[32] Google Gemini Team. 2024. Gemini: A Family of Highly Capable Multimodal\n",
            "Models. arXiv:2312.11805 [cs.CL]\n",
            "[33] Hugo Touvron, Louis Martin, Kevin Stone, and et al. 2023.\n",
            "Llama 2: Open\n",
            "Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]\n",
            "[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. 2017. Attention is all you\n",
            "need. In Proceedings of the 31st International Conference on Neural Information\n",
            "Processing Systems (Long Beach, California, USA) (NIPS’17). Curran Associates\n",
            "Inc., Red Hook, NY, USA, 6000–6010.\n",
            "[35] Yuxin Wang, Yuhan Chen, Zeyu Li, Zhenheng Tang, Rui Guo, Xin Wang, Qiang\n",
            "Wang, Amelie Chi Zhou, and Xiaowen Chu. 2024. Towards Efficient and Reliable\n",
            "LLM Serving: A Real-World Workload Study. arXiv:2401.17644 [cs.DC]\n",
            "[36] Vincent M. Weaver, Matt Johnson, Kiran Kasichayanula, James Ralph, Piotr\n",
            "Luszczek, Dan Terpstra, and Shirley Moore. 2012. Measuring Energy and Power\n",
            "with PAPI. In 2012 41st International Conference on Parallel Processing Workshops.\n",
            "262–268. https://doi.org/10.1109/ICPPW.2012.39\n",
            "[37] Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an\n",
            "insightful visual performance model for multicore architectures. Commun. ACM\n",
            "52, 4 (apr 2009), 65–76. https://doi.org/10.1145/1498765.1498785\n",
            "[38] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, and et al. 2022. Sustainable\n",
            "ai: Environmental implications, challenges and opportunities. Proceedings of\n",
            "Machine Learning and Systems 4 (2022), 795–813.\n",
            "[39] Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. LLM-\n",
            "PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and\n",
            "Adaptive Quantization. arXiv preprint arXiv:2403.01136 (2024).\n",
            "513\n",
            "\n",
            "Can Private LLM Agents Synthesize Household Energy\n",
            "Consumption Data?\n",
            "Mahathir Almashor∗\n",
            "mahathir.almashor@csiro.au\n",
            "CSIRO Energy\n",
            "Sydney, Australia\n",
            "Yusuke Miyashita∗\n",
            "yusuke.miyashita@csiro.au\n",
            "CSIRO Energy\n",
            "Melbourne, Australia\n",
            "Sam West∗\n",
            "sam.west@csiro.au\n",
            "CSIRO Energy\n",
            "Newcastle, Australia\n",
            "Thi Van Dai Dong∗\n",
            "thivandai.dong@csiro.au\n",
            "CSIRO Energy\n",
            "Wollonggong, Australia\n",
            "ABSTRACT\n",
            "Reproducible science requires easy access to data, especially with\n",
            "the rise of data-driven and increasingly complex models used within\n",
            "energy research. Too often however, the data to reconstruct and\n",
            "verify purported solutions in publications is hidden due to some\n",
            "combination of commercial, legal, and sensitivity issues. This early\n",
            "work presents our initial efforts to leverage the recent advance-\n",
            "ments in Large Language Models (LLMs) to create usable and share-\n",
            "able energy datasets. In particular, we’re utilising their mimicry of\n",
            "human behaviors, with the goal of extracting and exploring syn-\n",
            "thetic energy data through the simulation of LLM agents capable of\n",
            "interacting with and executing actions in controlled environments.\n",
            "We also analyse and visualise publicly available data in an attempt\n",
            "to create realistic but not quite exact copies of the originals. Our\n",
            "early results show some promise, with outputs that resemble the\n",
            "twin peak curves for household energy consumption. The hope is\n",
            "that our generalised approach can be used to easily replicate usable\n",
            "and realistic copies of otherwise secret or sensitive data.\n",
            "CCS CONCEPTS\n",
            "• Computing methodologies →Multi-agent systems; Natural\n",
            "language generation; • Security and privacy →Social aspects\n",
            "of security and privacy; • Information systems →Data ana-\n",
            "lytics.\n",
            "KEYWORDS\n",
            "Synthetic Data, Generative AI, Large Language Models, Household\n",
            "Electricity Consumption\n",
            "ACM Reference Format:\n",
            "Mahathir Almashor, Yusuke Miyashita, Sam West, and Thi Van Dai Dong.\n",
            "2024. Can Private LLM Agents Synthesize Household Energy Consumption\n",
            "Data?. In The 15th ACM International Conference on Future and Sustainable\n",
            "∗Commonwealth Science and Industrial Research Organisation (CSIRO) Energy\n",
            "This work is licensed under a Creative Commons\n",
            "Attribution-NonCommercial-ShareAlike International 4.0 License.\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "© 2024 Copyright held by the owner/author(s).\n",
            "ACM ISBN 979-8-4007-0480-2/24/06\n",
            "https://doi.org/10.1145/3632775.3661993\n",
            "Energy Systems (E-Energy ’24), June 04–07, 2024, Singapore, Singapore. ACM,\n",
            "New York, NY, USA, 5 pages. https://doi.org/10.1145/3632775.3661993\n",
            "1\n",
            "INTRODUCTION\n",
            "The energy community is working tirelessly on the transition to\n",
            "renewables and there is a pressing need to share data across research\n",
            "organisations. However, the sharing of useful and contemporary\n",
            "data in many domains is often an exercise fraught with hurdles\n",
            "and competing motives. The barriers to effective sharing range\n",
            "from privacy and cyber-security concerns, through to competing\n",
            "commercial interests. Given the urgency and enormity of the task\n",
            "at hand, we need a way for partners and collaborators across the\n",
            "globe to effectively evaluate their solutions against agreed datasets.\n",
            "It is in this spirit that we present this very early work exploring\n",
            "the generation of synthetic yet realistic data within the energy\n",
            "domain. Our aim is arrive at method to generate raw household\n",
            "energy consumption data that (i) reflects the demographics and\n",
            "behaviours of a particular geographic region; (ii) be realistic and\n",
            "emergent in a way that accounts for the seemingly stochastic nature\n",
            "of human actions; and (iii) be free of the encumbrances that typically\n",
            "prevent the wider dissemination of data between institutions.\n",
            "To this end, we propose the use of private Large Language Mod-\n",
            "els (LLMs) [3] in the synthesis of such data. We see a range of\n",
            "possibilities in the abilities of current models to drive interactions\n",
            "within multi-agent simulations, and want to leverage their unique\n",
            "so-called hallucinations to arrive at realistic behavioural patterns.\n",
            "Put simply, we want our LLM-powered agents to “dream” about\n",
            "their day-to-day actions and organically arrive at energy consump-\n",
            "tion patterns that mirror real-life. The reasons are as follows:\n",
            "• As was seen in [11], emergent behaviours were seen when the\n",
            "authors simulated a small town of 25 agents powered by ChatGPT\n",
            "[10]. This included an unscripted event where the agents created\n",
            "a mayoral election and then proceeded to realistically interact\n",
            "with one another about it. This is the same phenomena we wish\n",
            "to capture, where the agents can naturally vary their behaviours\n",
            "and activities during the day to form a better mimicry of real-life\n",
            "humans. This includes variations that account for their specific\n",
            "characterisations (i.e., identity), occupations, and interactions\n",
            "with immediate family members.\n",
            "• We use the term Private LLM both in terms of their localised\n",
            "nature, as well as their inherent privacy benefits. That is, rather\n",
            "than depend on cloud-based and costly implementations such as\n",
            "664\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "Authors et al.\n",
            "OpenAI’s ChatGPT, we strove for smaller models that can be run\n",
            "within any organisation’s infrastructure. While the performance\n",
            "of these local models may not be on par with their more famous\n",
            "brethren, better accessibility and lower costs presents a good\n",
            "counterbalance. In this way, the barrier to entry is lowered for\n",
            "most institutions when replicating our methods and results.\n",
            "• Private also refers to the ability to keep an organisation’s valuable\n",
            "intellectual property safer and only within its confines. There\n",
            "is no need to share text prompts, techniques and potential seed\n",
            "data with any third-party service as everything is run on-premise\n",
            "or within that organisation’s own cloud infrastructure.\n",
            "There is another dimension which delves further into privacy,\n",
            "and is the reason why we chose household energy consumption at\n",
            "the first attempt. Simply put, it is difficult to obtain current, usable\n",
            "and customisable datasets because it invariably impacts real-life\n",
            "privacy concerns. We cannot effectively measure the daily consump-\n",
            "tion of a human household, let alone the hundreds or thousands\n",
            "within a geographical region. In commercial settings, there are legal\n",
            "constraints on the collection of consumption data [13], given the in-\n",
            "trusiveness and sensitivities such gathering would entail. We often\n",
            "have to rely on observations, household demographics, presence of\n",
            "appliances, and wider energy usage that are entirely self-reported\n",
            "[1], with all the data quality issues that entails. In any case, electrical\n",
            "load is typically only measured at the meter, requiring heuristics or\n",
            "additional hardware to ascertain the presence and consumption of\n",
            "individual appliances. It is impractical to measure consumption of\n",
            "each appliance and outlet for an entire city, not to mention how in-\n",
            "vasive and time-consuming this would be for any given household.\n",
            "Our contributions may be summarised as follows:\n",
            "• A novel addition to an LLM-enabled simulation engine to gen-\n",
            "erate emergent daily routines of multiple agents, and the subse-\n",
            "quent extraction of corresponding energy patterns.\n",
            "• The customisation of an existing simulation engine with a pri-\n",
            "vate LLM implementation called Mistral that can be run within\n",
            "localised infrastructure and without costly ongoing access fees.\n",
            "• Experimentation with our approach that exhibits promising abil-\n",
            "ity to replicate the consumption patterns seen in publicly avail-\n",
            "able datasets that were discovered and analysed.\n",
            "Ethical Considerations\n",
            "No personally identifiable information (PII) or other sensitivities\n",
            "were found in the household electricity consumption data used\n",
            "beyond anonymised age-brackets, demographics and post-codes.\n",
            "2\n",
            "BACKGROUND & RELATED WORK\n",
            "2.1\n",
            "Synthetic Energy Data\n",
            "The use of GANs for generating energy time series data has recently\n",
            "gained prominence, fueled by the increased use of time series data\n",
            "across various domains. One objective in generating time series is\n",
            "to accurately capture temporal dynamics. Some earlier approaches,\n",
            "such as C-RNN-GAN [9] and RGAN/RCGAN [4] have been de-\n",
            "veloped to learn the temporal variations of data. However, these\n",
            "approaches require real data for model train, which in turn poses\n",
            "the risk of leaking sensitive information in any generated outputs.\n",
            "00:00\n",
            "01:00\n",
            "02:00\n",
            "03:00\n",
            "04:00\n",
            "05:00\n",
            "06:00\n",
            "07:00\n",
            "08:00\n",
            "09:00\n",
            "10:00\n",
            "11:00\n",
            "12:00\n",
            "13:00\n",
            "14:00\n",
            "15:00\n",
            "16:00\n",
            "17:00\n",
            "18:00\n",
            "19:00\n",
            "20:00\n",
            "21:00\n",
            "22:00\n",
            "23:00\n",
            "Time\n",
            "0.2\n",
            "0.4\n",
            "0.6\n",
            "0.8\n",
            "1.0\n",
            "1.2\n",
            "1.4\n",
            "Energy Consumption (Kwh)\n",
            "Daily Energy Consumption\n",
            "Random Household\n",
            "Aggregated Mean of All Household\n",
            "Figure 1: Snapshots of daily and aggregated mean of typical\n",
            "household energy consumption.\n",
            "Our approach attempts to overcome this issue by conducting\n",
            "household activities within an LLM-powered simulation world, and\n",
            "subsequently extracting energy data from those activities. This\n",
            "essentially avoids the use of any real data altogether and thus\n",
            "minimising any privacy or sensitivity concerns. In other words,\n",
            "there is little to no risk of inadvertently replicating a household’s\n",
            "exact consumption patterns, as the genesis of the data creation is\n",
            "entirely independent from any real-life source.\n",
            "2.2\n",
            "LLM and LLM Agents\n",
            "Large Language Models, epitomised by groundbreaking models\n",
            "like OpenAI’s GPT-3[2], FaceBook’s Llama2[14] and Mistral[6] AI’s\n",
            "eponymous model have showcased strong abilities to interpret, gen-\n",
            "erate, and simulate human-like text. It would be an understatement\n",
            "to say that LLM technologies have gained popularity recently, with\n",
            "various research looking at everything from teasing out hidden\n",
            "meanings in speech [5] to using their generation capabilities in\n",
            "software programming tasks [7].\n",
            "Beyond the obvious chat-bot category of applications, there\n",
            "is also a significant amount of research in LLM agents aimed at\n",
            "imitating human behavior. A primary example of this is Simulacra\n",
            "[11], which introduces a fusion between LLM and computationally\n",
            "interactive agents in a sandbox environment to enable believable\n",
            "simulations of human behavior. Similarly, MemGPT [8] uses LLM\n",
            "as an operating system, allowing it to think, reflect, and produce\n",
            "actions to interact with external devices. Here, we utilized the\n",
            "Mistral-7B private LLM to power the Simulacra agents instead of\n",
            "the original ChatGPT backend, due to cost and accessibility reasons.\n",
            "2.3\n",
            "Benchmark Datasets\n",
            "The Smart Grid Smart City Customer (SGSC) [1] data was collected\n",
            "between 2010 and 2014 as part of a joint industry and government\n",
            "initiative. It was one of only a few openly available datasets on\n",
            "household energy consumption that we discovered. It contains\n",
            "30-minute interval readings of electricity usage and generation\n",
            "(measured in kWh) for 78,720 participating customers, of which we\n",
            "only had access to a subset of 13,735 households.\n",
            "In addition, we also discovered the Solar Cities dataset [12],\n",
            "which contained energy consumption and generation information\n",
            "665\n",
            "Can Private LLM Agents Synthesize Household Energy Consumption Data?\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "for almost 38,000 homes in seven Australian cities, recorded at\n",
            "30-minute intervals from 2005 to 2013. After an extensive pre-\n",
            "processing step, we focused on only 4,332 households due to various\n",
            "data quality and accessibility issues. This data was collected mainly\n",
            "as part of a governmental initiative design to measure the impact\n",
            "of direct interventions on household usage patterns.\n",
            "In much the same way, there is the possibility to design for\n",
            "and simulate interventions to consumption patterns within our pro-\n",
            "posed approach. In [12], consumption was measured for households\n",
            "before and after an intervention such as the installation of solar\n",
            "panels, in an attempt to gauge their impacts to electricity demands.\n",
            "Similarly, we can use our LLM-enabled approach to simulate the\n",
            "same objective, by introducing interventions and appropriate pric-\n",
            "ing dynamics, and watching how the agents respond. There is also\n",
            "the ability to model the impact of incentives on the agents, and\n",
            "measure the organic spread of interventions introduced gradually.\n",
            "Figure 1 shows the daily energy consumption for randomly cho-\n",
            "sen household (in blue) as well as aggregated mean of all house-\n",
            "holds in our dataset (in red). Daily energy consumption varies for\n",
            "each household, but we see the trend of two peaks in the morning\n",
            "around 8am and evening around 7pm. We see the typical morning\n",
            "and evening peaks of energy usage in the aggregated mean, which\n",
            "is a well-known phenomena observed in individual households and\n",
            "up to the wider grid. On the other hand we witness the expected\n",
            "variability in energy usage for a single household on any given day.\n",
            "Figure 2: Example simulation step of two agents in their daily\n",
            "activities, with one using an electrical appliance (their TV).\n",
            "3\n",
            "APPROACH\n",
            "The approach involves two stages, with the first stage consisting\n",
            "of running the simulacra, and the second involving the extraction\n",
            "of household energy data from the simulation using a variety of\n",
            "methods to perform this extraction. For each step of the simulation,\n",
            "description of each persona’s actions and objects they are interact-\n",
            "ing with is generated. They are in the format of \"Persona A is doing\n",
            "Action B at Location C\". From these, appliances which consumes\n",
            "energy are identified. The advantage of this two-stage approach\n",
            "is its applicability onto any other LLM simulator. That is, simply\n",
            "allow the LLM to record its actions at each step, and then proceed\n",
            "with extracting useful information from it.\n",
            "3.1\n",
            "Private LLM in Simulacra\n",
            "Utilizing ChatGPT or any other API-based LLM can offer conve-\n",
            "nience, reliability, and strong performance. However, this approach\n",
            "may entail transferring sensitive data and routing it through ex-\n",
            "ternal services. In contrast, the deployment of Private/Local LLM\n",
            "enables the execution of the entire simulation within closed envi-\n",
            "ronments, ensuring the security and privacy of the data.\n",
            "We utilized Mistral-7B as it is one of the highest performing\n",
            "smaller models which are relatively fast when running iterative\n",
            "experiments, and more importantly, can be fit into more modest\n",
            "compute infrastructures. “7B” here refers to 7 Billion parameters,\n",
            "with some LLM models reaching 65B and more. Suffice it to say for\n",
            "now that the smaller the number of parameters, the more manage-\n",
            "able it is to run within an organisation’s infrastructure.\n",
            "This smaller Mistral model is then plugged into the Simulacra\n",
            "platform [11] as its LLM engine. The authors recommend a ro-\n",
            "bust LLM for the agents, ideally as proficient as or superior to\n",
            "ChatGPT-3.5 Turbo as using smaller models may degrade the sim-\n",
            "ulation quality. However, we’ve found that they still exhibit the\n",
            "ability to generate reasonable plans and actions, resulting in a plau-\n",
            "sible human-like energy patterns. For simplicity, we’ve reused the\n",
            "prompts within the original paper to condition our LLM agents.\n",
            "3.2\n",
            "Energy Data Extraction\n",
            "3.2.1\n",
            "String Match. In this straightforward approach, we employ\n",
            "string matching to determine whether the descriptions of daily\n",
            "activities from the simulations include energy appliances. This\n",
            "baseline is perhaps the most accurate reflection of energy data\n",
            "extraction from agent activities. For example in Figure 2, the simu-\n",
            "lation step is described as “Maria Lopez is watching TV@common\n",
            "room sofa, Klaus Mueller is having dinner@kitchen sink”, the ap-\n",
            "pliance TV is easily matched and included as a proxy for energy use.\n",
            "The identified list of appliances in the simulations are as follows:\n",
            "TV, shower, refrigerator, toaster, cooking area, microphone, piano,\n",
            "game console, computer desk, and computer.\n",
            "So, for every mention of an appliance in each time-step’s de-\n",
            "scription, we mark that as an active use of that appliance. A simple\n",
            "counting of all the appliances used in the household is performed\n",
            "for each time-step in the simulated day. This allows us to get a mea-\n",
            "sure of energy consumption purely from the activities of agents\n",
            "with regards to appliances, as they go about their day.\n",
            "3.2.2\n",
            "Other Approaches. In our quest to extract more accurate en-\n",
            "ergy usage, we also experimented with Semantic Embedding, which\n",
            "employs a text encoder to match the description of LLM generated\n",
            "interactions against appliances based on semantic meaning. One\n",
            "advantage with this method is its ability to capture synonyms or\n",
            "phrasings describing the same appliance. For instance, when the de-\n",
            "scription states “Maria is streaming on Twitch”, we can surmise that\n",
            "this would involve the use of either a game console or a computer.\n",
            "This semantic embedding method may better capture such cases\n",
            "compared to basic text matching. However, setting a threshold for\n",
            "matching top-K text embeddings was found to be challenging and\n",
            "resulted in inaccuracies with the extraction of energy data.\n",
            "We also experimented with using the LLM engine itself to in-\n",
            "fer appliance usage. LLMs have the capability to extract potential\n",
            "appliance use even if they are not present in the environment, or\n",
            "if the LLM agents are not explicitly described as interacting with\n",
            "them. Thus, with a line like “Maria reads a book while listening to\n",
            "666\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "Authors et al.\n",
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "Hours\n",
            "0.0\n",
            "0.1\n",
            "0.2\n",
            "0.3\n",
            "0.4\n",
            "0.5\n",
            "0.6\n",
            "0.7\n",
            "0.8\n",
            "State of Appliances\n",
            "Isabella House and Cafe Daily Usage\n",
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "Hours\n",
            "0.0\n",
            "0.1\n",
            "0.2\n",
            "0.3\n",
            "0.4\n",
            "0.5\n",
            "0.6\n",
            "0.7\n",
            "0.8\n",
            "State of Appliances\n",
            "Maria and Klaus Sharehouse Daily Usage\n",
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "Hours\n",
            "0.0\n",
            "0.1\n",
            "0.2\n",
            "0.3\n",
            "0.4\n",
            "0.5\n",
            "0.6\n",
            "0.7\n",
            "0.8\n",
            "State of Appliances\n",
            "Lin Household Daily Usage\n",
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "Hours\n",
            "0.0\n",
            "0.1\n",
            "0.2\n",
            "0.3\n",
            "0.4\n",
            "0.5\n",
            "0.6\n",
            "0.7\n",
            "0.8\n",
            "State of Appliances\n",
            "Moreno Household Daily Usage\n",
            "Figure 3: Single day energy usage for four simulated households\n",
            "music”, the LLM may infer that an appliance like a radio or TV is\n",
            "also turned on for the purposes of that background music. However,\n",
            "the challenge here is the occurrence of hallucinations, making it\n",
            "difficult to precisely control their outputs. After extensive analysis,\n",
            "both Semantic and LLM methods were abandoned in favour of the\n",
            "basic but most accurate string matching technique.\n",
            "4\n",
            "RESULTS\n",
            "The status of each appliance at every step is binary, indicating\n",
            "whether it is on or off. This information is consolidated across all\n",
            "appliances to generate discrete values for energy usage. Further-\n",
            "more, we implemented a rolling mean with a 1-hour window to\n",
            "smooth out our data, aiming to alleviate the instantaneous rises\n",
            "and drops in energy usage, given that energy usage is treated in\n",
            "binary states. Figure 3 depicts the daily energy usage patterns of our\n",
            "four simulated households, which include (clockwise from top-left)\n",
            "Isabella, Lin, Moreno, and Maria and Klaus. This is represented\n",
            "against the rolling mean of appliance states at each time-step.\n",
            "As can be seen, the daily energy data for each household varies\n",
            "according to their routines, occupations, and lifestyles. This is ex-\n",
            "actly the variety in energy usage patterns we were aiming for, which\n",
            "resembles the variances seen in recorded single-day usage seen in\n",
            "Figure 1. The energy usage reflects the simulated activities of the\n",
            "LLM agents. For example, the agent Isabella wakes up around 6 am,\n",
            "opens her cafe at 8 am, works at the counter until 8 pm, closes the\n",
            "cafe, and goes to bed around 11 pm. These activities correspond to\n",
            "actions such as turning on the lights in the cafe, serving customers,\n",
            "using the refrigerator, toaster, and cooking area, which collectively\n",
            "contributes to the overall energy consumption pattern.\n",
            "Similarly, these variances are observed for the Lin and Moreno\n",
            "households. The unique shapes of each household’s curve indicates\n",
            "that both the prompt of the LLMs and the simulation environment\n",
            "influence their activities, resulting in their distinct energy consump-\n",
            "tion curves. This suggests that by conditioning the prompt, one can\n",
            "generate tunable synthetic energy data for the desired household,\n",
            "within the limits of the simulated environment.\n",
            "Furthermore, to determine the average daily energy usage per\n",
            "person, the mean is calculated from the combined data by summing\n",
            "all the households’ consumption at each time-step. In diagram 4,\n",
            "we illustrate two peaks near morning and evening times, capturing\n",
            "the demand periods often seen on weekdays in many aggregated\n",
            "energy datasets. The fluctuations and small peaks during the day\n",
            "may arise from the nature of the simulation environments, such as\n",
            "Isabella running a cafe (which is counted against her household’s\n",
            "data) and LLM agents returning home for lunch breaks.\n",
            "5\n",
            "CONCLUSION\n",
            "The proposed approach uses private LLMs to synthesize daily house-\n",
            "hold energy consumption patterns. This is done with privacy in\n",
            "mind, and by using the emergent properties of LLMs to arrive at\n",
            "realistic datasets that can be then be freely shared amongst the\n",
            "energy community. The focus is also on using less computationally\n",
            "intensive and costly LLMs that allows for much easier adoption.\n",
            "5.1\n",
            "Limitations & Future Work\n",
            "The simulation results heavily depend on the capabilities of the LLM,\n",
            "as more advanced LLMs would lead to more realistic simulations\n",
            "and extracted energy data. While early results somewhat resembles\n",
            "the real data, the LLM simulation outputs only binary states of\n",
            "appliances, which doesn’t capture continuously varying loads well,\n",
            "and cannot simulate the majority of detailed activities in the cities\n",
            "from which the real data was measured. The translation from state\n",
            "to actual usage remains a potential area for future research.\n",
            "Figure 4: Aggregated mean of energy usage from all four\n",
            "households containing a total of eight agents.\n",
            "667\n",
            "Can Private LLM Agents Synthesize Household Energy Consumption Data?\n",
            "E-Energy ’24, June 04–07, 2024, Singapore, Singapore\n",
            "Generating more detailed energy data will require integrating\n",
            "more detailed interactions with appliances, simulating many more\n",
            "agents and will likely require integrating additional factors like\n",
            "climate, weather, traffic, seasons, demographics, and industries. Fur-\n",
            "thermore, the simulation does not assess climate control systems\n",
            "(e.g. air conditioning), lighting, or transport, despite their signifi-\n",
            "cant electricity consumption at home. We posit that they could be\n",
            "treated as constant factors used while occupants are at home, thus\n",
            "not affecting the dynamic usage of appliances. Doubtlessly, there\n",
            "remains a host of further experimentation and rigorous analysis to\n",
            "prove the effectiveness of this approach. The hope is that the wider\n",
            "energy research community will see its value, and collaborate to\n",
            "generate publicly available realistic synthetic datasets.\n",
            "ACKNOWLEDGMENTS\n",
            "This work was supported by resources provided by the Pawsey\n",
            "Supercomputing Centre with funding from the Australian Govern-\n",
            "ment and the Government of Western Australia.\n",
            "REFERENCES\n",
            "[1] Energy Australian Government Department of Climate Change. 2014. Smart-Grid\n",
            "Smart-City Customer Trial Data. https://www.data.gov.au/data/dataset/smart-\n",
            "grid-smart-city-customer-trial-data Last Modified: 2022-04-11T01:46:18.101034.\n",
            "[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\n",
            "Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\n",
            "Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\n",
            "Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\n",
            "Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\n",
            "Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\n",
            "Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\n",
            "arXiv:2005.14165 [cs.CL]\n",
            "[3] Vinton G. Cerf. 2023. Large Language Models. Commun. ACM 66, 8 (July 2023),\n",
            "7. https://doi.org/10.1145/3606337\n",
            "[4] Cristóbal Esteban, Stephanie L Hyland, and Gunnar Rätsch. 2017. Real-valued\n",
            "(medical) time series generation with recurrent conditional gans.\n",
            "[5] Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is ChatGPT better than Human\n",
            "Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate\n",
            "Speech. In Companion Proceedings of the ACM Web Conference 2023 (WWW ’23\n",
            "Companion). Association for Computing Machinery, New York, NY, USA, 294–297.\n",
            "https://doi.org/10.1145/3543873.3587368\n",
            "[6] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-\n",
            "vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
            "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
            "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
            "and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]\n",
            "[7] Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara J. Ericson, David\n",
            "Weintrop, and Tovi Grossman. 2023. Studying the effect of AI Code Generators\n",
            "on Supporting Novice Learners in Introductory Programming. In Proceedings\n",
            "of the 2023 CHI Conference on Human Factors in Computing Systems (CHI ’23).\n",
            "Association for Computing Machinery, New York, NY, USA, 1–23. https://doi.\n",
            "org/10.1145/3544548.3580919\n",
            "[8] MemGPT Community. 2024. Introduction. https://memgpt.readme.io/docs/index\n",
            "[9] Olof Mogren. 2016. C-RNN-GAN: Continuous recurrent neural networks with\n",
            "adversarial training.\n",
            "[10] OpenAI Blog. 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt\n",
            "[11] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy\n",
            "Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simu-\n",
            "lacra of Human Behavior. In Proceedings of the 36th Annual ACM Symposium on\n",
            "User Interface Software and Technology (UIST ’23). Association for Computing\n",
            "Machinery, New York, NY, USA, 1–22. https://doi.org/10.1145/3586183.3606763\n",
            "[12] Saad Sayeef, Sam West, Stephen Lindsay, Brad Sparkes, and Kate Cavanagh.\n",
            "2013. Solar Cities Data Analysis Final Report.\n",
            "https://publications.csiro.au/\n",
            "rpr/pub?list=SEA&pid=csiro:EP137924&sb=RECENT&expert=false&n=5&rpp=\n",
            "25&page=1&tr=6&q=Solar%20Cities%20Data%20Analysis&dr=all\n",
            "Publisher:\n",
            "Department of Resources, Energy and Tourism.\n",
            "[13] René Schwermer, Jonas Buchberger, Ruben Mayer, and Hans-Arno Jacobsen.\n",
            "2022. Federated office plug-load identification for building management systems.\n",
            "In Proceedings of the Thirteenth ACM International Conference on Future Energy\n",
            "Systems (e-Energy ’22). Association for Computing Machinery, New York, NY,\n",
            "USA, 114–126. https://doi.org/10.1145/3538637.3538845\n",
            "[14] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\n",
            "mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\n",
            "ale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucu-\n",
            "rull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia\n",
            "Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,\n",
            "Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\n",
            "Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\n",
            "Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,\n",
            "Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,\n",
            "Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\n",
            "Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross\n",
            "Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\n",
            "Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro-\n",
            "driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:\n",
            "Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]\n",
            "668\n",
            "\n",
            "Democratizing Energy Management with LLM-Assisted Optimization\n",
            "Autoformalism\n",
            "Ming Jin, Bilgehan Sel, Fnu Hardeep, Wotao Yin\n",
            "Abstract— This paper introduces a method for personaliz-\n",
            "ing energy optimization using large language models (LLMs)\n",
            "combined with an optimization solver. This approach, termed\n",
            "human-guided optimization autoformalism, translates natural\n",
            "language speciﬁcations into optimization problems, enabling\n",
            "LLMs to handle various user-speciﬁc energy-related tasks. It\n",
            "allows for nuanced understanding and nonlinear reasoning\n",
            "tailored to individual preferences. The research covers common\n",
            "energy sector tasks like electric vehicle charging, HVAC control,\n",
            "and long-term planning for renewable energy installations. This\n",
            "novel strategy represents a signiﬁcant advancement in context-\n",
            "based optimization using LLMs, facilitating sustainable energy\n",
            "practices customized to individual needs.\n",
            "I. INTRODUCTION\n",
            "Despite computational advances, the complex challenges\n",
            "of meeting energy demands and reducing carbon emissions\n",
            "persist. Optimization techniques, as seen in EV charging [1],\n",
            "energy storage [2], renewable investments [3], smart building\n",
            "operations [4], and demand side management [5], hold great\n",
            "promise. However, the broader democratization of these\n",
            "tools remains a signiﬁcant hurdle. This paper tackles this\n",
            "issue, advocating for the accessibility and practicality of\n",
            "advanced computational methods for all, especially the un-\n",
            "derserved [6]. The emergence of LLMs offers a breakthrough\n",
            "in overcoming these barriers. We demonstrate that LLMs can\n",
            "bridge the gap between the high costs of traditional optimiza-\n",
            "tion and the need for personalized, accessible solutions. Our\n",
            "goal is not industry-grade optimization, but rather to provide\n",
            "users with tools for informed decision-making, potentially\n",
            "involving optimization problem formulation. This approach\n",
            "leverages LLMs to streamline modeling and optimization,\n",
            "enabling interaction through natural language without exten-\n",
            "sive programming or mathematical optimization knowledge.\n",
            "Technical challenges and solutions. While LLMs such as\n",
            "OpenAI’s ChatGPT offer a natural substrate for conversa-\n",
            "tional AI (a technology that enables machines to understand,\n",
            "interpret, and engage in natural conversations), it is essential\n",
            "to recognize that current LLMs have not excelled in tasks like\n",
            "arithmetic and logical reasoning [7]. Recent works have in-\n",
            "troduced methods such as chain of thoughts [8] (and variants\n",
            "such as algorithm of thoughts [9]), which prompts a series\n",
            "of intermediate reasoning steps, and autoformalism [10],\n",
            "which automatically translates natural language mathematics\n",
            "to formal speciﬁcations and proofs. However, many energy\n",
            "problems, like energy storage control and long-term planning\n",
            "for PV panel installation, demand complex decision-making.\n",
            "These problems differ from arithmetic reasoning, common-\n",
            "sense reasoning, and symbolic reasoning in the following\n",
            "aspects: Complexity, as they often involve numerous vari-\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "Solar panel\n",
            "HVAC control\n",
            "Heat pump\n",
            "EV charging\n",
            "Battery\n",
            "charging\n",
            "Battery\n",
            "capacity\n",
            "Success rate (%)\n",
            "Problem category\n",
            "Baseline\n",
            "EC (CVXPY)\n",
            "EC (SCIPY)\n",
            "Fig. 1: Comparison of the baseline method (simple prompt-\n",
            "ing with GPT-4) and EC using either CVXPY or SCIPY as\n",
            "the code projection layer across various problem categories.\n",
            "Success rate denotes the percentage of instances in which the\n",
            "provided answer aligns with user expectations, satisﬁes all\n",
            "given constraints, and is either close to or exactly optimal.\n",
            "ables, constraints, and objectives with potentially nonlinear\n",
            "relationships between variables; and incomplete information,\n",
            "as energy systems are inﬂuenced by various factors, such as\n",
            "user preferences, which may not be provided in the initial\n",
            "problem description.\n",
            "A. Contributions\n",
            "In this research, we address the above challenges as\n",
            "follows. To manage complexity, we develop a procedure\n",
            "to map a problem description to code using the grammar\n",
            "of packages that support optimization formulation, and to\n",
            "iteratively formulate, debug, and execute the program. We\n",
            "also introduce external tooling to execute the written code\n",
            "and solve the formulated optimization using dedicated al-\n",
            "gorithms. Furthermore, we implement prompt engineering\n",
            "techniques to enable LLMs to accurately understand and\n",
            "respond to user-speciﬁc preferences. To address incomplete\n",
            "information, we leverage the reasoning capabilities of LLMs\n",
            "to identify key parameters and use a question-and-answer for-\n",
            "mat in natural conversations to solicit this information from\n",
            "users. Additionally, we employ LLMs for auto-informalism\n",
            "to explain the solution to users. An overview of the EC\n",
            "framework is depicted in Fig. 2. This approach bears dual\n",
            "beneﬁts: it not only guides the user through complex nonlin-\n",
            "ear reasoning tasks of energy saving planning but also offers\n",
            "an insightful explanation of the solution.\n",
            "We further demonstrate the possibility of our proposed ap-\n",
            "proach in solving a variety of tasks within the energy domain,\n",
            "ranging from EV charging, HVAC and battery control, to\n",
            "long-term planning problems such as cost-beneﬁt evaluation\n",
            "of installing rooftop solar PVs, heat pumps, and battery\n",
            "sizing. In comparison to the simple prompting method,\n",
            "where the task description is directly presented to the LLM,\n",
            "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\n",
            "979-8-3503-1855-5/24/$31.00 ©2024 IEEE\n",
            "258\n",
            "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm) | 979-8-3503-1855-5/24/$31.00 ©2024 IEEE | DOI: 10.1109/SmartGridComm60555.2024.10738100\n",
            "Authorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \n",
            "our proposed EC framework improves the success rate, as\n",
            "illustrated in Fig. 1. Although the optimizations formulated\n",
            "are not exceedingly sophisticated, we ﬁnd that the LLM\n",
            "effectively addresses the problem to a considerable degree.\n",
            "Throughout the remainder of the paper, unless speciﬁed\n",
            "otherwise, we refer to OpenAI’s ChatGPT (GPT-4) when\n",
            "mentioning LLM.\n",
            "B. Related Work\n",
            "Recent work has explored using LLMs for mathematical\n",
            "autoformalization, converting natural language into formal\n",
            "languages like Isabelle/HOL [11]. Analogously, we use in-\n",
            "termediaries like SCIPY and CVXPY to transform intuitive\n",
            "formulations of optimization problems into standard form\n",
            "[12]. Despite few CVXPY examples online, LLMs exhibit\n",
            "surprising adeptness, albeit with occasional syntax errors that\n",
            "we correct via Python debugging. While LLMs struggle with\n",
            "multi-step problems [13], techniques like in-context learning\n",
            "[14] and algorithmic search [9] have shown promise. We\n",
            "present a novel viewpoint using optimization itself as a rea-\n",
            "soning tool to enrich existing LLM augmentation techniques\n",
            "[15]. LLMs can also revise responses given feedback, e.g. in\n",
            "question-answering and code debugging [16]. We incorporate\n",
            "revisions based on programming language feedback and user\n",
            "requests, enabling rapid error correction.\n",
            "Optimizing energy systems requires commonsense reason-\n",
            "ing and domain knowledge [17]. Prior work has used ML\n",
            "and expert systems [18], but we harness LLM expertise for\n",
            "optimization through natural language interaction. To our\n",
            "knowledge, we are the ﬁrst to tackle non-trivial, energy-\n",
            "speciﬁc problems with incomplete information this way.\n",
            "The remaining sections of this paper are structured as\n",
            "follows. Section II describes our proposed framework in\n",
            "detail, including the design principles and the optimization\n",
            "autoformalism approach. Section III presents experimental\n",
            "results that demonstrate the effectiveness of our approach\n",
            "in solving various energy-related tasks. We discuss the po-\n",
            "tential and future directions of conversational AI for energy\n",
            "sustainability and conclude in Section IV.\n",
            "II. ENERGY CONCIERGE FRAMEWORK\n",
            "Using an EV charging query as an example, our Energy\n",
            "Concierge operates as follows:\n",
            "1) User submits natural language request (e.g. optimize\n",
            "my charging schedule). LLM determines if it is an\n",
            "optimization problem.\n",
            "2) If so, LLM requests input parameters needed to solve\n",
            "it. It may ask clarifying questions (e.g. charging ca-\n",
            "pacity? preferred hours?).\n",
            "3) LLM formulates Python code with user information,\n",
            "translating the query into an optimization problem via\n",
            "autoformalism.\n",
            "4) Interface executes code to solve problem, with debug-\n",
            "ging iterations if needed.\n",
            "5) LLM explains the optimal solution clearly (charging\n",
            "schedule, cost savings). Enabling informed user deci-\n",
            "sions.\n",
            "Fig. 2: Energy Concierge framework. The user engages with\n",
            "an LLM through natural language queries and responses.\n",
            "The LLM identiﬁes the necessary input parameters for opti-\n",
            "mization and generates Python code to address the problem.\n",
            "The program interface then executes the code and relays the\n",
            "solution back to the LLM, which subsequently provides a\n",
            "clear explanation to the user.\n",
            "Through interactivity and personalization grounded in opti-\n",
            "mization and autoformalism, this conversational framework\n",
            "empowers users to improve energy efﬁciency.\n",
            "A. Optimization autoformalism\n",
            "A general optimization problem can be written as:\n",
            "minimizex2Rp\n",
            "f(x; ✓)\n",
            "subject to\n",
            "gi(x; ✓) 0,\n",
            "i = 1, 2, . . . , m\n",
            "hj(x; ✓) = 0,\n",
            "j = 1, 2, . . . , n,\n",
            "(P(✓))\n",
            "where we seek to minimize the objective function f(x; ✓)\n",
            "subject to a set of inequality constraints gi(x; ✓) 0, and\n",
            "equality constraints hj(x; ✓) = 0. Here, x 2 Rp is the\n",
            "decision variable and ✓is the collection of hyperparameters\n",
            "that deﬁnes the optimization instance, including objective\n",
            "and constraint functions; in other words, the solution of\n",
            "the optimization P(✓) can be regarded as a function of the\n",
            "hyperparameters ✓[19].\n",
            "Optimization techniques can be used to solve energy-\n",
            "related problems [1]–[5]. Automating the formulation and\n",
            "solution of optimization problems is essential due to the tech-\n",
            "nical skills gap, challenges in manually incorporating user\n",
            "preferences and constraints, and the inefﬁciencies in manual\n",
            "modiﬁcations based on user feedback. The human-guided\n",
            "autoformalism proposed in this study automatically translates\n",
            "natural language task speciﬁcations to optimization instances.\n",
            "However, directlying implementing this approach may face\n",
            "issues such as ambiguity, incompleteness, and incorporating\n",
            "user-speciﬁc preferences. The subsequent subsections will\n",
            "delineate strategies to address them.\n",
            "1) Optimization formulation: The optimization process\n",
            "begins with identifying the objective function, decision vari-\n",
            "ables, and constraints from the user’s task description [T].\n",
            "We tested two approaches:\n",
            "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\n",
            "259\n",
            "Authorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \n",
            "• Approach 1: Directly ask the LLM to identify the\n",
            "optimization components in [T].\n",
            "• Approach 2: First prompt the LLM to identify the\n",
            "5 most important parameters in [T], then use these\n",
            "parameters to formulate the optimization instance.\n",
            "Approach 2 outperforms Approach 1 because LLMs like\n",
            "GPT-4 are trained on diverse data, not just optimization tasks.\n",
            "Asking for the top 5 parameters simpliﬁes the task and pro-\n",
            "vides context, helping LLMs generate more precise optimiza-\n",
            "tion formulations [8], [9]. Directly requesting optimization\n",
            "formulations (Approach 1) can introduce ambiguityDirectly\n",
            "requesting optimization formulations can introduce ambigu-\n",
            "ity; initial parameter identiﬁcation (Approach 2) reduces this\n",
            "uncertainty.\n",
            "After identifying the essential parameters, the natural\n",
            "language query is transformed into a computational instance\n",
            "using the prompt: “Write a Python code using [lib] to solve\n",
            "this optimization problem,” where [lib] is either CVXPY or\n",
            "SCIPY. We found that SCIPY yields a higher success rate\n",
            "(71% to 100%) compared to CVXPY (51% to 80%) (see\n",
            "Fig. 1). This difference is likely due to SCIPY’s broader\n",
            "acceptance and longer existence in the Python community,\n",
            "resulting in more SCIPY-related content available online for\n",
            "the model to leverage during training.1\n",
            "2) Solving an optimization and debugging: To address\n",
            "LLMs’ limitations in nonlinear reasoning [7], we enable\n",
            "the model to interact with an external Python program to\n",
            "solve optimization tasks. We extract the code block from\n",
            "the LLM’s output using regular expressions, searching for\n",
            "unique delimiters enclosing the code. This method reliably\n",
            "extracts code without requiring a large labeled dataset, unlike\n",
            "training a machine learning model [20]. During development,\n",
            "we encountered two types of errors in the LLM-generated\n",
            "code: erroneous translation into an optimization problem and\n",
            "syntactic bugs in an otherwise correctly translated problem.2\n",
            "To rectify translation errors, we rely on user interaction\n",
            "and clariﬁcation of the formulated optimization. For syntactic\n",
            "errors, we use an automated process: identify the error\n",
            "message, feed it to the LLM to isolate relevant code snippets,\n",
            "generate potential remedies, and assess the proposed solu-\n",
            "tions in a new iteration (Fig. 7 in Appendix). Taking multiple\n",
            "code samples before debugging often results in fewer LLM\n",
            "queries (Fig. 5). By following this procedure, we successfully\n",
            "resolved most errors encountered during our experiments.\n",
            "3) Optimization auto-informalism: After discovering the\n",
            "optimal solution, the LLM articulates the results in a com-\n",
            "prehensible, natural language format. This system provides\n",
            "detailed explanations of the optimal solution and any con-\n",
            "straints or preferences factored in during the optimization\n",
            "process (Steps 7a and 7b in Fig. 2). This auto-informalism\n",
            "approach complements autoformalism by offering intuitive\n",
            "1A search on GitHub (as of 4/25/24) returns 8K repositories for ”SCIPY”\n",
            "and only 272 for ”CVXPY,” supporting our preference for SCIPY.\n",
            "2Errors can also arise due to the limitations of the optimization solvers,\n",
            "but since most energy problems involve linear or quadratic convex opti-\n",
            "mization, we presume such errors are comparatively infrequent. We do not\n",
            "classify infeasibility as an error.\n",
            "0\n",
            "0.03\n",
            "0.06\n",
            "0.09\n",
            "0.12\n",
            "0.15\n",
            "0.18\n",
            "0.21\n",
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "12\n",
            "14\n",
            "6pm\n",
            "7pm\n",
            "8pm\n",
            "9pm\n",
            "10pm\n",
            "11pm\n",
            "12am\n",
            "1am\n",
            "2am\n",
            "3am\n",
            "4am\n",
            "5am\n",
            "Electricity Price ($)\n",
            "Charging Power (kW)\n",
            "Common Charging\n",
            "Baseline\n",
            "EC\n",
            "Price\n",
            "Fig. 3: A comparison of EV charging plans to improve\n",
            "the cost and load on the grid. Observations reveal that EC\n",
            "effectively capitalizes on hours with lower prices in contrast\n",
            "to the baseline.\n",
            "insight into the optimization outcomes.\n",
            "III. EXPERIMENTS AND CASE STUDIES\n",
            "This section examines the potential of LLM-based auto-\n",
            "formalism in real-time decision-making (Sec. III-A) and sus-\n",
            "tainable long-term planning (Sec. III-B). A general analysis\n",
            "of the proposed methods is provided in Sec. III-C. The [21,\n",
            "appendix] offers example interactions.\n",
            "A. Real-time decision making\n",
            "1) Smart EV charging: The core issue in smart EV\n",
            "charging is to optimize EV charging patterns to balance\n",
            "power grid loads, mitigate energy costs, and fulﬁll user pref-\n",
            "erences [1]. EC can correspond user-provided information to\n",
            "optimization parameters, such as the maximum charging rate.\n",
            "It grasps the user’s charging availability and translates this\n",
            "information into decision variables and constraints, leading\n",
            "to an optimized schedule (see Fig. 3).\n",
            "2) HVAC control: HVAC control is a pivotal issue that\n",
            "has been extensively studied [4]. However, the algorithms\n",
            "that have been developed are often more advanced than\n",
            "the current control panels in most buildings, leading to a\n",
            "disconnect between sophisticated methods and user compre-\n",
            "hension. The heart of the issue is to discover a setpoint\n",
            "that ensures a comfortable indoor climate while minimizing\n",
            "energy consumption. EC provides clear and actionable advice\n",
            "like “Set your thermostat to the optimal temperature (75◦F in\n",
            "this case) during hot, humid days...” with a clear rationale. It\n",
            "also offers customized suggestions based on user conditions,\n",
            "along with valuable energy efﬁciency tips such as “monitor\n",
            "your energy consumption”, and “adapt to changing condi-\n",
            "tions”. Nevertheless, the cost of simplicity is the inability\n",
            "to perform sophisticated controls like pre-heating/cooling,\n",
            "which entails heating the room in anticipation of occupancy\n",
            "and can only be accounted for through a multiperiod formu-\n",
            "lation involving intricate room thermal dynamics [4].\n",
            "3) Battery charging control: This problem involves opti-\n",
            "mizing the charging and discharging cycles of a home battery\n",
            "system, considering factors such as electricity pricing, solar\n",
            "generation, and household demand, as seen in competitions\n",
            "like the CityLearn Challenge [22]. As discussed in [23],\n",
            "selecting the appropriate optimization problem for a given\n",
            "context is crucial. Unlike [23], where the context is derived\n",
            "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\n",
            "260\n",
            "Authorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \n",
            "from a reward signal, our context is provided by the user in\n",
            "natural language. The EC interprets this context accurately\n",
            "to establish the right optimization problem.\n",
            "EC successfully formulates an accurate objective that com-\n",
            "putes the weighted sum of total electricity cost, considering\n",
            "the electricity price. It also correctly formulates the lower\n",
            "and upper limits for the charging and discharging variables,\n",
            "the temporal interdependence of the state of charge and\n",
            "the charging rate, and the maximum capacity constraint.\n",
            "The energy balance constraint identiﬁed demonstrates LLM’s\n",
            "understanding of world modeling, including this constraint\n",
            "without speciﬁc user instruction. EC’s explanation further\n",
            "shows an understanding of physical constraints and its over-\n",
            "all objective. However, EC implicitly assumes that excess\n",
            "energy produced each hour can be sold back to the grid at\n",
            "the same rate as it was purchased, which is not typically\n",
            "the case. Future work can explore methods to encourage\n",
            "EC to be more explicit about assumptions when constructing\n",
            "optimization instances.\n",
            "B. Long-term planning for sustainability\n",
            "1) Cost-beneﬁt analysis of installing rooftop solar PVs:\n",
            "To perform a cost-beneﬁt analysis of solar PV, one must\n",
            "estimate the costs and beneﬁts over the system’s lifespan\n",
            "and compare them to a relevant alternative system [24].\n",
            "EC shows proﬁciency in understanding physical con-\n",
            "straints, such as the stipulation that the panel area shouldn’t\n",
            "surpass the roof area, and proposes that the installed area\n",
            "be adequate to supply the electricity demand. However, it\n",
            "operates under a few assumptions, such as the user planning\n",
            "to source all electricity demand from PV to fully leverage\n",
            "the budget, and that the PV’s efﬁciency stands at 0.12. While\n",
            "these assumptions are generally sensible, stating them in\n",
            "the explanations would make the model’s workings more\n",
            "transparent. Additionally, the model seems to underutilize the\n",
            "user’s provided information, such as the building’s location,\n",
            "which could potentially inform the required area-to-power\n",
            "conversion efﬁciency more accurately. This is understandable\n",
            "given the model’s lack of access to an external database,\n",
            "and future work that enhances this capability could be a\n",
            "worthwhile pursuit.\n",
            "2) Cost-beneﬁt analysis of installing a heat pump: Key\n",
            "considerations in the cost-beneﬁt analysis of a heat pump\n",
            "include the initial cost of installation, which can vary based\n",
            "on the type, size, and complexity of the system, as well as\n",
            "the availability of incentives and rebates [25].\n",
            "Similar to the PV installation scenario in Sec. III-B.1,\n",
            "EC proﬁciently identiﬁes pivotal relationships, including the\n",
            "annual operating cost of a central AC and a heat pump.\n",
            "EC’s approach focuses on optimizing annual savings, leaving\n",
            "the actual estimation of the payback period to the user.\n",
            "For instance, EC calculates that the annual savings with a\n",
            "heat pump equals $550, then elaborates on how to use this\n",
            "data for investment decisions, suggesting “If the purchase\n",
            "and installation of the heat pump cost $5,000, the payback\n",
            "period would equate to around 9.1 years ($5,000 / $550).”\n",
            "EV Charging\n",
            "Battery Charging Battery Capacity\n",
            "Solar Panel\n",
            "HVAC\n",
            "Heat Pump\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "Compilation\n",
            "Correctness\n",
            "Explanation\n",
            "Problem category\n",
            "Success rate (%)\n",
            "Fig. 4: The efﬁciency of EC when integrated with SCIPY\n",
            "is gauged by the compilation success of the Python code it\n",
            "generates, the solution’s precision, and the lucidity of the\n",
            "explanation presented to the user. The black lines highlight\n",
            "the performance shifts observed when employing CVXPY.\n",
            "Fig. 5: (A) A comparative analysis of the number of itera-\n",
            "tions needed to achieve executable code across all problem\n",
            "categories (note that none of the experimental instances use\n",
            "4 iterations). (B) A comparison of the average number of\n",
            "code generations needed for EC to produce executable code,\n",
            "limited to a maximum of 5. The black bars represent the\n",
            "variation in iterations across problem categories over 20\n",
            "random runs.\n",
            "EC also suggests users compare alternatives, plan long-\n",
            "term strategies, evaluate environmental impact, and seek\n",
            "incentives or rebates. This shows that EC can provide not\n",
            "only a decision but also the context and information nec-\n",
            "essary to empower users to make more cost-effective and\n",
            "environmentally conscious choices.\n",
            "C. General ﬁndings and analysis\n",
            "To elucidate the types of errors EC commits, we display\n",
            "the success rates concerning error-free code production,\n",
            "logical correctness of the code, and the ability to articulate\n",
            "the solution derived from the optimization code to the end\n",
            "user in Fig. 4. Our framework enables EC to achieve a 100%\n",
            "compilation rate. When EC generates an error-free optimiza-\n",
            "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\n",
            "261\n",
            "Authorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \n",
            "tion code, it consistently explains the results to the end\n",
            "user. SCIPY outperforms CVXPY in overall performance,\n",
            "aligning with our observation that SCIPY has broader online\n",
            "recognition and use, where most LLMs receive training.\n",
            "Fig. 5 presents the average number of code regenerations\n",
            "needed and the average debugging iterations required to\n",
            "obtain syntax error-free optimization code. Although each\n",
            "subproblem within the optimization problem category ex-\n",
            "hibits different success rates, a relatively low number of code\n",
            "regenerations are typically needed. When the LLM consis-\n",
            "tently commits errors, incorporating debugging information\n",
            "in our framework is essential for achieving optimization solu-\n",
            "tions. Reducing the number of generated code samples from\n",
            "s = 5 to s = 1 slightly increases the required debugging\n",
            "iterations across all examples, suggesting that debugging is\n",
            "a more efﬁcient error-handling strategy compared to simple\n",
            "regeneration.\n",
            "Estimating the Probability of One-Round Autoformal-\n",
            "ism Success. The aforementioned results allow us to infer\n",
            "the probability of generating valid code successfully, denoted\n",
            "as p. We postulate that the event of successfully generating\n",
            "valid code is independent and identically distributed for the\n",
            "same category of problems, following a truncated geometric\n",
            "distribution with a cap at 5 trials. Based on this supposition,\n",
            "we deduce that P5\n",
            "k=1 k(1−p)k−1p+6 P\n",
            "k≥6(1−p)k−1p =\n",
            "z, where z signiﬁes the number of generations needed to\n",
            "achieve success (with generations capped at 5, as shown in\n",
            "Fig. 5), and p is the probability of success to be estimated.\n",
            "Substituting the values for z from Fig. 5 into the equation and\n",
            "solving for p, we obtain the one-round autoformalism success\n",
            "rates p = 0.8, 0.25, 0.38, 0.53, 0.83, 0.38 for the problems\n",
            "of EV Charging, Battery Charging, Battery Capacity, Solar\n",
            "Panel, HVAC Control, and Heat Pump Investment, respec-\n",
            "tively.\n",
            "Estimation of the Probability of Debugging Success.\n",
            "We posit that the event of successful debugging, given the\n",
            "presence of an erroneous code, is independent and identically\n",
            "distributed across all problem classes. Using Fig. 5, we ﬁrst\n",
            "normalize the frequency of the required number of debugging\n",
            "iterations by the frequency of generating an erroneous code\n",
            "in the ﬁrst run (i.e., 1 −0.7 = 0.3). This reveals that the\n",
            "frequencies of observing debugging iterations 1, 2, 3, 4, and\n",
            "≥5 are represented by yk, where k 2 {1, 2, ..., 5}. Let q\n",
            "denote the probability of successfully debugging the code.\n",
            "From this, we can construct a system of polynomial equa-\n",
            "tions in the form of (1 −q)k−1q = yk for k 2 {1, 2, 3, 4},\n",
            "and 1 −P4\n",
            "k=1(1 −q)k−1q = y5 given that debugging is\n",
            "capped at 5 iterations. Using a line search between 0 and 1\n",
            "in increments of 0.01, we calculate the values of ˆy1 to ˆy5\n",
            "for each q using the speciﬁed equations, and determine the\n",
            "mean squared error (MSE) between the vectors ˆy and y. Our\n",
            "optimal estimate for the probability is q = 0.26.\n",
            "Optimality Gap and Improvement Over Baseline. The\n",
            "optimality gap for a feasible solution is calculated as the ratio\n",
            "v/v⇤−1, where v denotes the objective value of the candidate\n",
            "solution and v⇤signiﬁes the optimal value of the corre-\n",
            "sponding minimization problem. In Fig. 4, we classify test\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "Problem Category\n",
            "EV Charging\n",
            "Battery Charging\n",
            "Battery Capacity\n",
            "Solar Panel\n",
            "HVAC\n",
            "Heat Pump\n",
            "Improvement over Baseline (%)\n",
            "Optimality Gap (%)\n",
            "Fig. 6: We assess the average optimality gap by compar-\n",
            "ing the exact solution of the optimization problem with\n",
            "the solution determined by EC, alongside evaluating the\n",
            "enhancement over the baseline in terms of optimality.\n",
            "cases as correct only if they equate to the globally optimal\n",
            "solution value, i.e., a 0% optimality gap. Additional results\n",
            "presented in Fig. 6 demonstrate the average optimality gap\n",
            "when incorrect logic within the code results in the omission\n",
            "of key parameters (such as the efﬁciency of a battery). Our\n",
            "framework was evaluated using 20 examples for each energy\n",
            "optimization problem, including EV charging. As depicted,\n",
            "the instances display an optimality gap generally within the\n",
            "20% range, with the majority under 10%.\n",
            "For the baseline method, the model was simply prompted\n",
            "with our question along with necessary parameters to solve\n",
            "the optimization problem. We noticed that even when the\n",
            "problem is clearly an optimization issue, LLMs may opt\n",
            "to generate responses by attempting to apply logic towards\n",
            "reaching an answer. As seen in Fig. 6, this approach does not\n",
            "yield favorable results in comparison to the EC framework.\n",
            "This can be attributed to the fact that many energy-related\n",
            "optimization problems of importance do not yield closed-\n",
            "form solutions, making the correct utilization of convex\n",
            "program solvers critical.\n",
            "We further report on the improvement over the baseline,\n",
            "measured by vb/v −1, where vb represents the objective\n",
            "value of the baseline solution. As is evident, the improvement\n",
            "can amount to as much as 60%, with most instances falling\n",
            "within the 30% range. Some of the problems yielding the\n",
            "greatest improvements include Solar Panel, Heat Pump,\n",
            "Battery Charging, and EV Charging, which encompass both\n",
            "real-time decisions and long-term investments.\n",
            "IV. DISCUSSIONS AND CONCLUSION\n",
            "Conversational AI for sustainability has long seemed\n",
            "aspirational, but LLMs offer new potential. By providing\n",
            "an intuitive interface, our framework helps individuals en-\n",
            "gage more consciously about energy use. Greater awareness\n",
            "drives sustainable behaviors like responsible consumption,\n",
            "efﬁciency, and solar adoption [26], [27].\n",
            "Households average 3.5 kWh peak electricity use, while\n",
            "EV chargers approach 10 kWh. EV charging at peak triples\n",
            "load, straining infrastructure. Utilities offer off-peak rates,\n",
            "but ﬁnancial incentives alone don’t fully shift behaviors—\n",
            "an “efﬁciency gap” phenomenon [26], [27]. Our framework\n",
            "simpliﬁes this via conversation. With 2022’s 3 million EVs,\n",
            "adopting off-peak charging could save $876 million annually\n",
            "(at $0.06 vs $0.14 per kWh rates) [28], [29]. With 2035’s\n",
            "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\n",
            "262\n",
            "Authorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \n",
            "projected 73 million EVs [30], savings could reach $21.3\n",
            "billion. Beyond consumer savings, off-peak charging lessens\n",
            "grid stress, stabilizes prices, and reduces needs for new\n",
            "plants. By simplifying optimization, our system enables\n",
            "impactful sustainability actions.\n",
            "A key limitation of EC is its reliance on the LLM’s\n",
            "ability to accurately formulate optimization problems, ensur-\n",
            "ing proper alignment of objectives and constraints. Incorrect\n",
            "formulations can lead to ineffective or harmful solutions,\n",
            "despite the perceived reliability of these systems. Auto-\n",
            "informalism (Sec. II-A.3) helps mitigate this risk by scru-\n",
            "tinizing problem formulations more carefully and alerting\n",
            "users to potential issues or assumptions. Enhancing auto-\n",
            "informalism’s effectiveness is thus critical for reliable solu-\n",
            "tions. Additional reliability methods like validation routines,\n",
            "robustness testing, and expanded user feedback loops can\n",
            "further strengthen the framework.\n",
            "While we merely sketch out the potential, the pro-\n",
            "posed shift towards human-guided optimization autoformal-\n",
            "ism could democratize access to sophisticated technologies\n",
            "and set the stage for a more equitable and sustainable future.\n",
            "REFERENCES\n",
            "[1] S. M. Arif, T. T. Lie, B. C. Seet, S. Ayyadi, and K. Jensen, “Review\n",
            "of electric vehicle technologies, charging methods, standards and\n",
            "optimization techniques,” Electronics, vol. 10, no. 16, p. 1910, 2021.\n",
            "[2] R. Khezri, A. Mahmoudi, and H. Aki, “Optimal planning of solar\n",
            "photovoltaic and battery storage systems for grid-connected residential\n",
            "sector: Review, challenges and new perspectives,” Renewable and\n",
            "Sustainable Energy Reviews, vol. 153, p. 111763, 2022.\n",
            "[3] Y. Yang, S. Bremner, C. Menictas, and M. Kay, “Battery energy\n",
            "storage system size determination in renewable energy systems: A\n",
            "review,” Renewable and Sustainable Energy Reviews, vol. 91, pp. 109–\n",
            "125, 2018.\n",
            "[4] J. Drgoˇna, J. Arroyo, I. C. Figueroa, D. Blum, K. Arendt, D. Kim,\n",
            "E. P. Oll´e, J. Oravec, M. Wetter, D. L. Vrabie et al., “All you need to\n",
            "know about model predictive control for buildings,” Annual Reviews\n",
            "in Control, vol. 50, pp. 190–232, 2020.\n",
            "[5] B. P. Esther and K. S. Kumar, “A survey on residential demand\n",
            "side management architecture, approaches, optimization models and\n",
            "methods,” Renewable and Sustainable Energy Reviews, vol. 59, pp.\n",
            "342–351, 2016.\n",
            "[6] J. Currie, D. I. Wilson, N. Sahinidis, and J. Pinto, “Opti: Lowering the\n",
            "barrier between open source optimizers and the industrial matlab user,”\n",
            "Foundations of computer-aided process operations, vol. 24, p. 32,\n",
            "2012.\n",
            "[7] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\n",
            "J. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling language\n",
            "models: Methods, analysis & insights from training gopher,” arXiv\n",
            "preprint arXiv:2112.11446, 2021.\n",
            "[8] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V.\n",
            "Le, D. Zhou et al., “Chain-of-thought prompting elicits reasoning in\n",
            "large language models,” in Advances in Neural Information Processing\n",
            "Systems, 2022.\n",
            "[9] B. Sel, A. Al-Tawaha, v Vanshaj, R. Jia, and M. Jin, “Algorithm of\n",
            "thoughts: Enhancing exploration of ideas in large language models,”\n",
            "ICML, 2024.\n",
            "[10] Y. Wu, A. Q. Jiang, W. Li, M. N. Rabe, C. E. Staats, M. Jamnik,\n",
            "and C. Szegedy, “Autoformalization with large language models,” in\n",
            "Advances in Neural Information Processing Systems, 2022.\n",
            "[11] Y. Wu, A. Q. Jiang, W. Li, M. Rabe, C. Staats, M. Jamnik, and\n",
            "C. Szegedy, “Autoformalization with large language models,” Ad-\n",
            "vances in Neural Information Processing Systems, vol. 35, pp. 32 353–\n",
            "32 368, 2022.\n",
            "[12] A. Agrawal, R. Verschueren, S. Diamond, and S. Boyd, “A rewriting\n",
            "system for convex optimization problems,” Journal of Control and\n",
            "Decision, vol. 5, no. 1, pp. 42–60, 2018.\n",
            "[13] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\n",
            "E. Jiang, C. Cai, M. Terry, Q. Le et al., “Program synthesis with large\n",
            "language models,” arXiv preprint arXiv:2108.07732, 2021.\n",
            "[14] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V.\n",
            "Le, D. Zhou et al., “Chain-of-thought prompting elicits reasoning in\n",
            "large language models,” Advances in Neural Information Processing\n",
            "Systems, vol. 35, pp. 24 824–24 837, 2022.\n",
            "[15] G. Mialon, R. Dess`ı, M. Lomeli, C. Nalmpantis, R. Pasunuru,\n",
            "R. Raileanu, B. Rozi`ere, T. Schick, J. Dwivedi-Yu, A. Celikyil-\n",
            "maz et al., “Augmented language models: a survey,” arXiv preprint\n",
            "arXiv:2302.07842, 2023.\n",
            "[16] T. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama,\n",
            "“Demystifying gpt self-repair for code generation,” arXiv preprint\n",
            "arXiv:2306.09896, 2023.\n",
            "[17] A. B. Saka, L. O. Oyedele, L. A. Akanbi, S. A. Ganiyu, D. W.\n",
            "Chan, and S. A. Bello, “Conversational artiﬁcial intelligence in the\n",
            "aec industry: A review of present status, challenges and opportunities,”\n",
            "Advanced Engineering Informatics, vol. 55, p. 101869, 2023.\n",
            "[18] R. Panchalingam and K. C. Chan, “A state-of-the-art review on\n",
            "artiﬁcial intelligence for smart buildings,” Intelligent Buildings Inter-\n",
            "national, vol. 13, no. 4, pp. 203–226, 2021.\n",
            "[19] M. Jin, V. Khattar, H. Kaushik, B. Sel, and R. Jia, “On solution func-\n",
            "tions of optimization: Universal approximation and covering number\n",
            "bounds,” AAAI, 2023.\n",
            "[20] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, “Summarizing\n",
            "source code using a neural attention model,” in Proceedings of the\n",
            "54th Annual Meeting of the Association for Computational Linguistics\n",
            "(Volume 1: Long Papers), 2016, pp. 2073–2083.\n",
            "[21] M. Jin, B. Sel, H. FNU, and W. Yin, “Democratizing energy manage-\n",
            "ment with llm-assisted optimization autoformalism,” 2024, full Ver-\n",
            "sion:\n",
            "http://www.jinming.tech/papers/ChatEnergy23-smartgridcomm.\n",
            "pdf.\n",
            "[22] J. R. V´azquez-Canteli, J. K¨ampf, G. Henze, and Z. Nagy, “Citylearn\n",
            "v1.0: An openai gym environment for demand response with deep\n",
            "reinforcement learning,” in Proceedings of the 6th ACM International\n",
            "Conference on Systems for Energy-Efﬁcient Buildings, Cities, and\n",
            "Transportation, ser. BuildSys ’19.\n",
            "New York, NY, USA: Association\n",
            "for Computing Machinery, 2019, p. 356–357. [Online]. Available:\n",
            "https://doi.org/10.1145/3360322.3360998\n",
            "[23] V. Khattar and M. Jin, “Winning the citylearn challenge: Adaptive opti-\n",
            "mization with evolutionary search under trajectory-based guidance,” in\n",
            "Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 37,\n",
            "no. 12, 2023, pp. 14 286–14 294.\n",
            "[24] M. Thebault and L. Gaillard, “Optimization of the integration of\n",
            "photovoltaic systems on buildings for self-consumption–case study in\n",
            "france,” City and Environment Interactions, vol. 10, p. 100057, 2021.\n",
            "[25] F. Bela¨ıd, Z. Ranjbar, and C. Massi´e, “Exploring the cost-effectiveness\n",
            "of energy efﬁciency implementation measures in the residential sector,”\n",
            "Energy Policy, vol. 150, p. 112122, 2021.\n",
            "[26] O. I. Asensio and M. A. Delmas, “Nonprice incentives and energy\n",
            "conservation,” Proceedings of the National Academy of Sciences, vol.\n",
            "112, no. 6, pp. E510–E515, 2015.\n",
            "[27] T. D. Gerarden, R. G. Newell, and R. N. Stavins, “Assessing the\n",
            "energy-efﬁciency gap,” Journal of economic literature, vol. 55, no. 4,\n",
            "pp. 1486–1525, 2017.\n",
            "[28] APPALACHIAN\n",
            "POWER,\n",
            "“Virginia\n",
            "s.c.c.\n",
            "tariff\n",
            "no.\n",
            "26\n",
            "appalachian\n",
            "power\n",
            "company,”\n",
            "2021.\n",
            "[Online].\n",
            "Available: https://www.appalachianpower.com/lib/docs/ratesandtariffs/\n",
            "Virginia/Tariff26-MASTER-Standard-June1-2023RPS-RAC.pdf\n",
            "[29] IEA,\n",
            "“Trends\n",
            "in\n",
            "electric\n",
            "light-duty\n",
            "vehicles,”\n",
            "2023.\n",
            "[On-\n",
            "line]. Available: https://www.iea.org/reports/global-ev-outlook-2023/\n",
            "trends-in-electric-light-duty-vehicles\n",
            "[30] NREL, “National economic value assessment of plug-in electric\n",
            "vehicles,”\n",
            "2016.\n",
            "[Online].\n",
            "Available:\n",
            "https://www.nrel.gov/docs/\n",
            "fy17osti/66980.pdf\n",
            "[31] H. C. Hesse, R. Martins, P. Musilek, M. Naumann, C. N. Truong, and\n",
            "A. Jossen, “Economic optimization of component sizing for residential\n",
            "battery storage systems,” Energies, vol. 10, no. 7, p. 835, 2017.\n",
            "2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)\n",
            "263\n",
            "Authorized licensed use limited to: TU Wien Bibliothek. Downloaded on November 12,2024 at 18:17:04 UTC from IEEE Xplore.  Restrictions apply. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF\n",
        "import os\n",
        "import glob\n",
        "import fitz\n",
        "\n",
        "#TODO:  Function to extract text from a PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    print(\"\")\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = ''\n",
        "    for page in doc:\n",
        "      text = text + page.get_text('text')\n",
        "    return text\n",
        "\n",
        "# Extract text from all uploaded PDF files\n",
        "pdf_texts = {}\n",
        "\n",
        "for i in range(10):\n",
        "  name = 'paper' + str(i) +'.pdf'\n",
        "  text = extract_text_from_pdf(name)\n",
        "  pdf_texts[name] = text\n",
        "\n",
        "\n",
        "#Display the text from all the PDF files\n",
        "for pdf_file, text in pdf_texts.items():\n",
        "    print(text) #implement PDF read"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zauB9H_UmreF"
      },
      "source": [
        "### Creating an index of vectors to represent the documents\n",
        "\n",
        "To perform efficient searches, we need to convert our text data into numerical vectors. To do so, we will use the first step of the BERT transformer.\n",
        "\n",
        "Since our full pdf files are very long to be fed as input into BERT, we perform a step in which we create a structure where we associate a document number to its abstract, and in a separate dictionary we associate a document number to its full text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xuZVO8ECmreF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "3861734087fb448896b2175e84b78e7a",
            "e225a715b42e467e94b93f55a2437629",
            "8a7c19ae202d4fa9a8ffff85a12a3ce4",
            "cad8f78894a24caebe31e28d6323eb75",
            "f93cecca786d41f5b589dfef53c06b61",
            "0871c186292b413382d8da12d02a65bc",
            "0ec8d04b742f46cda426ae0f73bd7aed",
            "f879860b696c422fb29e3d819a21d914",
            "be673739575c47f49b706227b3a4fed3",
            "5deffddd9f314048a3278a1b2434164a",
            "17140ef0064e4f47939823c68d6880c9",
            "89958f0cbc9b40ec9a2ac86efbfe9937",
            "5bb8d56c99404796a6603a11229b1516",
            "edb259ea68a946d5a4fb4985270a5f83",
            "47016b4c176a440e963207c3c3cc64bc",
            "424216a1ec8e4913b54c7968ae1650bd",
            "b5e45d392b0e4c2cae5179a5d5119ff4",
            "1318b39a70394fa8bffe339f0f6b7dd7",
            "0a5549b0a0a343b0b01fb0aada15a803",
            "926e6369c12843aab0d83074e8bd8d43",
            "e587f1e49e7c4ab48edf61a48c5afa52",
            "71b98e808c3b42c7945a3861953f8804",
            "ce6e261701e44fae9c3d36d27a24f80c",
            "786e433a4fb9482caa36a2a4cfd3393f",
            "e6274345fafe46f9ae4728ead006a08f",
            "d98beaad83cf45beae888f6d0f7ab770",
            "07bde6e5bc89410faf0dc1a60b71d3a6",
            "146e2db7dfbe4de0b3ae18b231440f4c",
            "b62b9b59bfd242b9898a0ee8a12b3b2f",
            "be52c1f9ec73457b88f69cb94bdef7a7",
            "382d17edf7d844a6a3c8e33bf551da8b",
            "9ea180df2b3c46ecb097161bf24593be",
            "c87c1c80636f49899d3b19ab969c268d",
            "c776fec81c734574b446e200093d26cb",
            "d6f3c4c952c54903be2e639b6589a131",
            "4cd2e35537974e24a3c1e660a551ecc8",
            "200f6e3a7eca477ba8f920de1199be00",
            "b511a528684243bc8bf72fa3f1b7d729",
            "8bc554814ed84d8ba889cb59ce665c77",
            "b92179dcf5344bf7b6aec66c2e79d240",
            "ecf48173c0b3462eb0bb4a9f3cf08852",
            "61a7c1e07f49426d8f0a8d639d62a9b2",
            "f4f4c75a1d144efe9223e53e525a2a41",
            "e7f34261b282425895c1b3068a37c077",
            "6c65d4e13efa4f8d8245c5e368af304b",
            "c9a947dec20d419ea5f83ab5097744d9",
            "f7f7a4b0a7b54274a876d5bd896f1cd0",
            "da818cbc1a9a44dca9399a5b33aec61a",
            "08884f78f7ec434eba852b3df76cadef",
            "cc0a5ab5ddb64a1ba83936c35cb5a54b",
            "853277b4dc46474a9ce09cc7db911724",
            "5ec0308cb02f4986b7d10d087d75c25b",
            "44b3ce56e8ee497bb5a015e2518a519d",
            "0ca091e54ff44b3fbc3095b389c32aa9",
            "bbde1be02b4a4a6988d3689fa77d6d7e"
          ]
        },
        "outputId": "7b35295a-7b1c-4f54-a8bb-afefbb0de88f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3861734087fb448896b2175e84b78e7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89958f0cbc9b40ec9a2ac86efbfe9937"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce6e261701e44fae9c3d36d27a24f80c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c776fec81c734574b446e200093d26cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c65d4e13efa4f8d8245c5e368af304b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#import the Bert pretrained model from the transformers library\n",
        "model_bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "#initialization of the dictionary of abstracts. Substitute this with the abstracts of the 10 papers considered as sources for RAG\n",
        "#(we could use functions to read the PDFs to \"cut\" the abstracts from the papers. For simplicity reasons, we will copy and paste them)\n",
        "abstracts_dict = {\n",
        "    0: \"\"\"Large Language Models (LLMs) are undergoing a period of rapid updates and changes, with stateof-\n",
        "the-art (SOTA) model frequently being replaced. When applying LLMs to a specific scientific\n",
        "field, it’s challenging to acquire unique domain knowledge while keeping the model itself advanced.\n",
        "To address this challenge, a sophisticated large language model system named as Xiwu has been\n",
        "developed, allowing you switch between the most advanced foundation models and quickly teach the\n",
        "model domain knowledge. In this work, we will report on the best practices for applying LLMs in the\n",
        "field of high-energy physics (HEP), including: a seed fission technology is proposed and some data\n",
        "collection and cleaning tools are developed to quickly obtain domain AI-Ready dataset; a just-in-time\n",
        "learning system is implemented based on the vector store technology; an on-the-fly fine-tuning system\n",
        "has been developed to facilitate rapid training under a specified foundation model.\n",
        "The results show that Xiwu can smoothly switch between foundation models such as LLaMA, Vicuna,\n",
        "ChatGLM and Grok-1. The trained Xiwu model is significantly outperformed the benchmark model\n",
        "on the HEP knowledge Q&A and code generation. This strategy significantly enhances the potential\n",
        "for growth of our model’s performance, with the hope of surpassing GPT-4 as it evolves with the\n",
        "development of open-source models. This work provides a customized LLM for the field of HEP,\n",
        "while also offering references for applying LLM to other fields, the corresponding codes are available\n",
        "on Github https://github.comzhang/zhengde0225/Xiwu\"\"\",\n",
        "    1: \"\"\"With the ubiquitous use of modern large language\n",
        "models (LLMs) across industries, the inference serving for these\n",
        "models is ever expanding. Given the high compute and memory\n",
        "requirements of modern LLMs, more and more top-of-theline\n",
        "GPUs are being deployed to serve these models. Energy\n",
        "availability has come to the forefront as the biggest challenge for\n",
        "data center expansion to serve these models. In this paper, we\n",
        "present the trade-offs brought up by making energy efficiency\n",
        "the primary goal of LLM serving under performance SLOs.\n",
        "We show that depending on the inputs, the model, and the\n",
        "service-level agreements, there are several knobs available to\n",
        "the LLM inference provider to use for being energy efficient.\n",
        "We characterize the impact of these knobs on the latency,\n",
        "throughput, as well as the energy. By exploring these tradeoffs,\n",
        "we offer valuable insights into optimizing energy usage\n",
        "without compromising on performance, thereby paving the way\n",
        "for sustainable and cost-effective LLM deployment in data center\n",
        "environments. \"\"\",\n",
        "    2: \"\"\"The rapid adoption of large language models (LLMs) has led to\n",
        "significant advances in natural language processing and text generation.\n",
        "However, the energy consumed through LLM model inference\n",
        "remains a major challenge for sustainable AI deployment. To\n",
        "address this problem, we model the workload-dependent energy\n",
        "consumption and runtime of LLM inference tasks on heterogeneous\n",
        "GPU-CPU systems. By conducting an extensive characterization\n",
        "study of several state-of-the-art LLMs and analyzing their energy\n",
        "and runtime behavior across different magnitudes of input prompts\n",
        "and output text, we develop accurate (𝑅2 > 0.96) energy and runtime\n",
        "models for each LLM. We employ these models to explore\n",
        "an offline, energy-optimal LLM workload scheduling framework.\n",
        "Through a case study, we demonstrate the advantages of energy\n",
        "and accuracy aware scheduling compared to existing best practices.\"\"\",\n",
        "    3: \"\"\"The growing demand for e\\x0ecient and scalable AI solutions has driven research into optimizing the performance and energy\n",
        "e\\x0eciency of computational infrastructures. The novel concept of redesigning inference clusters and modifying the GPT-Neo\n",
        "model o\\x0eers a signi\\x0bcant advancement in addressing the computational and environmental challenges associated with AI\n",
        "deployment. By developing a novel cluster architecture and implementing strategic architectural and algorithmic changes,\n",
        "the research achieved substantial improvements in throughput, latency, and energy consumption. The integration of advanced\n",
        "interconnect technologies, high-bandwidth memory modules, and energy-e\\x0ecient power management techniques, alongside\n",
        "software optimizations, enabled the redesigned clusters to outperform baseline models signi\\x0bcantly. Empirical evaluations\n",
        "demonstrated superior scalability, robustness, and environmental sustainability, emphasizing the potential for more sustainable\n",
        "AI technologies. The \\x0bndings underscore the importance of balancing performance with energy e\\x0eciency and provide a robust\n",
        "framework for future research and development in AI optimization. The research contributes valuable insights into the design\n",
        "and deployment of more e\\x0ecient and environmentally responsible AI systems.\"\"\",\n",
        "    4: \"\"\"Establishing building energy models (BEMs) for building design and analysis poses significant challenges due to\n",
        "demanding modeling efforts, expertise to use simulation software, and building science knowledge in practice.\n",
        "These make building modeling labor-intensive, hindering its widespread adoptions in building development.\n",
        "Therefore, to overcome these challenges in building modeling with enhanced automation in modeling practice,\n",
        "this paper proposes Eplus-LLM (EnergyPlus-Large Language Model) as the auto-building modeling platform,\n",
        "building on a fine-tuned large language model (LLM) to directly translate natural language description of\n",
        "buildings to established building models of various geometries, occupancy scenarios, and equipment loads.\n",
        "Through fine-tuning, the LLM (i.e., T5) is customized to digest natural language and simulation demands from\n",
        "users and convert human descriptions into EnergyPlus modeling files. Then, the Eplus-LLM platform realizes the\n",
        "automated building modeling through invoking the API of simulation software (i.e., the EnergyPlus engine) to\n",
        "simulate the auto-generated model files and output simulation results of interest. The validation process,\n",
        "involving four different types of prompts, demonstrates that Eplus-LLM reduces over 95% modeling efforts and\n",
        "achieves 100% accuracy in establishing BEMs while being robust to interference in usage, including but not\n",
        "limited to different tones, misspells, omissions, and redundancies. Overall, this research serves as the pioneering\n",
        "effort to customize LLM for auto-modeling purpose (directly build-up building models from natural language),\n",
        "aiming to provide a user-friendly human-AI interface that significantly reduces building modeling efforts. This\n",
        "work also further facilitates large-scale building model efforts, e.g., urban building energy modeling (UBEM), in\n",
        "modeling practice.\"\"\",\n",
        "    5: \"\"\"The rapid evolution and widespread adoption of\n",
        "generative large language models (LLMs) have made them a\n",
        "pivotal workload in various applications. Today, LLM inference\n",
        "clusters receive a large number of queries with strict Service\n",
        "Level Objectives (SLOs). To achieve the desired performance,\n",
        "these models execute on power-hungry GPUs causing the inference\n",
        "clusters to consume large amount of energy and, consequently,\n",
        "result in excessive carbon emissions. Fortunately, we find\n",
        "that there is a great opportunity to exploit the heterogeneity in\n",
        "inference compute properties and fluctuations in inference workloads,\n",
        "to significantly improve energy-efficiency. However, such\n",
        "a diverse and dynamic environment creates a large search-space\n",
        "where different system configurations (e.g., number of instances,\n",
        "model parallelism, and GPU frequency) translate into different\n",
        "energy-performance trade-offs. To address these challenges, we\n",
        "propose DynamoLLM, the first energy-management framework\n",
        "for LLM inference environments. DynamoLLM automatically\n",
        "and dynamically reconfigures the inference cluster to optimize for\n",
        "energy and cost of LLM serving under the service’s performance\n",
        "SLOs. We show that at a service-level, DynamoLLM conserves\n",
        "53% energy and 38% operational carbon emissions, and reduces\n",
        "61% cost to the customer, while meeting the latency SLOs.\"\"\",\n",
        "    6: \"\"\"Large language model (LLM) has recently been\n",
        "considered a promising technique for many fields. This work\n",
        "explores LLM-based wireless network optimization via in-context\n",
        "learning. To showcase the potential of LLM technologies, we\n",
        "consider the base station (BS) power control as a case study,\n",
        "a fundamental but crucial technique that is widely investigated\n",
        "in wireless networks. Different from existing machine learning\n",
        "(ML) methods, our proposed in-context learning algorithm relies\n",
        "on LLM’s inference capabilities. It avoids the complexity of\n",
        "tedious model training and hyper-parameter fine-tuning, which is\n",
        "a well-known bottleneck of many ML algorithms. Specifically, the\n",
        "proposed algorithm first describes the target task via formatted\n",
        "natural language, and then designs the in-context learning\n",
        "framework and demonstration examples. After that, it considers\n",
        "two cases, namely discrete-state and continuous-state problems,\n",
        "and proposes state-based and ranking-based methods to select\n",
        "appropriate examples for these two cases, respectively. Finally, the\n",
        "simulations demonstrate that the proposed algorithm can achieve\n",
        "comparable performance as conventional deep reinforcement\n",
        "learning (DRL) techniques without dedicated model training or\n",
        "fine-tuning. Such an efficient and low-complexity approach has\n",
        "great potential for future wireless network optimization.\n",
        "Index Terms—Large language model, in-context learning, network\n",
        "optimization, transmission power control.\"\"\",\n",
        "    7: \"\"\"Both the training and use of Large Language Models (LLMs) require\n",
        "large amounts of energy. Their increasing popularity, therefore,\n",
        "raises critical concerns regarding the energy efficiency and sustainability\n",
        "of data centers that host them. This paper addresses the\n",
        "challenge of reducing energy consumption in data centers running\n",
        "LLMs.We propose a hybrid data center model that uses a cost-based\n",
        "scheduling framework to dynamically allocate LLM tasks across\n",
        "hardware accelerators that differ in their energy efficiencies and\n",
        "computational capabilities. Specifically, our workload-aware strategy\n",
        "determines whether tasks are processed on energy-efficient\n",
        "processors or high-performance GPUs based on the number of input\n",
        "and output tokens in a query. Our analysis of a representative\n",
        "LLM dataset, finds that this hybrid strategy can reduce CPU+GPU\n",
        "energy consumption by 7.5% compared to a workload-unaware\n",
        "baseline.\"\"\",\n",
        "    8: \"\"\"Reproducible science requires easy access to data, especially with\n",
        "the rise of data-driven and increasingly complex models used within\n",
        "energy research. Too often however, the data to reconstruct and\n",
        "verify purported solutions in publications is hidden due to some\n",
        "combination of commercial, legal, and sensitivity issues. This early\n",
        "work presents our initial efforts to leverage the recent advancements\n",
        "in Large Language Models (LLMs) to create usable and shareable\n",
        "energy datasets. In particular, we’re utilising their mimicry of\n",
        "human behaviors, with the goal of extracting and exploring synthetic\n",
        "energy data through the simulation of LLM agents capable of\n",
        "interacting with and executing actions in controlled environments.\n",
        "We also analyse and visualise publicly available data in an attempt\n",
        "to create realistic but not quite exact copies of the originals. Our\n",
        "early results show some promise, with outputs that resemble the\n",
        "twin peak curves for household energy consumption. The hope is\n",
        "that our generalised approach can be used to easily replicate usable\n",
        "and realistic copies of otherwise secret or sensitive data.\"\"\",\n",
        "    9: \"\"\"This paper introduces a method for personalizing\n",
        "energy optimization using large language models (LLMs)\n",
        "combined with an optimization solver. This approach, termed\n",
        "human-guided optimization autoformalism, translates natural\n",
        "language specifications into optimization problems, enabling\n",
        "LLMs to handle various user-specific energy-related tasks. It\n",
        "allows for nuanced understanding and nonlinear reasoning\n",
        "tailored to individual preferences. The research covers common\n",
        "energy sector tasks like electric vehicle charging, HVAC control,\n",
        "and long-term planning for renewable energy installations. This\n",
        "novel strategy represents a significant advancement in contextbased\n",
        "optimization using LLMs, facilitating sustainable energy\n",
        "practices customized to individual needs.\"\"\"\n",
        "}\n",
        "\n",
        "#the text for rag is used as an input to the BERT model\n",
        "\n",
        "#The tokenized inputs are passed to the BERT model for processing.\n",
        "#(#remember padding=True: Ensures that all inputs are padded to the same length, allowing batch processing.)\n",
        "#The model outputs a tensor (last_hidden_state), where each input token is represented by a high-dimensional vector.\n",
        "#last_hidden_state is of shape (batch_size, sequence_length, hidden_size), where:\n",
        "#batch_size: Number of input texts.\n",
        "#sequence_length: Length of each tokenized text (after padding).\n",
        "#hidden_size: Dimensionality of the vector representation for each token (default 768 for bert-base-uncased).\n",
        "\n",
        "#last_hidden_state[:, 0]: Selects the representation of the [CLS] token for each input text. The [CLS] token is a special token added at the start of each input and is often used as the aggregate representation for the entire sequence.\n",
        "\n",
        "\n",
        "input = tokenizer_bert(list(abstracts_dict.values()), padding=True, return_tensors='pt')\n",
        "\n",
        "output = model_bert(**input)\n",
        "\n",
        "abstract_vectors = output.last_hidden_state[:,0]\n",
        "\n",
        "#abstract_vectors is a tensor of shape (batch_size, hidden_size) (e.g., (3, 768) in this case), representing each text as a single 768-dimensional vector.\n",
        "\n",
        "#print(abstract_vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dagYbH3nmreF"
      },
      "source": [
        "### Search\n",
        "\n",
        "With our text data vectorized and indexed, we can now perform searches. We will define a function to search the index for the most relevant documents based on a query.\n",
        "\n",
        "To perform the search, we need a function (search documents) where we perform the cosine similarity between the query vector and all the abstract vectors. This function will give our the top-k indexes. Once we find the top-k indexes, with another function, we can collect the full text of the documents from the paper dictionary.\n",
        "\n",
        "To compute cosine similarity, refer to the following formula\n",
        "\n",
        "```cs = cosine_similarity(vector_a.detach().numpy(), vector_b.detach().numpy())```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zaADea87mreF"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_top_k_similar_indices(query_vector, abstract_vectors, k):\n",
        "\n",
        "    #Computes the top k indices of the most similar abstracts to the query based on cosine similarity.\n",
        "\n",
        "    #Parameters:\n",
        "    #- query_vector: A tensor of shape (1, hidden_size) representing the query vector.\n",
        "    #- abstract_vectors: A tensor of shape (batch_size, hidden_size) representing the abstract vectors.\n",
        "    #- k: The number of top indices to return.\n",
        "    tmp_dict = {}\n",
        "    for index, item in enumerate(abstract_vectors):\n",
        "      # Reshape both query_vector and item to 2D arrays for sklearn's cosine_similarity\n",
        "\n",
        "      cs = cosine_similarity(query_vector.detach().numpy().reshape(1, -1), item.detach().numpy().reshape(1, -1))[0][0]\n",
        "      tmp_dict[index] = cs\n",
        "\n",
        "    # Sort the dictionary items by similarity in descending order\n",
        "    sorted_items = sorted(tmp_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Extract only the keys (indices) from the sorted items\n",
        "    key_sorted = [item[0] for item in sorted_items]\n",
        "\n",
        "\n",
        "    #Returns:\n",
        "    #- sorted_indices: A numpy array of shape (1, k) containing the indices of the top k most similar abstracts.\n",
        "\n",
        "    return key_sorted[0:k]\n",
        "\n",
        "\n",
        "def retrieve_documents(indices, documents_dict):\n",
        "\n",
        "    #Retrieves the documents corresponding to the given indices and concatenates them into a single string.\n",
        "\n",
        "    #Parameters:\n",
        "    #- indices: A numpy array or list of top-k indices of the most similar documents.\n",
        "    #- documents_dict: A dictionary where keys are document indices (integers) and values are the document texts (strings).\n",
        "    print(indices)\n",
        "    documents = [documents_dict['paper'+str(i)+'.pdf'] for i in indices]\n",
        "    #Returns:\n",
        "    #- concatenated_documents: A string containing the concatenated texts of the retrieved documents.\n",
        "\n",
        "    return \" \".join(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLhs-6A7mreF"
      },
      "source": [
        "### A function to perform Retrieval Augmented Generation\n",
        "\n",
        "In this step, we’ll combine the context retrieved from our documents with LLAMA to generate responses. The context will provide the necessary information to the model to produce more accurate and relevant answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F042qv3hmreF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#now we put it all together\n",
        "\n",
        "def generate_augmented_response(query, model, tokenizer):\n",
        "#TODO: define system prompt\n",
        "\n",
        "    system = \"\"\"\n",
        "You are an AI assistant specialized in analyzing academic research papers, particularly in telecommunications, machine learning, and network optimization.\n",
        "Your task is to answer user questions based exclusively on the content of the provided documents.\n",
        "\n",
        "**Guidelines:**\n",
        "- Provide clear, concise, and well-structured answers\n",
        "- Support your statements with references to specific sections, equations, or results from the document\n",
        "- Do not invent information not present in the text\n",
        "- If a question is beyond the document's scope, clarify that you lack the relevant information\n",
        "- Use technical yet accessible language\n",
        "\n",
        "**Preferred Response Format:**\n",
        "- Brief introduction\n",
        "- Bullet points for methods or results\n",
        "- References to tables/figures when available\n",
        "- Concise conclusion summarizing key points\n",
        "\n",
        "Context:\n",
        "\"\"\"\n",
        "    query_tokenized = tokenizer_bert(query, return_tensors='pt')\n",
        "    query_embeddings =  model_bert(**query_tokenized).last_hidden_state[:,0]\n",
        "    context = retrieve_documents(get_top_k_similar_indices(query_embeddings, abstract_vectors, 1), pdf_texts)               #TODO: concatenate here all the search results\n",
        "\n",
        "\n",
        "    prompt = system+\"\\n\"+context+\"\\n\"+\"Query:\\n\"+query+\"\\n\"+\"Answer:\"                 #TODO: create the prompt for LLAMA (system + context + query)\n",
        "\n",
        "    input = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
        "\n",
        "    output = model.generate(**input, max_new_tokens=500)\n",
        "\n",
        "    response = tokenizer.decode(output, skip_special_tokens=True)\n",
        "\n",
        "    #perform a query with LLAMA in the usual way\n",
        "\n",
        "    #return the response\n",
        "    return response\n",
        "\n",
        "\n",
        "\n",
        "#TODO: now compare the results with a prompt without RAG. What are the results?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "30b72d8e469544599ff8ef0a0f135fcf",
            "fca4cf0746ef4b57b18a055177aefd2e",
            "e6fdb8e6cb574a55bc1e560feae19a5e",
            "c71678c1736f45c1b42fc30384a31971",
            "110997ca006a468bb0a09c43894ea817",
            "66c78a6a0bfc43059c8c15e4a235276f",
            "eaa7db8978914a54b9c43a24481ce7dd",
            "ddb9654657e74948b4fc0063822706fd",
            "6714dcc37396433c8161021eb6e12dc9",
            "25d1759fa4874909acf4e68d3e1c598a",
            "738de58e906746ceb22d96caf219916f",
            "9d50f1a66d184372964ca5eb2eaa49a5",
            "e59a6b6f2253443881468d90fabb8c65",
            "42fb18575c5e4850828fab661142bf14",
            "2482da37c47a4401bbadd8105ccbd0f9",
            "b3d036d7f73e4429967f732b114090b9",
            "e50fb714dfb841eb973b8b3da4a350e4",
            "9adcdce850a4495bb50671fe1bb1c722",
            "5ccc3bf92ddf4e44b879f6d3b93e0990",
            "1131172bd0aa42728a3d839fcaf31105"
          ]
        },
        "id": "qv1WpwAKREUL",
        "outputId": "da547b34-15a5-4bfc-b3ff-a9b93c0b1220"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30b72d8e469544599ff8ef0a0f135fcf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "queries = \"In this study, how is in-context learning applied to optimize base station transmission power control in wireless networks? Describe the proposed method, including task description design, example selection strategies for both discrete and continuous state problems, and summarize the main performance results compared to traditional deep reinforcement learning approaches.\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226,
          "referenced_widgets": [
            "55a4dc7a53d5479e86e648cfacf992ae",
            "3770506e91604113a39e07e3ef4a57e7",
            "867dabff3cfe487ba0ef2d09b1053efc",
            "35168a0cc7724819995144032be44a73",
            "4bfbfcb9f15a4903ba36fd7702ae1305",
            "312aa0b5085c4fe4a22d7fc1de1eafe6",
            "ff7a30db92a54fefaa552d45e39b06b4",
            "a0d851e37c26433fb98f14d6aafc3e1a",
            "b824acbe675e42768455857082d48dca",
            "9a6f5d2a4d274e8caee84ab8ecc46ea1",
            "c2fbb6503a1e4a42854ef20859b45b46",
            "ed75e63703e6486f8fb44551b583c3bc",
            "7d2060182b394507b15e141aaf5594f3",
            "ca08de95765f4c1ab14ecd36e4d19550",
            "46f94779db0e4fabbe1bb17c3f9f4fb9",
            "3a1ce337c38d457594a2bd6844b99aa3",
            "ef95b6d04ae541c48a9854cb7d2620ba",
            "beaa665bbd2a451f8edc8655696116ad",
            "251d6277e4cb4df5830ebfb2fe88dd9f",
            "68514c9383334aa187bd7139a84b9b9a",
            "8cd2e518682e461b9edd965681d34686",
            "48e8ec0f39994f6e84e9ec9a7371d007",
            "70567b5bbe544ad393a0c1feef83420b",
            "557dded585e043b0b7cebc600e2e476c",
            "4ead470adb4a42d2b4204def1a622650",
            "5fac50ee5bb441b99469a877b6f8be82",
            "13c63e67c78d47a4ba100ac4318e951d",
            "7d727677a0224116aea9ec255d446d44",
            "18568e89f65d4d39a81457d21a62db43",
            "d531c90bb2e8403e9cb70cad79ec8b74",
            "b86900f64ba34e11b3cac0cdfaf916b3",
            "da9984ef4ea04ee3802600dacc0d5635",
            "f73153d0234e40a69ede5badba287c2e",
            "e2bc775a272c45d9a6036fd618874ffa",
            "6e5a2a4761fa496ab8b1bc5e5a037394",
            "fb15c531c23f40b897d0a7d77146c529",
            "7060ccdfb603401daf458b1305a9b283",
            "cd34fa232e004a28bc510524b63e34c7",
            "c3ea72167e924732ac275a81f6cf8581",
            "e3978d297aae4f2cac65e29440f4bb3b",
            "0cf1bf6f1ae741c2b0e64bc81575551d",
            "219a0c36a4754cfea2c360079a100fdb",
            "57f4ad9a04584f7ea6097cac75631534",
            "1dc1d6f59b3b4545879b8626b3dd389f",
            "a38f7dc3daa84a829880235185847b4a",
            "89ac6510c503423b8f85758c5355353c",
            "fbf5550519d549f9a24d71f680500a03",
            "171bba2797034e8faf36b4471a7e3b0a",
            "12061f1afac644a89d2cfd0bf8dfee01",
            "e313316889b24d42a326acd86036410f",
            "56a32268f0184d369db1c080e1038e66",
            "83c7eb12383047a097572d6c32b354b3",
            "77c3b6a1558a45bda218db8c490904f5",
            "74802a698fa844b991ef1b73826d54aa",
            "1da357160e754d9cbe44634a8b922879",
            "f44302c3e1414510934cfaab7ea82c92",
            "f07233bfbb9a4072ad87b748520a9628",
            "3c0ee42a6dc4448489ba929e424dc42d",
            "318df7d888c34559b5ceeda63c58be77",
            "691c8304600546e6b12b61051aca405b",
            "f76e5ddc6ec247d3ac574b8ca0aaf58e",
            "5f1671a9dd7f4b00816c06f34ed35403",
            "2bf57d618b9340c199cd666c9445e610",
            "37d9bb9ffa5e492397900f574f6bb9b4",
            "6cdcecb9a7964027b19896421d048039",
            "f112e3e30072468ba05e1e8394a8a27c"
          ]
        },
        "id": "UeLOANGEO8FG",
        "outputId": "08bee832-c23f-4795-e481-85a8b03126d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55a4dc7a53d5479e86e648cfacf992ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed75e63703e6486f8fb44551b583c3bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70567b5bbe544ad393a0c1feef83420b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2bc775a272c45d9a6036fd618874ffa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a38f7dc3daa84a829880235185847b4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f44302c3e1414510934cfaab7ea82c92"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = generate_augmented_response(queries, model, tokenizer)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "oF0tZw9YSA1c",
        "outputId": "6e9eb67a-d3e9-4832-f2a9-e65d40971a33"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 10.36 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.11 GiB is free. Process 353841 has 13.63 GiB memory in use. Of the allocated memory 13.44 GiB is allocated by PyTorch, and 68.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3170496587.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_augmented_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4244657135.py\u001b[0m in \u001b[0;36mgenerate_augmented_response\u001b[0;34m(query, model, tokenizer)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 459\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 10.36 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.11 GiB is free. Process 353841 has 13.63 GiB memory in use. Of the allocated memory 13.44 GiB is allocated by PyTorch, and 68.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3861734087fb448896b2175e84b78e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e225a715b42e467e94b93f55a2437629",
              "IPY_MODEL_8a7c19ae202d4fa9a8ffff85a12a3ce4",
              "IPY_MODEL_cad8f78894a24caebe31e28d6323eb75"
            ],
            "layout": "IPY_MODEL_f93cecca786d41f5b589dfef53c06b61"
          }
        },
        "e225a715b42e467e94b93f55a2437629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0871c186292b413382d8da12d02a65bc",
            "placeholder": "​",
            "style": "IPY_MODEL_0ec8d04b742f46cda426ae0f73bd7aed",
            "value": "config.json: 100%"
          }
        },
        "8a7c19ae202d4fa9a8ffff85a12a3ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f879860b696c422fb29e3d819a21d914",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be673739575c47f49b706227b3a4fed3",
            "value": 570
          }
        },
        "cad8f78894a24caebe31e28d6323eb75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5deffddd9f314048a3278a1b2434164a",
            "placeholder": "​",
            "style": "IPY_MODEL_17140ef0064e4f47939823c68d6880c9",
            "value": " 570/570 [00:00&lt;00:00, 15.7kB/s]"
          }
        },
        "f93cecca786d41f5b589dfef53c06b61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0871c186292b413382d8da12d02a65bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ec8d04b742f46cda426ae0f73bd7aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f879860b696c422fb29e3d819a21d914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be673739575c47f49b706227b3a4fed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5deffddd9f314048a3278a1b2434164a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17140ef0064e4f47939823c68d6880c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89958f0cbc9b40ec9a2ac86efbfe9937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bb8d56c99404796a6603a11229b1516",
              "IPY_MODEL_edb259ea68a946d5a4fb4985270a5f83",
              "IPY_MODEL_47016b4c176a440e963207c3c3cc64bc"
            ],
            "layout": "IPY_MODEL_424216a1ec8e4913b54c7968ae1650bd"
          }
        },
        "5bb8d56c99404796a6603a11229b1516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5e45d392b0e4c2cae5179a5d5119ff4",
            "placeholder": "​",
            "style": "IPY_MODEL_1318b39a70394fa8bffe339f0f6b7dd7",
            "value": "model.safetensors: 100%"
          }
        },
        "edb259ea68a946d5a4fb4985270a5f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a5549b0a0a343b0b01fb0aada15a803",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_926e6369c12843aab0d83074e8bd8d43",
            "value": 440449768
          }
        },
        "47016b4c176a440e963207c3c3cc64bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e587f1e49e7c4ab48edf61a48c5afa52",
            "placeholder": "​",
            "style": "IPY_MODEL_71b98e808c3b42c7945a3861953f8804",
            "value": " 440M/440M [00:08&lt;00:00, 48.7MB/s]"
          }
        },
        "424216a1ec8e4913b54c7968ae1650bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5e45d392b0e4c2cae5179a5d5119ff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1318b39a70394fa8bffe339f0f6b7dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a5549b0a0a343b0b01fb0aada15a803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "926e6369c12843aab0d83074e8bd8d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e587f1e49e7c4ab48edf61a48c5afa52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71b98e808c3b42c7945a3861953f8804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce6e261701e44fae9c3d36d27a24f80c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_786e433a4fb9482caa36a2a4cfd3393f",
              "IPY_MODEL_e6274345fafe46f9ae4728ead006a08f",
              "IPY_MODEL_d98beaad83cf45beae888f6d0f7ab770"
            ],
            "layout": "IPY_MODEL_07bde6e5bc89410faf0dc1a60b71d3a6"
          }
        },
        "786e433a4fb9482caa36a2a4cfd3393f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_146e2db7dfbe4de0b3ae18b231440f4c",
            "placeholder": "​",
            "style": "IPY_MODEL_b62b9b59bfd242b9898a0ee8a12b3b2f",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e6274345fafe46f9ae4728ead006a08f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be52c1f9ec73457b88f69cb94bdef7a7",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_382d17edf7d844a6a3c8e33bf551da8b",
            "value": 48
          }
        },
        "d98beaad83cf45beae888f6d0f7ab770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ea180df2b3c46ecb097161bf24593be",
            "placeholder": "​",
            "style": "IPY_MODEL_c87c1c80636f49899d3b19ab969c268d",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.14kB/s]"
          }
        },
        "07bde6e5bc89410faf0dc1a60b71d3a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "146e2db7dfbe4de0b3ae18b231440f4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b62b9b59bfd242b9898a0ee8a12b3b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be52c1f9ec73457b88f69cb94bdef7a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "382d17edf7d844a6a3c8e33bf551da8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ea180df2b3c46ecb097161bf24593be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c87c1c80636f49899d3b19ab969c268d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c776fec81c734574b446e200093d26cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6f3c4c952c54903be2e639b6589a131",
              "IPY_MODEL_4cd2e35537974e24a3c1e660a551ecc8",
              "IPY_MODEL_200f6e3a7eca477ba8f920de1199be00"
            ],
            "layout": "IPY_MODEL_b511a528684243bc8bf72fa3f1b7d729"
          }
        },
        "d6f3c4c952c54903be2e639b6589a131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bc554814ed84d8ba889cb59ce665c77",
            "placeholder": "​",
            "style": "IPY_MODEL_b92179dcf5344bf7b6aec66c2e79d240",
            "value": "vocab.txt: 100%"
          }
        },
        "4cd2e35537974e24a3c1e660a551ecc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecf48173c0b3462eb0bb4a9f3cf08852",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61a7c1e07f49426d8f0a8d639d62a9b2",
            "value": 231508
          }
        },
        "200f6e3a7eca477ba8f920de1199be00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4f4c75a1d144efe9223e53e525a2a41",
            "placeholder": "​",
            "style": "IPY_MODEL_e7f34261b282425895c1b3068a37c077",
            "value": " 232k/232k [00:00&lt;00:00, 2.32MB/s]"
          }
        },
        "b511a528684243bc8bf72fa3f1b7d729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bc554814ed84d8ba889cb59ce665c77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b92179dcf5344bf7b6aec66c2e79d240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecf48173c0b3462eb0bb4a9f3cf08852": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a7c1e07f49426d8f0a8d639d62a9b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4f4c75a1d144efe9223e53e525a2a41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7f34261b282425895c1b3068a37c077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c65d4e13efa4f8d8245c5e368af304b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9a947dec20d419ea5f83ab5097744d9",
              "IPY_MODEL_f7f7a4b0a7b54274a876d5bd896f1cd0",
              "IPY_MODEL_da818cbc1a9a44dca9399a5b33aec61a"
            ],
            "layout": "IPY_MODEL_08884f78f7ec434eba852b3df76cadef"
          }
        },
        "c9a947dec20d419ea5f83ab5097744d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc0a5ab5ddb64a1ba83936c35cb5a54b",
            "placeholder": "​",
            "style": "IPY_MODEL_853277b4dc46474a9ce09cc7db911724",
            "value": "tokenizer.json: 100%"
          }
        },
        "f7f7a4b0a7b54274a876d5bd896f1cd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ec0308cb02f4986b7d10d087d75c25b",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44b3ce56e8ee497bb5a015e2518a519d",
            "value": 466062
          }
        },
        "da818cbc1a9a44dca9399a5b33aec61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ca091e54ff44b3fbc3095b389c32aa9",
            "placeholder": "​",
            "style": "IPY_MODEL_bbde1be02b4a4a6988d3689fa77d6d7e",
            "value": " 466k/466k [00:00&lt;00:00, 9.80MB/s]"
          }
        },
        "08884f78f7ec434eba852b3df76cadef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc0a5ab5ddb64a1ba83936c35cb5a54b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "853277b4dc46474a9ce09cc7db911724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ec0308cb02f4986b7d10d087d75c25b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44b3ce56e8ee497bb5a015e2518a519d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ca091e54ff44b3fbc3095b389c32aa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbde1be02b4a4a6988d3689fa77d6d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30b72d8e469544599ff8ef0a0f135fcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_eaa7db8978914a54b9c43a24481ce7dd"
          }
        },
        "fca4cf0746ef4b57b18a055177aefd2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddb9654657e74948b4fc0063822706fd",
            "placeholder": "​",
            "style": "IPY_MODEL_6714dcc37396433c8161021eb6e12dc9",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "e6fdb8e6cb574a55bc1e560feae19a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_25d1759fa4874909acf4e68d3e1c598a",
            "placeholder": "​",
            "style": "IPY_MODEL_738de58e906746ceb22d96caf219916f",
            "value": ""
          }
        },
        "c71678c1736f45c1b42fc30384a31971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_9d50f1a66d184372964ca5eb2eaa49a5",
            "style": "IPY_MODEL_e59a6b6f2253443881468d90fabb8c65",
            "value": true
          }
        },
        "110997ca006a468bb0a09c43894ea817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_42fb18575c5e4850828fab661142bf14",
            "style": "IPY_MODEL_2482da37c47a4401bbadd8105ccbd0f9",
            "tooltip": ""
          }
        },
        "66c78a6a0bfc43059c8c15e4a235276f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3d036d7f73e4429967f732b114090b9",
            "placeholder": "​",
            "style": "IPY_MODEL_e50fb714dfb841eb973b8b3da4a350e4",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "eaa7db8978914a54b9c43a24481ce7dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "ddb9654657e74948b4fc0063822706fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6714dcc37396433c8161021eb6e12dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25d1759fa4874909acf4e68d3e1c598a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738de58e906746ceb22d96caf219916f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d50f1a66d184372964ca5eb2eaa49a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e59a6b6f2253443881468d90fabb8c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42fb18575c5e4850828fab661142bf14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2482da37c47a4401bbadd8105ccbd0f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b3d036d7f73e4429967f732b114090b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e50fb714dfb841eb973b8b3da4a350e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9adcdce850a4495bb50671fe1bb1c722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ccc3bf92ddf4e44b879f6d3b93e0990",
            "placeholder": "​",
            "style": "IPY_MODEL_1131172bd0aa42728a3d839fcaf31105",
            "value": "Connecting..."
          }
        },
        "5ccc3bf92ddf4e44b879f6d3b93e0990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1131172bd0aa42728a3d839fcaf31105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55a4dc7a53d5479e86e648cfacf992ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3770506e91604113a39e07e3ef4a57e7",
              "IPY_MODEL_867dabff3cfe487ba0ef2d09b1053efc",
              "IPY_MODEL_35168a0cc7724819995144032be44a73"
            ],
            "layout": "IPY_MODEL_4bfbfcb9f15a4903ba36fd7702ae1305"
          }
        },
        "3770506e91604113a39e07e3ef4a57e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_312aa0b5085c4fe4a22d7fc1de1eafe6",
            "placeholder": "​",
            "style": "IPY_MODEL_ff7a30db92a54fefaa552d45e39b06b4",
            "value": "config.json: 100%"
          }
        },
        "867dabff3cfe487ba0ef2d09b1053efc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0d851e37c26433fb98f14d6aafc3e1a",
            "max": 843,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b824acbe675e42768455857082d48dca",
            "value": 843
          }
        },
        "35168a0cc7724819995144032be44a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a6f5d2a4d274e8caee84ab8ecc46ea1",
            "placeholder": "​",
            "style": "IPY_MODEL_c2fbb6503a1e4a42854ef20859b45b46",
            "value": " 843/843 [00:00&lt;00:00, 18.8kB/s]"
          }
        },
        "4bfbfcb9f15a4903ba36fd7702ae1305": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "312aa0b5085c4fe4a22d7fc1de1eafe6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff7a30db92a54fefaa552d45e39b06b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0d851e37c26433fb98f14d6aafc3e1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b824acbe675e42768455857082d48dca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a6f5d2a4d274e8caee84ab8ecc46ea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2fbb6503a1e4a42854ef20859b45b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed75e63703e6486f8fb44551b583c3bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d2060182b394507b15e141aaf5594f3",
              "IPY_MODEL_ca08de95765f4c1ab14ecd36e4d19550",
              "IPY_MODEL_46f94779db0e4fabbe1bb17c3f9f4fb9"
            ],
            "layout": "IPY_MODEL_3a1ce337c38d457594a2bd6844b99aa3"
          }
        },
        "7d2060182b394507b15e141aaf5594f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef95b6d04ae541c48a9854cb7d2620ba",
            "placeholder": "​",
            "style": "IPY_MODEL_beaa665bbd2a451f8edc8655696116ad",
            "value": "model.safetensors: 100%"
          }
        },
        "ca08de95765f4c1ab14ecd36e4d19550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_251d6277e4cb4df5830ebfb2fe88dd9f",
            "max": 2471645608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68514c9383334aa187bd7139a84b9b9a",
            "value": 2471645608
          }
        },
        "46f94779db0e4fabbe1bb17c3f9f4fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cd2e518682e461b9edd965681d34686",
            "placeholder": "​",
            "style": "IPY_MODEL_48e8ec0f39994f6e84e9ec9a7371d007",
            "value": " 2.47G/2.47G [02:27&lt;00:00, 29.4MB/s]"
          }
        },
        "3a1ce337c38d457594a2bd6844b99aa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef95b6d04ae541c48a9854cb7d2620ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "beaa665bbd2a451f8edc8655696116ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "251d6277e4cb4df5830ebfb2fe88dd9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68514c9383334aa187bd7139a84b9b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cd2e518682e461b9edd965681d34686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48e8ec0f39994f6e84e9ec9a7371d007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70567b5bbe544ad393a0c1feef83420b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_557dded585e043b0b7cebc600e2e476c",
              "IPY_MODEL_4ead470adb4a42d2b4204def1a622650",
              "IPY_MODEL_5fac50ee5bb441b99469a877b6f8be82"
            ],
            "layout": "IPY_MODEL_13c63e67c78d47a4ba100ac4318e951d"
          }
        },
        "557dded585e043b0b7cebc600e2e476c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d727677a0224116aea9ec255d446d44",
            "placeholder": "​",
            "style": "IPY_MODEL_18568e89f65d4d39a81457d21a62db43",
            "value": "generation_config.json: 100%"
          }
        },
        "4ead470adb4a42d2b4204def1a622650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d531c90bb2e8403e9cb70cad79ec8b74",
            "max": 185,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b86900f64ba34e11b3cac0cdfaf916b3",
            "value": 185
          }
        },
        "5fac50ee5bb441b99469a877b6f8be82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da9984ef4ea04ee3802600dacc0d5635",
            "placeholder": "​",
            "style": "IPY_MODEL_f73153d0234e40a69ede5badba287c2e",
            "value": " 185/185 [00:00&lt;00:00, 10.1kB/s]"
          }
        },
        "13c63e67c78d47a4ba100ac4318e951d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d727677a0224116aea9ec255d446d44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18568e89f65d4d39a81457d21a62db43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d531c90bb2e8403e9cb70cad79ec8b74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b86900f64ba34e11b3cac0cdfaf916b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da9984ef4ea04ee3802600dacc0d5635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f73153d0234e40a69ede5badba287c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2bc775a272c45d9a6036fd618874ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e5a2a4761fa496ab8b1bc5e5a037394",
              "IPY_MODEL_fb15c531c23f40b897d0a7d77146c529",
              "IPY_MODEL_7060ccdfb603401daf458b1305a9b283"
            ],
            "layout": "IPY_MODEL_cd34fa232e004a28bc510524b63e34c7"
          }
        },
        "6e5a2a4761fa496ab8b1bc5e5a037394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3ea72167e924732ac275a81f6cf8581",
            "placeholder": "​",
            "style": "IPY_MODEL_e3978d297aae4f2cac65e29440f4bb3b",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "fb15c531c23f40b897d0a7d77146c529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cf1bf6f1ae741c2b0e64bc81575551d",
            "max": 50500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_219a0c36a4754cfea2c360079a100fdb",
            "value": 50500
          }
        },
        "7060ccdfb603401daf458b1305a9b283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57f4ad9a04584f7ea6097cac75631534",
            "placeholder": "​",
            "style": "IPY_MODEL_1dc1d6f59b3b4545879b8626b3dd389f",
            "value": " 50.5k/50.5k [00:00&lt;00:00, 5.48MB/s]"
          }
        },
        "cd34fa232e004a28bc510524b63e34c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3ea72167e924732ac275a81f6cf8581": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3978d297aae4f2cac65e29440f4bb3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cf1bf6f1ae741c2b0e64bc81575551d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "219a0c36a4754cfea2c360079a100fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57f4ad9a04584f7ea6097cac75631534": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dc1d6f59b3b4545879b8626b3dd389f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a38f7dc3daa84a829880235185847b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89ac6510c503423b8f85758c5355353c",
              "IPY_MODEL_fbf5550519d549f9a24d71f680500a03",
              "IPY_MODEL_171bba2797034e8faf36b4471a7e3b0a"
            ],
            "layout": "IPY_MODEL_12061f1afac644a89d2cfd0bf8dfee01"
          }
        },
        "89ac6510c503423b8f85758c5355353c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e313316889b24d42a326acd86036410f",
            "placeholder": "​",
            "style": "IPY_MODEL_56a32268f0184d369db1c080e1038e66",
            "value": "tokenizer.json: 100%"
          }
        },
        "fbf5550519d549f9a24d71f680500a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83c7eb12383047a097572d6c32b354b3",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77c3b6a1558a45bda218db8c490904f5",
            "value": 9085657
          }
        },
        "171bba2797034e8faf36b4471a7e3b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74802a698fa844b991ef1b73826d54aa",
            "placeholder": "​",
            "style": "IPY_MODEL_1da357160e754d9cbe44634a8b922879",
            "value": " 9.09M/9.09M [00:00&lt;00:00, 29.1MB/s]"
          }
        },
        "12061f1afac644a89d2cfd0bf8dfee01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e313316889b24d42a326acd86036410f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56a32268f0184d369db1c080e1038e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83c7eb12383047a097572d6c32b354b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77c3b6a1558a45bda218db8c490904f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74802a698fa844b991ef1b73826d54aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1da357160e754d9cbe44634a8b922879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f44302c3e1414510934cfaab7ea82c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f07233bfbb9a4072ad87b748520a9628",
              "IPY_MODEL_3c0ee42a6dc4448489ba929e424dc42d",
              "IPY_MODEL_318df7d888c34559b5ceeda63c58be77"
            ],
            "layout": "IPY_MODEL_691c8304600546e6b12b61051aca405b"
          }
        },
        "f07233bfbb9a4072ad87b748520a9628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f76e5ddc6ec247d3ac574b8ca0aaf58e",
            "placeholder": "​",
            "style": "IPY_MODEL_5f1671a9dd7f4b00816c06f34ed35403",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "3c0ee42a6dc4448489ba929e424dc42d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bf57d618b9340c199cd666c9445e610",
            "max": 301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37d9bb9ffa5e492397900f574f6bb9b4",
            "value": 301
          }
        },
        "318df7d888c34559b5ceeda63c58be77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cdcecb9a7964027b19896421d048039",
            "placeholder": "​",
            "style": "IPY_MODEL_f112e3e30072468ba05e1e8394a8a27c",
            "value": " 301/301 [00:00&lt;00:00, 38.3kB/s]"
          }
        },
        "691c8304600546e6b12b61051aca405b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f76e5ddc6ec247d3ac574b8ca0aaf58e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f1671a9dd7f4b00816c06f34ed35403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bf57d618b9340c199cd666c9445e610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37d9bb9ffa5e492397900f574f6bb9b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cdcecb9a7964027b19896421d048039": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f112e3e30072468ba05e1e8394a8a27c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}