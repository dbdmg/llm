{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7228,"status":"ok","timestamp":1764054497049,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"she-bl9Ng5BX"},"outputs":[],"source":["from huggingface_hub import login\n","import torch\n","\n","#----------------Google Colab only---------------------\n","from google.colab import userdata\n","login(userdata.get('HF_TOKEN'))\n","#----------------Google Colab only---------------------\n","\n","#detect the device available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["def make_a_query(prompt, tokenizer, model):\n","\n","    # Set pad_token_id if missing\n","    if tokenizer.pad_token_id is None:\n","        tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","\n","    # Tokenize the input with padding and truncation\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","\n","    # Compute the lenght of the input prompt to be able to extract the model's response later\n","    input_ids = inputs[\"input_ids\"]\n","    prompt_length = input_ids.shape[1]\n","\n","    # Generate a response\n","    output = model.generate(\n","        inputs['input_ids'],\n","        attention_mask=inputs['attention_mask'],\n","        max_new_tokens=200,  # Limit the number of new tokens generated (e.g., a single word)\n","        #temperature=0.3,  # Reduce randomness, use with do_sample = True\n","        repetition_penalty=2.0,  # Penalize repetition\n","        no_repeat_ngram_size=3,  # Avoid repeating bigrams\n","        do_sample= False,  # Set to False to use Greedy or Beam search\n","        num_beams=3, # Use with do_sample = False\n","        eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n","        pad_token_id=tokenizer.pad_token_id,  # Avoid padding tokens\n","        early_stopping=True,\n","    )\n","\n","    generated_tokens = output[0, prompt_length:]\n","\n","    # Decode the response into human-readable text\n","    response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n","\n","    return response"],"metadata":{"id":"l9cZxUQT13V4","executionInfo":{"status":"ok","timestamp":1764054526225,"user_tz":-60,"elapsed":36,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"htlWMlUG8ZxS"},"source":["## Exercise 1: Prompt Engineering\n","\n","Let's consider LLAMA as our starting point. In the following, we see a typical prompt feeding and text generation with LLAMA"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":653,"referenced_widgets":["4fae9c86e7324e0694532f1af804a478","fa5681a285b6407190ec9f71fac20a37","c39d0e0b9d024ab5ac7f5f37939d6e76","342d2a1dc1184dc893d781b4c1d7091c","3886c889d3bf48fd9f6e04dad567a99b","0ddb2c259f964acbb0cabe50f4c59b8a","c84e098203e94171afb25db9519a8885","551c225a76724aa09291f1a086026a61","34db60b320244dda959a3a8009d4a9c0","18ecbd12e21b46e3b03991b2becd82cc","756f796ddc704f6c96c4fd044a54a2e3"]},"executionInfo":{"elapsed":79827,"status":"ok","timestamp":1764052603807,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"rlLrPFe98ZxY","outputId":"2e03f0e2-8191-4d14-c5c5-29c3e6c90466"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fae9c86e7324e0694532f1af804a478"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","Cutting Knowledge Date: December 2023\n","Today Date: 25 Nov 2025\n","\n","<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","Tell me the capital of France.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","\n","\n","----------------------------------------------\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["Device set to use cuda\n"]},{"output_type":"stream","name":"stdout","text":["The capital city of France is Paris.\n","\n","----------------------------------------------\n","\n","\n","<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","Cutting Knowledge Date: December 2023\n","Today Date: 25 Nov 2025\n","\n","<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","Tell me the capital of France.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","The national anthem is \"La Marseillaise\" and it was written in Marseille, but Paris has been known as its de facto (and officially since) Capital city for centuries now! The official name remains 'Paris'. It's located on both banks along with Seine River within Île-de-France region which makes up about two-thirds to three-quarters portion from all metropolitan area that comprises most part urbanized landmasses around this major river system; however technically speaking there are other cities like Lyon or Bordeaux too - they also have their own claimants though not quite at same level status yet still important regional capitals nonetheless so let us focus back onto our main topic here being just one single answer only please... \n","\n","So what I am saying? That would be PARIS!!! Would you want more info regarding anything else related? Or perhaps some fun facts surrounding these beautiful places we discussed today?? Let know if yes then feel free ask away!! :)! :D. Have a great day ahead &\n"]}],"source":["\n","from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n","import datetime\n","\n","\n","# Load the model and the tokenizer\n","model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n","model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=device)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","\n","\n","\n","query = \"Tell me the capital of France.\"\n","messages = [\n","    {'role': 'user', 'content': query},\n","]\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","print(prompt)\n","\n","\n","\n","#alternative 1: use the tokenizer and produce the output\n","\n","def make_a_query(prompt, tokenizer, model):\n","\n","    # Set pad_token_id if missing\n","    if tokenizer.pad_token_id is None:\n","        tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","\n","    # Tokenize the input with padding and truncation\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","\n","    # Compute the lenght of the input prompt to be able to extract the model's response later\n","    input_ids = inputs[\"input_ids\"]\n","    prompt_length = input_ids.shape[1]\n","\n","    # Generate a response\n","    output = model.generate(\n","        inputs['input_ids'],\n","        attention_mask=inputs['attention_mask'],\n","        max_new_tokens=200,  # Limit the number of new tokens generated (e.g., a single word)\n","        #temperature=0.3,  # Reduce randomness, use with do_sample = True\n","        repetition_penalty=2.0,  # Penalize repetition\n","        no_repeat_ngram_size=3,  # Avoid repeating bigrams\n","        do_sample= False,  # Set to False to use Greedy or Beam search\n","        num_beams=3, # Use with do_sample = False\n","        eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n","        pad_token_id=tokenizer.pad_token_id,  # Avoid padding tokens\n","        early_stopping=True,\n","    )\n","\n","    generated_tokens = output[0, prompt_length:]\n","\n","    # Decode the response into human-readable text\n","    response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n","\n","    return response\n","\n","print(\"\\n----------------------------------------------\\n\\n\")\n","print(make_a_query(prompt, tokenizer, model))\n","\n","\n","#alternative 2: use the chat template and pipelines from huggingface\n","\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")\n","\n","\n","outputs = pipe(\n","    prompt,\n","    max_new_tokens=200,  # Limit the number of new tokens generated (e.g., a single word)\n","    #temperature=0.3,  # Reduce randomness\n","    repetition_penalty=2.0,  # Penalize repetition\n","    no_repeat_ngram_size=3,  # Avoid repeating bigrams\n","    do_sample=False,  # Make the output deterministic (not sampled)\n","    eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n","    pad_token_id=tokenizer.pad_token_id  # Avoid padding tokens\n",")\n","\n","print(\"\\n----------------------------------------------\\n\\n\")\n","print(outputs[0][\"generated_text\"])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vJnyAVjn8Zxb"},"source":["### Fitz\n","\n","Reference libraries to install: pip install openai pymupdf faiss-cpu scikit-learn\n","\n","PyMuPDF is a Python library that provides tools for working with PDF files (as well as other document formats like XPS, OpenXPS, CBZ, EPUB, and FB2). It's built on the MuPDF library, a lightweight, high-performance PDF and XPS rendering engine. With PyMuPDF, you can perform various tasks like reading, creating, editing, and extracting content from PDFs, images, and annotations."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13127,"status":"ok","timestamp":1764051525704,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"TVqc4EX18Zxc","outputId":"f917897d-633e-4b64-9479-a18f1cc99a0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.6)\n","Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n","professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n","A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n","around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n","built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n","largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n","with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n"," \n","Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n","Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n","they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n","trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n","1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n","without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n","players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n","podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n"," \n","Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n","international competitions, Milan is Italy's most successful club. The club has won seven European \n","Cup/Champions League titles, making them the competition's second-most successful team behind \n","Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n","record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n"," \n","Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n","now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n","Club Association. \n"," \n","\n"]}],"source":["#-----------Google Colab only----------------------\n","!pip install pymupdf # The name of the module to be installed may vary depending on the environment where you run this code (i.e., Colab versus local python environment)\n","#--------------------------------------------------\n","\n","import fitz\n","\n","# Note: Ensure \"example2.pdf\" is uploaded to the Colab runtime before running this.\n","# If the file is missing, you will get a \"RuntimeError: cannot open ...\"\n","\n","#open an example pdf\n","doc = fitz.open(\"example2.pdf\")\n","\n","# Extract text from the first page\n","page = doc.load_page(0)\n","text = page.get_text(\"text\")  # Use 'text' mode to get raw text\n","\n","\n","def extract_text_from_pdf(pdf_path):\n","    doc = fitz.open(pdf_path)\n","    text = \"\"\n","    for page_num in range(len(doc)):\n","        page = doc.load_page(page_num)\n","        text+=page.get_text()\n","    return text\n","\n","print(extract_text_from_pdf(\"example2.pdf\"))"]},{"cell_type":"markdown","metadata":{"id":"K3zpmgu48Zxc"},"source":["### Example: Text Summarization\n","\n","Let's ask LLAMA to perform a summarization of the example PDF."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27593,"status":"ok","timestamp":1764053214049,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"uqddIFVy8Zxd","outputId":"8a93039f-465c-49f1-d028-683f7913fc02"},"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the main points from the article about AC Milan:\n","\n","* AC Milan is a professional football club founded in 1909 (corrected year) in Milano, Italy.\n","* The club plays in the Seria A, Italy's top tier, and shares its home stadium, Stadio Giuseppe Meazza (San Siro), with rival Inter Milan.\n","* Milan has a rich history, having won numerous domestic and international titles, including 7 European Cup/UEFA Champions League trophies.\n","* Notable periods of success include Silvio Berusconi's 31 years as president, who led the team to 29 trophies, and the 1991-92 season when they went undefeated in the league.\n","* Domestic titles include 19 Scudetto (league titles), 5 Coppa Italia (Italian Cup) titles, and 4 other domestic cups.\n","* Internationally, Milan holds a record number of European Cup wins (7) and has also won several other prestigious tournaments.\n"]}],"source":["\n","\n","\n","#define the prompt to ask for text summarization.\n","text_summarization_prompt = \"Write a concise summary of the main ideas in article below.\"\n","text = extract_text_from_pdf(\"example2.pdf\")                          #load here the text of the article\n","p1 =  \"\"\"{PROMPT}.\\n\\n article:\\n {BODY}\"\"\".format(PROMPT=text_summarization_prompt, BODY=text)\n","messages = [\n","    {'role': 'user', 'content': p1}\n","]\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","#feed the prompt to llama\n","print(make_a_query(prompt, tokenizer, model))"]},{"cell_type":"markdown","metadata":{"id":"cgm9rDlj8Zxd"},"source":["### Adding a System Prompt\n","\n","Llama was trained with a system message that set the context and persona to assume when solving a task. One of the unsung advantages of open-access models is that you have full control over the system prompt in chat applications. This is essential to specify the behavior of your chat assistant –and even imbue it with some personality–, but it's unreachable in models served behind APIs.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27054,"status":"ok","timestamp":1764053241106,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"UDS2Qpjn8Zxe","outputId":"b3a6ccdb-6770-4b59-eb20-5261775499f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["The article provides an overview of AC Milan, a professional football club from Milan, Italy. Here are the main points:\n","\n","**Club History and Stadium**: AC Milan was founded in 1869 and currently plays in the Seria A, Italy's top-tier league. The team moved to its iconic San Siero stadium in 1936, which it shares with rival Inter Milan.\n","\n","**Success and Achievements**: Milan has a rich history of success, winning numerous domestic and international titles, including:\n","\n","* 19 Serie A league titles\n","* 5 Copa Italia (Italian Cup)\n","* 7 Super Coppa Italiana (Italian Super Cup) trophies\n","* Record-breaking 7 European Cup/Champion's League titles\n","\n","**Notable Players and Coaches**: The team has produced several notable players who have won individual awards, such as the Ballon D'Or. Notably, three of Milan's players (Marco van Bastein, Ruurd Gullitt, and Frans R\n"]}],"source":["#default standard system message from the Hugging Face blog to the prompt from above\n","system_prompt = \"You are a helpful, respectful and honest assistant. \\\n","    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n","    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n","    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n","    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n","    please don't share false information.\"\n","\n","\n","#concatenate the system prompt with your pront and get the response\n","p2 =  \"\"\"{PROMPT}. article: {BODY}\"\"\".format(PROMPT=text_summarization_prompt, BODY=text)\n","\n","messages = [\n","    {'role': 'system', 'content': system_prompt},\n","    {'role': 'user', 'content': p2}\n","]\n","\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","print(make_a_query(prompt, tokenizer, model))"]},{"cell_type":"markdown","metadata":{"id":"dJCYzgNs8Zxf"},"source":["### Customizing the System prompt\n","\n","With Llama we have full control over the system prompt. The following experiment will instruct Llama to assume the persona of a researcher tasked with writing a concise brief.\n","\n","Apply the following changes the original system prompt:\n","- Use the researcher persona and specify the tasks to summarize articles.\n","- Remove safety instructions; they are unnecessary since we ask Llama to be truthful to the article.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29041,"status":"ok","timestamp":1764053270155,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"M0BenSmf8Zxf","outputId":"099d4964-eb3c-4ebd-f944-0cbd6589fc3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Here is a summary of the article in bullet points:\n","\n","- AssociazioneCalcio Milan (AC Milan) is anItalian professional football club founded in 1889.\n","- The club plays in theSerie A, Italy's top-tier football league.\n","- Milan's home stadium is theSan Siro Stadium, which has a capacity of over 80,000 spectators.\n","- AC Milan has a longstanding rivalry withInter Milan, with whom they compete in theDerby dellaMadonnina.\n","- Under SilvioBerlusconi's presidency, Milan experienced significant success, winning 29 trophies between 1986 and 2000.\n","- In the 1991-1992 season, Milan achieved the unique feat of winning theSeria A title without losing any games.\n","- Several AC Milan players, includingMarco vanBasten,RuudGullit,andFrankRijkaad, have won theBallond'Or award.\n","- Domestically,Milan has won19league titles\n"]}],"source":["new_system_prompt = \"\"\"You are a helpful assistant.\n","    When summarizing an article or providing key points, you must:\n","    1. Use **bullet points** to list the main ideas.\n","    2. **Bullet points must be formatted like this:**\n","       - First key point goes here.\n","       - Second key point goes here.\n","       - Third key point goes here.\n","    3. Each bullet point should be one short sentence.\n","    4. Keep bullet points clear, concise, and easy to read.\n","    5. Make sure every distinct idea from the article is listed as a separate bullet point.\n","    6. Avoid paragraphs; only bullet points should be used in the output.\"\"\"\n","\n","text_summarization_prompt2 = \"Summarize the article below into bullet points. Format bullet points (use bullet points exactly like this: - point 1)\"\n","\n","p3 =  \"\"\"{PROMPT}. \\n\\narticle: \\n{BODY}\"\"\".format(PROMPT=text_summarization_prompt2, BODY=text)\n","\n","messages = [\n","    {'role': 'system', 'content': new_system_prompt},\n","    {'role': 'user', 'content': p3}\n","]\n","\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","r3 =  make_a_query(prompt, tokenizer, model)\n","\n","print(r3)"]},{"cell_type":"markdown","metadata":{"id":"FMhcKz2A8Zxg"},"source":["### Chain-of-Thought prompting\n","\n","Chain-of-thought is when a prompt is being constructed using a previous prompt answer. For our use case to extract information from text, we will first ask Llama what the article is about and then use the response to ask a second question: what problem does [what the article is about] solve?\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2287,"status":"ok","timestamp":1764053428356,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"MAsUGV2V8Zxg","outputId":"adac0b79-3eda-4ebe-ff9f-5f7d8d6fa6b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["PROMPT:\n","\n","<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","Cutting Knowledge Date: December 2023\n","Today Date: 25 Nov 2025\n","\n","You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information.\n","    Provide straight to the point answers, without bullet points.<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","How many titles did AC Milan win?.\n","\n"," article: \n","Here is a summary of the article in bullet points:\n","\n","- AssociazioneCalcio Milan (AC Milan) is anItalian professional football club founded in 1889.\n","- The club plays in theSerie A, Italy's top-tier football league.\n","- Milan's home stadium is theSan Siro Stadium, which has a capacity of over 80,000 spectators.\n","- AC Milan has a longstanding rivalry withInter Milan, with whom they compete in theDerby dellaMadonnina.\n","- Under SilvioBerlusconi's presidency, Milan experienced significant success, winning 29 trophies between 1986 and 2000.\n","- In the 1991-1992 season, Milan achieved the unique feat of winning theSeria A title without losing any games.\n","- Several AC Milan players, includingMarco vanBasten,RuudGullit,andFrankRijkaad, have won theBallond'Or award.\n","- Domestically,Milan has won19league titles<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","\n","\n","\n","\n","\n","RESPONSE:\n","\n","AC Milan has won 19 league titles.\n"]}],"source":["import re\n","\n","#define a prompt to ask what the article is about\n","\n","#r3 is already ok to know what the article is about\n","\n","new_system_prompt = \"\"\"You are a helpful, respectful and honest assistant. \\\n","    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n","    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n","    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n","    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n","    please don't share false information.\n","    Provide straight to the point answers, without bullet points.\"\"\"\n","\n","\n","#now embed the result of the previous prompt in a new prompt to ask some additional information (i.e., the year of foundation). you have to split the original prompt to create a new one\n","\n","additional_info_prompt = \"\"\"How many titles did AC Milan win?\"\"\"\n","\n","\n","p5 =  \"\"\"{PROMPT}.\\n\\n article: \\n{BODY}\"\"\".format(PROMPT=additional_info_prompt, BODY=r3)\n","\n","messages = [\n","    {'role': 'system', 'content': new_system_prompt},\n","    {'role': 'user', 'content': p5}\n","]\n","\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","\n","print(\"PROMPT:\\n\")\n","print(prompt)\n","print(\"\\n\\n\\n\")\n","\n","print(\"RESPONSE:\\n\")\n","print(make_a_query(prompt, tokenizer, model))\n"]},{"cell_type":"markdown","metadata":{"id":"eyh6U6ca8Zxh"},"source":["### Generating JSONs with Llama\n","\n","Llama needs precise instructions when asking it to generate JSON. In essence, here is what works for me to get valid JSON consistently:\n","\n","- Explicitly state — “ All output must be in valid JSON. Don’t add explanation beyond the JSON” in the system prompt.\n","- Add an “explanation” variable to the JSON example. Llama enjoys explaining its answers. Give it an outlet.\n","- Use the JSON as part of the instruction. See the “in_less_than_ten_words” example below.\n","Change “write the answer” to “output the answer.”\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30076,"status":"ok","timestamp":1764053589909,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"A9WWXrxf8Zxh","outputId":"4474ca20-4de6-4e61-b6fc-a75ad90c72b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["PROMPT:\n","\n","Summarize the article below. Provide the different main aspects of AC Milan in JSON format in the format of the above example. Provide many different aspects. \n","\n","article: \n","Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n","professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n","A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n","around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n","built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n","largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n","with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n"," \n","Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n","Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n","they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n","trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n","1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n","without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n","players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n","podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n"," \n","Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n","international competitions, Milan is Italy's most successful club. The club has won seven European \n","Cup/Champions League titles, making them the competition's second-most successful team behind \n","Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n","record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n"," \n","Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n","now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n","Club Association. \n"," \n","\n","\n","\n","\n","\n","RESPONSE:\n","\n","{\"name\": \"AC Milan\", \"founded\": \"1899\", \"current_stadium\": \"San Siro\", \"stadium_capacity\": \"75817\", \"league\": \"Serie A\", \"domestic_trophies\": {\"league_titles\": 19, \"coppa_italia\": 5, \"supercoppia_italiana\": 7}, \"international_trophIES\": {\"champions_league\": 13, \"uefa_super_cup\": 4, \"cup_winners_cup\" : 2, \"latin_cups\": 2, \"intercontinental_cups \": 3, \"fifa_club_world_cup \": 1}, \"notable_players\": {\"ballon_d_or_winners\": [\"Marco Van Basten\", \"Ruud Gullyt\", \"Frank Rijkard\"], \"top_3_podium\": \"3\"}}\n"]}],"source":["\n","\n","#example addition to a prompt to deal with jsons\n","json_prompt_addition = \"Output must be in valid JSON like the following example {{\\\"sport\\\": sport, \\\"explanation\\\": [in_less_than_ten_words]}}. Output must include only JSON.\"\n","\n","new_system_prompt = \"\"\"\n","    You are a helpful, respectful and honest assistant. \\\n","    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n","    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n","    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n","    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n","    please don't share false information.\n","    Provide straight to the point answers, without bullet points.\n","    Output must be in valid JSON like the following example {{\\\"sport\\\": sport, \\\"explanation\\\": [in_less_than_ten_words]}}. Output must include only JSON.\"\"\"\n","\n","text_summarization_prompt3 = \"Summarize the article below. Provide the different main aspects of AC Milan in JSON format in the format of the above example. Provide many different aspects\"\n","\n","#now generate a prompt by correctly concatenating the system prompt, the json prompt instruction, and a sports article\n","p6 =  \"\"\"{PROMPT}. \\n\\narticle: \\n{BODY}\"\"\".format(PROMPT=text_summarization_prompt3, BODY=text)\n","\n","messages = [\n","    {'role': 'system', 'content': new_system_prompt},\n","    {'role': 'user', 'content': p6}\n","]\n","\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","print(\"PROMPT:\\n\")\n","print(p6)\n","\n","print(\"\\n\\n\\n\")\n","\n","print(\"RESPONSE:\\n\")\n","print(make_a_query(prompt, tokenizer, model))\n","\n","#compare the difference between the prompt with the formatting instruction and a regular prompt without formatting instructions. is there any difference?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O3ipPu3i8Zxh"},"source":["### One-to-Many Shot Learning Prompting\n","\n","One-to-Many Shot Learning is a term that refers to a type of machine learning problem where the goal is to learn to recognize many different classes of objects from only one or a few examples of each class. For example, if you have only one image of a cat and one image of a dog, can you train a model to distinguish between cats and dogs in new images? This is a challenging problem because the model has to generalize well from minimal data (source)\n","\n","Important points about the prompts:\n","\n","- The system prompt includes the instructions to output the answer in JSON.\n","- The prompt consists of an one-to-many shot learning section that starts after the end of the system prompt.\n","- Shot examples are represented through pairs composed by user's question an assistant's response, as reported in the template below.\n","- The examples are given in JSON because the answers need to be JSON.\n","- The JSON allows defining the response with name, type, and explanation.\n","- The prompt question is represented by the last user's question - assistant's response pair, where the response is blank and the last <|eot_id|> is missing.\n","\n","```\n","<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","Istruzioni generali sul comportamento dell’assistente.\n","<|eot_id|>\n","\n","<|start_header_id|>user<|end_header_id|>\n","Question shot 1.\n","<|eot_id|>\n","<|start_header_id|>assistant<|end_header_id|>\n","Answer shot 1.\n","<|eot_id|>\n","\n","<|start_header_id|>user<|end_header_id|>\n","Question shot 2.\n","<|eot_id|>\n","<|start_header_id|>assistant<|end_header_id|>\n","Answer shot 2.\n","<|eot_id|>\n","\n","<|start_header_id|>user<|end_header_id|>\n","Final prompt.\n","<|eot_id|>\n","<|start_header_id|>assistant<|end_header_id|>\n","```\n","\n","\n","NOTE: in the following, we implement a slightly different version of few shot prompting, where actually all the shots are contained within the system prompt. This is needed because we want to influence the output format, and not to provide input-output examples as done in the template above."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4pzsCzFu8Zxh","outputId":"0952d22b-9023-4f7f-f442-f48bdb172aae","executionInfo":{"status":"ok","timestamp":1764053782621,"user_tz":-60,"elapsed":29863,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["PROMPT:\n","\n","Summarize the article below. Provide the different main aspects of AC Milan in JSON format in the format of the above example. Provide many different info about Milan.. \n","\n","article: \n","Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n","professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n","A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n","around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n","built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n","largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n","with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n"," \n","Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n","Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n","they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n","trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n","1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n","without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n","players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n","podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n"," \n","Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n","international competitions, Milan is Italy's most successful club. The club has won seven European \n","Cup/Champions League titles, making them the competition's second-most successful team behind \n","Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n","record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n"," \n","Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n","now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n","Club Association. \n"," \n","\n","\n","\n","\n","\n","RESPONSE:\n","\n","= {\"name\":\"AC Milan\",\"type\":\"football club\",\"explanation\":\"An Italian professional football club from Milan, Italy, competing in the Seria A.\"}, \n","{\"name\":\"San Siro\",\"type\":\"\",\"explanation\":\"\"}, \n","={\"name\":\"Serie A\",\"type \":\"top tier Italian football league\",\"explanatio\":\"The top-tier league of Italy where AC Milan competes\"}, \n","{\"Name\":\"Derby della Madonna\",\"type\" : \"rivalry name\", explanation:\"A famous football rivalry between AC Milan and Inter Milan\"}, \n","={\"Name\":\"Pierro Pirellli\",\"type :\"person\", explanation: \"A person who helped build the san Siro stadium\"}, \n"," {\"Name\":\"Silvio Belusconi\",\"type \": \"president\", explanation : \"A former president of AC milan who had a 31 year tenure\"}, \n","{ \"Name\":\"Ballon D'or \",\"type \":\"award\", explanation :\"An award given to the best footballer\n"]}],"source":["new_system_prompt = \"\"\"You are a helpful, respectful and honest assistant. \\\n","    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n","    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n","    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n","    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n","    please don't share false information.\n","    Provide straight to the point answers, without bullet points.\n","    Output must be in the following format, by describing names and descriptions extracted by the article = [\\\n","    {{\"name\": \"semiconductor\", \"type\": \"industry\", \"explanation\": \"Companies engaged in the design and fabrication of semiconductors and semiconductor devices\"}},\\\n","    {{\"name\": \"NBA\", \"type\": \"sport league\", \"explanation\": \"NBA is the national basketball league\"}},\\\n","    {{\"name\": \"Ford F150\", \"type\": \"vehicle\", \"explanation\": \"Article talks about the Ford F150 truck\"}},\\\n","    {{\"name\": \"Ford\", \"type\": \"company\", \"explanation\": \"Ford is a company that built vehicles\"}},\\\n","    {{\"name\": \"John Smith\", \"type\": \"person\", \"explanation\": \"Mentioned in the article\"}},\\\n","    ]. Output must include only this.\"\"\"\n","\n","text_summarization_prompt3 = \"Summarize the article below. Provide the different main aspects of AC Milan in JSON format in the format of the above example. Provide many different info about Milan.\"\n","\n","#now generate a prompt by correctly concatenating the system prompt, the json prompt instruction, and a sports article\n","p7 =  \"\"\"{PROMPT}. \\n\\narticle: \\n{BODY}\"\"\".format(PROMPT=text_summarization_prompt3, BODY=text)\n","\n","messages = [\n","    {'role': 'system', 'content': new_system_prompt},\n","    {'role': 'user', 'content': p7}\n","]\n","\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","print(\"PROMPT:\\n\")\n","print(p7)\n","\n","print(\"\\n\\n\\n\")\n","\n","print(\"RESPONSE:\\n\")\n","print(make_a_query(prompt, tokenizer, model))\n","\n","#compare the difference between the prompt with the formatting instruction and a regular prompt without formatting instructions. is there any difference?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1IfO-xcf8Zxi"},"source":["## Exercise 2: RAG (Retrieval-Augmented-Generation)\n","\n","\n","RAG (Retrieval-Augmented Generation) is a powerful framework in Natural Language Processing (NLP) that enhances the performance of language models by combining traditional generative models with external knowledge retrieval. This hybrid approach allows models to retrieve relevant information from a large corpus (like a database or document collection) and incorporate this information into the generation process. It is particularly useful when a model needs to answer questions, generate content, or provide explanations based on real-time or domain-specific data.\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18133,"status":"ok","timestamp":1764054549079,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"RPZMei9Q8Zxi","outputId":"9ad13e01-4fb5-4eef-c826-4dffc0fde9d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.6)\n","--- 0 ---\n","Applied Energy 367 (2024) 123431\n","Available online 16 May 2024\n","0306-2619/© 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\n","EPlus-LLM: A large language model-based computing platform for \n","automated building energy modeling \n","Gang Jiang a, Zhihao Ma a, Liang Zhang b, Jianli Chen a,* \n","a The University of Utah, United States \n","b The University of Arizona, United States   \n","H I G H L I G H T S  \n","• This study represents the pione\n","\n","\n","--- 1 ---\n","Democratizing Energy Management with LLM-Assisted Optimization\n","Autoformalism\n","Ming Jin, Bilgehan Sel, Fnu Hardeep, Wotao Yin\n","Abstract— This paper introduces a method for personaliz-\n","ing energy optimization using large language models (LLMs)\n","combined with an optimization solver. This approach, termed\n","human-guided optimization autoformalism, translates natural\n","language speciﬁcations into optimization problems, enabling\n","LLMs to handle various user-speciﬁc energy-related tasks. It\n","allows for nuanced \n","\n","\n","--- 2 ---\n","Hybrid Heterogeneous Clusters Can Lower the Energy\n","Consumption of LLM Inference Workloads\n","Grant Wilkins\n","gfw27@cam.ac.uk\n","University of Cambridge\n","Cambridge, UK\n","Srinivasan Keshav\n","sk818@cam.ac.uk\n","University of Cambridge\n","Cambridge, UK\n","Richard Mortier\n","rmm1002@cam.ac.uk\n","University of Cambridge\n","Cambridge, UK\n","ABSTRACT\n","Both the training and use of Large Language Models (LLMs) require\n","large amounts of energy. Their increasing popularity, therefore,\n","raises critical concerns regarding the energy efficiency a\n","\n","\n","--- 3 ---\n","Offline Energy-Optimal LLM Serving: Workload-Based Energy\n","Models for LLM Inference on Heterogeneous Systems\n","Grant Wilkins\n","gfw27@cam.ac.uk\n","University of Cambridge\n","Cambridge, UK\n","Srinivasan Keshav\n","sk818@cam.ac.uk\n","University of Cambridge\n","Cambridge, UK\n","Richard Mortier\n","rmm1002@cam.ac.uk\n","University of Cambridge\n","Cambridge, UK\n","ABSTRACT\n","The rapid adoption of large language models (LLMs) has led to\n","significant advances in natural language processing and text gener-\n","ation. However, the energy consumed throu\n","\n","\n","--- 4 ---\n","DynamoLLM: Designing LLM Inference Clusters\n","for Performance and Energy Efficiency\n","Jovan Stojkovic, Chaojie Zhang†, ´I˜nigo Goiri†, Josep Torrellas, Esha Choukse†\n","University of Illinois at Urbana-Champaign\n","†Microsoft Azure Research - Systems\n","Abstract—The rapid evolution and widespread adoption of\n","generative large language models (LLMs) have made them a\n","pivotal workload in various applications. Today, LLM inference\n","clusters receive a large number of queries with strict Service\n","Level Objectives (SL\n","\n","\n","--- 5 ---\n","1\n","Towards Greener LLMs: Bringing Energy-Efficiency\n","to the Forefront of LLM Inference\n","Jovan Stojkovic, Esha Choukse†, Chaojie Zhang†, Inigo Goiri†, Josep Torrellas\n","University of Illinois at Urbana-Champaign\n","†Microsoft Azure Research - Systems\n","Abstract—With the ubiquitous use of modern large language\n","models (LLMs) across industries, the inference serving for these\n","models is ever expanding. Given the high compute and memory\n","requirements of modern LLMs, more and more top-of-the-\n","line GPUs are being \n","\n","\n","--- 6 ---\n","XIWU: A BASIS FLEXIBLE AND LEARNABLE LLM FOR HIGH\n","ENERGY PHYSICS\n","Zhengde Zhang1, Yiyu Zhang1, Haodong Yao1, Jianwen Luo2, Rui Zhao1, Bo Huang1, Jiameng Zhao3,1,\n","Yipu Liao1, Ke Li1, Lina Zhao1, Jun Cao1, Fazhi Qi1,∗, and Changzheng Yuan1,†\n","1Institute of High Energy Physics, Chinese Academy of Sciences, Beijing 100049, China\n","2School of Physical Sciences, University of Chinese Academy of Sciences, Beijing 100049, China\n","3School of Computer and Artificial Intelligence, ZhengZhou University, Henan 450\n","\n","\n","--- 7 ---\n","Large Language Model (LLM)-enabled In-context\n","Learning for Wireless Network Optimization: A\n","Case Study of Power Control\n","Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu,\n","Xue Liu, Fellow, IEEE, and Charlie Zhang, Fellow, IEEE.\n","Abstract—Large language model (LLM) has recently been\n","considered a promising technique for many fields. This work\n","explores LLM-based wireless network optimization via in-context\n","learning. To showcase the potential of LLM technologies, we\n","consider the base station (BS) power\n","\n","\n","--- 8 ---\n","Can Private LLM Agents Synthesize Household Energy\n","Consumption Data?\n","Mahathir Almashor∗\n","mahathir.almashor@csiro.au\n","CSIRO Energy\n","Sydney, Australia\n","Yusuke Miyashita∗\n","yusuke.miyashita@csiro.au\n","CSIRO Energy\n","Melbourne, Australia\n","Sam West∗\n","sam.west@csiro.au\n","CSIRO Energy\n","Newcastle, Australia\n","Thi Van Dai Dong∗\n","thivandai.dong@csiro.au\n","CSIRO Energy\n","Wollonggong, Australia\n","ABSTRACT\n","Reproducible science requires easy access to data, especially with\n","the rise of data-driven and increasingly complex models used\n","\n","\n","--- 9 ---\n","Posted on 12 Aug 2024 — CC-BY-NC-SA 4 — https://doi.org/10.36227/techrxiv.172348951.12175366/v1 — e-Prints posted on TechRxiv are preliminary reports that are not peer reviewed. They should not b...\n","Optimizing LLM Inference Clusters for Enhanced Performance and\n","Energy Eﬃciency\n","Soka Hisaharo1, Yuki Nishimura1, and Aoi Takahashi1\n","1Aﬃliation not available\n","August 12, 2024\n","Abstract\n","The growing demand for eﬃcient and scalable AI solutions has driven research into optimizing the performance and energy\n","\n","\n","\n"]}],"source":["#-----------Google Colab only----------------------\n","!pip install pymupdf\n","#--------------------------------------------------\n","\n","\n","import os\n","import glob\n","import fitz\n","\n","\n","#Function to extract text from a PDF\n","def extract_text_from_pdf(pdf_path):\n","    doc = fitz.open(pdf_path)\n","    text = \"\"\n","    for page_num in range(len(doc)):\n","        page = doc.load_page(page_num)\n","        text += page.get_text()\n","    return text\n","\n","\n","# Extract text from all PDF files\n","pdf_texts = {}\n","directory=\".\"\n","pdf_files = glob.glob(os.path.join(directory,\"paper*.pdf\"))\n","n = 0\n","for pdf_file in pdf_files:\n","    pdf_texts[n] = extract_text_from_pdf(pdf_file)\n","    n = n+1\n","\n","#Display the text from all the PDF files\n","for pdf_file, text in pdf_texts.items():\n","    print(f\"--- {pdf_file} ---\")\n","    print(text[:500])  # Display the first 500 characters of each document\n","    print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"A1MXrLoJ8Zxi"},"source":["### Creating an index of vectors to represent the documents\n","\n","To perform efficient searches, we need to convert our text data into numerical vectors. To do so, we will use the first step of the BERT transformer.\n","\n","Since our full pdf files are very long to be fed as input into BERT, we perform a step in which we create a structure where we associate a document number to its abstract, and in a separate dictionary we associate a document number to its full text.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31991,"status":"ok","timestamp":1764054581075,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"DCXQtyV78Zxi","outputId":"2d8307d5-df68-4531-bfc2-66727b4182e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 768])\n"]}],"source":["from transformers import AutoModel, AutoTokenizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","\n","\n","\n","#import the Bert pretrained model from the transformers library\n","model_indexing = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","#initialization of the dictionary of abstracts. Substitute this with the abstracts of the 10 papers considered as sources for RAG\n","#(we could use functions to read the PDFs to \"cut\" the abstracts from the papers. For simplicity reasons, we will copy and paste them)\n","abstracts_dict = {\n","    0: \"\"\"Large Language Models (LLMs) are undergoing a period of rapid updates and changes, with state-\n","of-the-art (SOTA) model frequently being replaced. When applying LLMs to a specific scientific\n","field, it’s challenging to acquire unique domain knowledge while keeping the model itself advanced.\n","To address this challenge, a sophisticated large language model system named as Xiwu has been\n","developed, allowing you switch between the most advanced foundation models and quickly teach the\n","model domain knowledge. In this work, we will report on the best practices for applying LLMs in the\n","field of high-energy physics (HEP), including: a seed fission technology is proposed and some data\n","collection and cleaning tools are developed to quickly obtain domain AI-Ready dataset; a just-in-time\n","learning system is implemented based on the vector store technology; an on-the-fly fine-tuning system\n","has been developed to facilitate rapid training under a specified foundation model.\n","The results show that Xiwu can smoothly switch between foundation models such as LLaMA, Vicuna,\n","ChatGLM and Grok-1. The trained Xiwu model is significantly outperformed the benchmark model\n","on the HEP knowledge Q&A and code generation. This strategy significantly enhances the potential\n","for growth of our model’s performance, with the hope of surpassing GPT-4 as it evolves with the\n","development of open-source models. This work provides a customized LLM for the field of HEP,\n","while also offering references for applying LLM to other fields, the corresponding codes are available\n","on Github https://github.comzhang/zhengde0225/Xiwu.\"\"\",\n","    1: \"\"\"\n","Abstract—With the ubiquitous use of modern large language\n","models (LLMs) across industries, the inference serving for these\n","models is ever expanding. Given the high compute and memory\n","requirements of modern LLMs, more and more top-of-the-\n","line GPUs are being deployed to serve these models. Energy\n","availability has come to the forefront as the biggest challenge for\n","data center expansion to serve these models. In this paper, we\n","present the trade-offs brought up by making energy efficiency\n","the primary goal of LLM serving under performance SLOs.\n","We show that depending on the inputs, the model, and the\n","service-level agreements, there are several knobs available to\n","the LLM inference provider to use for being energy efficient.\n","We characterize the impact of these knobs on the latency,\n","throughput, as well as the energy. By exploring these trade-\n","offs, we offer valuable insights into optimizing energy usage\n","without compromising on performance, thereby paving the way\n","for sustainable and cost-effective LLM deployment in data center\n","environments\n","    \"\"\",\n","    2: \"\"\"\n","The rapid adoption of large language models (LLMs) has led to\n","significant advances in natural language processing and text gener-\n","ation. However, the energy consumed through LLM model infer-\n","ence remains a major challenge for sustainable AI deployment. To\n","address this problem, we model the workload-dependent energy\n","consumption and runtime of LLM inference tasks on heterogeneous\n","GPU-CPU systems. By conducting an extensive characterization\n","study of several state-of-the-art LLMs and analyzing their energy\n","and runtime behavior across different magnitudes of input prompts\n","and output text, we develop accurate (𝑅2 >0.96)energy and run-\n","time models for each LLM. We employ these models to explore\n","an offline, energy-optimal LLM workload scheduling framework.\n","Through a case study, we demonstrate the advantages of energy\n","and accuracy aware scheduling compared to existing best practices\n","    \"\"\",\n","    3: \"\"\"\n","The growing demand for efficient and scalable AI solutions has driven research into optimizing the performance and energy\n","efficiency of computational infrastructures. The novel concept of redesigning inference clusters and modifying the GPT-Neo\n","model offers a significant advancement in addressing the computational and environmental challenges associated with AI\n","deployment. By developing a novel cluster architecture and implementing strategic architectural and algorithmic changes,\n","the research achieved substantial improvements in throughput, latency, and energy consumption. The integration of advanced\n","interconnect technologies, high-bandwidth memory modules, and energy-efficient power management techniques, alongside\n","software optimizations, enabled the redesigned clusters to outperform baseline models significantly. Empirical evaluations\n","demonstrated superior scalability, robustness, and environmental sustainability, emphasizing the potential for more sustainable\n","AI technologies. The findings underscore the importance of balancing performance with energy efficiency and provide a robust\n","framework for future research and development in AI optimization. The research contributes valuable insights into the design\n","and deployment of more efficient and environmentally responsible AI systems.\n","1\n","\"\"\",\n","    4: \"\"\"\n","Establishing building energy models (BEMs) for building design and analysis poses significant challenges due to demanding modeling efforts, expertise to use simulation software, and building science knowledge in practice. These make building modeling labor-intensive, hindering its widespread adoptions in building development. Therefore, to overcome these challenges in building modeling with enhanced automation in modeling practice, this paper proposes Eplus-LLM (EnergyPlus-Large Language Model) as the auto-building modeling platform, building on a fine-tuned large language model (LLM) to directly translate natural language description of buildings to established building models of various geometries, occupancy scenarios, and equipment loads. Through fine-tuning, the LLM (i.e., T5) is customized to digest natural language and simulation demands from users and convert human descriptions into EnergyPlus modeling files. Then, the Eplus-LLM platform realizes the automated building modeling through invoking the API of simulation software (i.e., the EnergyPlus engine) to simulate the auto-generated model files and output simulation results of interest. The validation process, involving four different types of prompts, demonstrates that Eplus-LLM reduces over 95% modeling efforts and achieves 100% accuracy in establishing BEMs while being robust to interference in usage, including but not limited to different tones, misspells, omissions, and redundancies. Overall, this research serves as the pioneering effort to customize LLM for auto-modeling purpose (directly build-up building models from natural language), aiming to provide a user-friendly human-AI interface that significantly reduces building modeling efforts. This work also further facilitates large-scale building model efforts, e.g., urban building energy modeling (UBEM), in modeling practice.\n","\"\"\",\n","5: \"\"\"\n","The rapid evolution and widespread adoption of\n","generative large language models (LLMs) have made them a\n","pivotal workload in various applications. Today, LLM inference\n","clusters receive a large number of queries with strict Service\n","Level Objectives (SLOs). To achieve the desired performance,\n","these models execute on power-hungry GPUs causing the in-\n","ference clusters to consume large amount of energy and, conse-\n","quently, result in excessive carbon emissions. Fortunately, we find\n","that there is a great opportunity to exploit the heterogeneity in\n","inference compute properties and fluctuations in inference work-\n","loads, to significantly improve energy-efficiency. However, such\n","a diverse and dynamic environment creates a large search-space\n","where different system configurations (e.g., number of instances,\n","model parallelism, and GPU frequency) translate into different\n","energy-performance trade-offs. To address these challenges, we\n","propose DynamoLLM, the first energy-management framework\n","for LLM inference environments. DynamoLLM automatically\n","and dynamically reconfigures the inference cluster to optimize for\n","energy and cost of LLM serving under the service’s performance\n","SLOs. We show that at a service-level, DynamoLLM conserves\n","53 percent energy and 38 percent operational carbon emissions, and reduces\n","61 percent cost to the customer, while meeting the latency SLOs\n","\"\"\",\n","6: \"\"\"\n","Large language model (LLM) has recently been\n","considered a promising technique for many fields. This work\n","explores LLM-based wireless network optimization via in-context\n","learning. To showcase the potential of LLM technologies, we\n","consider the base station (BS) power control as a case study,\n","a fundamental but crucial technique that is widely investigated\n","in wireless networks. Different from existing machine learning\n","(ML) methods, our proposed in-context learning algorithm relies\n","on LLM’s inference capabilities. It avoids the complexity of\n","tedious model training and hyper-parameter fine-tuning, which is\n","a well-known bottleneck of many ML algorithms. Specifically, the\n","proposed algorithm first describes the target task via formatted\n","natural language, and then designs the in-context learning\n","framework and demonstration examples. After that, it considers\n","two cases, namely discrete-state and continuous-state problems,\n","and proposes state-based and ranking-based methods to select\n","appropriate examples for these two cases, respectively. Finally, the\n","simulations demonstrate that the proposed algorithm can achieve\n","comparable performance as conventional deep reinforcement\n","learning (DRL) techniques without dedicated model training or\n","fine-tuning. Such an efficient and low-complexity approach has\n","great potential for future wireless network optimization.\n","Index Terms—Large language model, in-context learning, net-\n","work optimization, transmission power contro\n","\"\"\",\n","7: \"\"\"\n","Both the training and use of Large Language Models (LLMs) require\n","large amounts of energy. Their increasing popularity, therefore,\n","raises critical concerns regarding the energy efficiency and sus-\n","tainability of data centers that host them. This paper addresses the\n","challenge of reducing energy consumption in data centers running\n","LLMs. We propose a hybrid data center model that uses a cost-based\n","scheduling framework to dynamically allocate LLM tasks across\n","hardware accelerators that differ in their energy efficiencies and\n","computational capabilities. Specifically, our workload-aware strat-\n","egy determines whether tasks are processed on energy-efficient\n","processors or high-performance GPUs based on the number of in-\n","put and output tokens in a query. Our analysis of a representative\n","LLM dataset, finds that this hybrid strategy can reduce CPU+GPU\n","energy consumption by 7.5 percent compared to a workload-unaware\n","baseline\n","\"\"\",\n","8: \"\"\"\n","Reproducible science requires easy access to data, especially with\n","the rise of data-driven and increasingly complex models used within\n","energy research. Too often however, the data to reconstruct and\n","verify purported solutions in publications is hidden due to some\n","combination of commercial, legal, and sensitivity issues. This early\n","work presents our initial efforts to leverage the recent advance-\n","ments in Large Language Models (LLMs) to create usable and share-\n","able energy datasets. In particular, we’re utilising their mimicry of\n","human behaviors, with the goal of extracting and exploring syn-\n","thetic energy data through the simulation of LLM agents capable of\n","interacting with and executing actions in controlled environments.\n","We also analyse and visualise publicly available data in an attempt\n","to create realistic but not quite exact copies of the originals. Our\n","early results show some promise, with outputs that resemble the\n","twin peak curves for household energy consumption. The hope is\n","that our generalised approach can be used to easily replicate usable\n","and realistic copies of otherwise secret or sensitive data\n","\"\"\",\n","9: \"\"\"\n","This paper introduces a method for personaliz-\n","ing energy optimization using large language models (LLMs)\n","combined with an optimization solver. This approach, termed\n","human-guided optimization autoformalism, translates natural\n","language specifications into optimization problems, enabling\n","LLMs to handle various user-specific energy-related tasks. It\n","allows for nuanced understanding and nonlinear reasoning\n","tailored to individual preferences. The research covers common\n","energy sector tasks like electric vehicle charging, HVAC control,\n","and long-term planning for renewable energy installations. This\n","novel strategy represents a significant advancement in context-\n","based optimization using LLMs, facilitating sustainable energy\n","practices customized to individual needs\n","\"\"\"\n","}\n","\n","#the text for rag is used as an input to the BERT model\n","\n","#The tokenized inputs are passed to the BERT model for processing.\n","#(#remember padding=True: Ensures that all inputs are padded to the same length, allowing batch processing.)\n","#The model outputs a tensor (last_hidden_state), where each input token is represented by a high-dimensional vector.\n","#last_hidden_state is of shape (batch_size, sequence_length, hidden_size), where:\n","#batch_size: Number of input texts.\n","#sequence_length: Length of each tokenized text (after padding).\n","#hidden_size: Dimensionality of the vector representation for each token (default 768 for bert-base-uncased).\n","\n","#last_hidden_state[:, 0]: Selects the representation of the [CLS] token for each input text. The [CLS] token is a special token added at the start of each input and is often used as the aggregate representation for the entire sequence.\n","tokenized_inputs = tokenizer(\n","    list(abstracts_dict.values()),\n","    return_tensors='pt', # stands for 'PyTorch': tensors will be returned (if omitted, we will get Python lists)\n","    padding=True)   # all the abstracts will be padded to the same length, in order to have a uniformed batch of abstracts => an attention mask will be created\n","tokenized_inputs = {key: value.to(device) for key, value in tokenized_inputs.items()}\n","\n","abstract_vectors = model_indexing(**tokenized_inputs).last_hidden_state[:, 0]\n","\n","\n","#abstract_vectors is a tensor of shape (batch_size, hidden_size) (e.g., (3, 768) in this case), representing each text as a single 768-dimensional vector.\n","\n","print(abstract_vectors.shape)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uDjLdVOF8Zxj"},"source":["### Search\n","\n","With our text data vectorized and indexed, we can now perform searches. We will define a function to search the index for the most relevant documents based on a query.\n","\n","To perform the search, we need a function (search documents) where we perform the cosine similarity between the query vector and all the abstract vectors. This function will give our the top-k indexes. Once we find the top-k indexes, with another function, we can collect the full text of the documents from the paper dictionary.\n","\n","To compute cosine similarity, refer to the following formula\n","\n","```cs = cosine_similarity(vector_a.detach().numpy(), vector_b.detach().numpy())```\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58,"status":"ok","timestamp":1764054581135,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"ezb-nnxF8Zxk","outputId":"5ff8ec36-e34b-46ed-ae34-5fb5896e8ed1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model is on device: cuda:0\n","Query tensor is on device: cuda:0\n","Similarities:  [[0.7173538  0.6634521  0.7067648  0.5549063  0.5200508  0.487019\n","  0.64192444 0.6023423  0.67189443 0.7392405 ]]\n","Sorted indices:  [[9 0 2 8 1 6 7 3 4 5]]\n","best index: [[9]]\n"]}],"source":["\n","\n","def get_top_k_similar_indices(query_vector, abstract_vectors, k):\n","    \"\"\"\n","    Computes the top k indices of the most similar abstracts to the query based on cosine similarity.\n","\n","    Parameters:\n","    - query_vector: A tensor of shape (1, hidden_size) representing the query vector.\n","    - abstract_vectors: A tensor of shape (batch_size, hidden_size) representing the abstract vectors.\n","    - k: The number of top indices to return.\n","\n","    Returns:\n","    - sorted_indices: A numpy array of shape (1, k) containing the indices of the top k most similar abstracts.\n","    \"\"\"\n","    # Compute cosine similarity between the query and each abstract\n","    similarities = cosine_similarity(query_vector.cpu().detach().numpy(), abstract_vectors.cpu().detach().numpy()) # ATTENTION: using CPU for this instruction is mandatory, so the call to cpu() is fundamental\n","    print(\"Similarities: \", similarities)\n","\n","    # Sort the indices by similarity in descending order\n","    sorted_indices = np.argsort(similarities, axis=1)[:, ::-1] # Pay attention to the shape\n","    print(\"Sorted indices: \", sorted_indices)\n","\n","    # Return the top k indices\n","    return sorted_indices[:, :k]\n","\n","\n","def retrieve_documents(indices, documents_dict):\n","\n","    #Retrieves the documents corresponding to the given indices and concatenates them into a single string.\n","\n","    #Parameters:\n","    #- indices: A numpy array or list of top-k indices of the most similar documents.\n","    #- documents_dict: A dictionary where keys are document indices (integers) and values are the document texts (strings).\n","\n","    #Returns:\n","    #- concatenated_documents: A string containing the concatenated texts of the retrieved documents.\n","\n","    # Retrieve the documents corresponding to the indices\n","    retrieved_docs = [documents_dict[i] for i in indices[0]]  # Assuming indices is a 2D array\n","\n","    # Concatenate the documents into a single string\n","    concatenated_documents = \" \".join(retrieved_docs)\n","\n","    return concatenated_documents\n","\n","\n","#now I create a vector also for my query\n","\n","query = \"energy topics covered by papers with large language models\"\n","\n","tokenized_query = tokenizer(query, return_tensors='pt')\n","\n","# Move the tokenized query to the same device as the model\n","tokenized_query = {key: value.to(device) for key, value in tokenized_query.items()}\n","\n","# Now pass the tokenized query to the model\n","query_vector = model_indexing(**tokenized_query).last_hidden_state[:, 0]\n","\n","print(f\"Model is on device: {model_indexing.device}\")\n","print(f\"Query tensor is on device: {tokenized_query['input_ids'].device}\")\n","\n","\n","indices = get_top_k_similar_indices(query_vector, abstract_vectors, 1)\n","\n","print(f\"best index: {indices}\")\n","\n","\n","concatenated_documents=retrieve_documents(indices, pdf_texts)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3xDNf5sB8Zxk"},"source":["### A function to perform Retrieval Augmented Generation\n","\n","In this step, we’ll combine the context retrieved from our documents with LLAMA to generate responses. The context will provide the necessary information to the model to produce more accurate and relevant answers."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":530,"referenced_widgets":["6cb40c1c7e0042a3a4e19605be2a6727","45b803a0b306447289a4800bbf501b2b","eafd4da2cc8e4fc0adcbfd285892a2f2","44f55deff78f4b9caa641d35ea39eabd","64539f9d562548a2b8404d33767369b0","d3d9b170e38d42ad9b94a058d276a6d9","431d5de00ad94ef28856add47dc9cc9a","26fab9b95dc34b71ad86e69dc369413e","fe7773f2ef02480c802098c35a54cee3","5123a2fce010408ea18bf81aaa57e559","aae139a108ba485d8ec9b6567797746f","7e7bcdbda90d4ed38c2cfa377fde31ac","0429ea9993734b4eb634ed89722b6a58","fdf787bba4d1448d976d8545ddeb568f","57735f2c2fd14e4493c87a9650c74744","75e00fbc905743ae881e1a1b08d172e8","12144838327149b6be72319361563f3d","a18034c0c1094d21b21a2cb2b7722582","654e36d993a54b909dfeebe1c149c998","eabbcea1ad7c4577a25fd717b3c27596","7aca10f27cb5402f834e9cae8d844a71","0b3b072de29e47aea3e82002f8a142b8","313411d94e7c40358d4a14a5e4737751","f42d4a5ef9e54342bac76283d11c6c61","20dfaeb1f1a54119b187c16086c9faee","cf4bbc9d22774447b66c3a24afd7a058","04e48f117e8447d6bf0bb90a6282d036","c7739c1138b0464e99d1e0aa1ad4954a","00c1b112af6b49c488f58748c0bf5e7c","1086cfc799694513964790669fdc2f4c","e7a6863a529b424d80f33d8b6c05ea1c","9a5b7e23e80543deadfdd9a6d99517c1","eb92cbd4dd254824b8331f6fcb53fdd4","40c1c6020e2548609a3c1f3df2444bae","3b8982135967434dbbf7d7e88866e958","ac8933ac8d3c4bc0bd08aa4468a3d88e","8bf52a0a1d7c445fbaa542d4af6f7cf8","b89ce04db22f4221b0af786fcd853cee","1622ab807630437783b08b7d070256a8","074f8bd803ea4c01ba5c2c2cc16e79e7","f4c0812c66a64358ba03b339f89619ca","9af3ad1405c144508385a7a89c37966b","b5910390d38e4574a973843528d4771d","4ed46d8881cd4b539a4f7277a02e3da6"]},"executionInfo":{"elapsed":80609,"status":"ok","timestamp":1764054661760,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"mmnsrjjD8Zxk","outputId":"af17f6ec-43ce-4a2b-ad13-dfb1052fb565"},"outputs":[{"output_type":"stream","name":"stderr","text":["`torch_dtype` is deprecated! Use `dtype` instead!\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cb40c1c7e0042a3a4e19605be2a6727"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e7bcdbda90d4ed38c2cfa377fde31ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"313411d94e7c40358d4a14a5e4737751"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40c1c6020e2548609a3c1f3df2444bae"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["PROMPT:\n","\n","\n","What are the main energy issues for Large Language Models?\n","\n","\n","\n","\n","RESPONSE:\n","\n","\n","Large language models, such as myself, face several key energy-related challenges. One major issue is the significant computational power required to train and operate these models. This is due to the large amounts of data they need to process and the complex neural network architectures used to generate human-like responses.\n","\n","Training a large language model requires massive amounts of computational resources, including powerful hardware like graphics processing units (GPUs) and tensor processing unit (TPU) accelerators. The energy consumption of these systems can be substantial, contributing to greenhouse gas emissions and e-waste.\n","\n","Another concern is the carbon footprint associated with data storage and transmission. Large language models rely on vast amounts of text data, which needs to be stored and transmitted over the internet. This can result in significant energy consumption, particularly if the data centers where the models are hosted are powered by non-renewable energy sources.\n","\n","Additionally, the training process itself can be energy-intensive, as it involves complex mathematical operations that require significant computational resources. Furthermore, the\n"]}],"source":["import torch\n","from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n","import datetime\n","\n","\n","model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n","model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","query_for_rag = \"What are the main energy issues for Large Language Models?\"\n","\n","new_system_prompt = \"\"\"You are a helpful, respectful and honest assistant. \\\n","    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n","    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n","    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n","    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n","    please don't share false information.\n","    Provide straight to the point answers, without bullet points.\"\"\"\n","\n","\n","#now embed the result of the previous prompt in a new prompt to ask some additional information (i.e., the year of foundation). you have to split the original prompt to create a new one\n","\n","p_not_augmented =  query_for_rag\n","\n","messages = [\n","    {'role': 'system', 'content': new_system_prompt},\n","    {'role': 'user', 'content': p_not_augmented}\n","]\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","print(\"PROMPT:\\n\\n\")\n","print(p_not_augmented)\n","\n","print(\"\\n\\n\\n\")\n","\n","print(\"RESPONSE:\\n\\n\")\n","print(make_a_query(prompt, tokenizer, model))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":493,"status":"error","timestamp":1764054662295,"user":{"displayName":"anna arnaudo","userId":"15108618421102072944"},"user_tz":-60},"id":"EzR9sYvC8Zxk","outputId":"2dac54a3-8dce-48e4-efd8-9a30b2323310"},"outputs":[{"output_type":"stream","name":"stdout","text":["PROMPT:\n","\n","\n","What are the main energy issues for Large Language Models? \n","\n","Collect information from the following article: Posted on 12 Aug 2024 — CC-BY-NC-SA 4 — https://doi.org/10.36227/techrxiv.172348951.12175366/v1 — e-Prints posted on TechRxiv are preliminary reports that are not peer reviewed. They should not b...\n","Optimizing LLM Inference Clusters for Enhanced Performance and\n","Energy Eﬃciency\n","Soka Hisaharo1, Yuki Nishimura1, and Aoi Takahashi1\n","1Aﬃliation not available\n","August 12, 2024\n","Abstract\n","The growing demand for eﬃcient and scalable AI solutions has driven research into optimizing the performance and energy\n","eﬃciency of computational infrastructures. The novel concept of redesigning inference clusters and modifying the GPT-Neo\n","model oﬀers a signiﬁcant advancement in addressing the computational and environmental challenges associated with AI\n","deployment.\n","By developing a novel cluster architecture and implementing strategic architectural and algorithmic changes,\n","the research achieved substantial improvements in throughput, latency, and energy consumption. The integration of advanced\n","interconnect technologies, high-bandwidth memory modules, and energy-eﬃcient power management techniques, alongside\n","software optimizations, enabled the redesigned clusters to outperform baseline models signiﬁcantly.\n","Empirical evaluations\n","demonstrated superior scalability, robustness, and environmental sustainability, emphasizing the potential for more sustainable\n","AI technologies. The ﬁndings underscore the importance of balancing performance with energy eﬃciency and provide a robust\n","framework for future research and development in AI optimization. The research contributes valuable insights into the design\n","and deployment of more eﬃcient and environmentally responsible AI systems.\n","1\n","1\n","Optimizing LLM Inference Clusters for Enhanced\n","Performance and Energy Efficiency\n","Soka Hisaharo*, Yuki Nishimura, and Aoi Takahashi\n","Abstract—The growing demand for efficient and scalable AI\n","solutions has driven research into optimizing the performance\n","and energy efficiency of computational infrastructures. The novel\n","concept of redesigning inference clusters and modifying the\n","GPT-Neo model offers a significant advancement in addressing\n","the computational and environmental challenges associated with\n","AI deployment. By developing a novel cluster architecture and\n","implementing strategic architectural and algorithmic changes,\n","the research achieved substantial improvements in throughput,\n","latency, and energy consumption. The integration of advanced\n","interconnect technologies, high-bandwidth memory modules, and\n","energy-efficient power management techniques, alongside soft-\n","ware optimizations, enabled the redesigned clusters to out-\n","perform baseline models significantly. Empirical evaluations\n","demonstrated superior scalability, robustness, and environmental\n","sustainability, emphasizing the potential for more sustainable\n","AI technologies. The findings underscore the importance of\n","balancing performance with energy efficiency and provide a\n","robust framework for future research and development in AI\n","optimization. The research contributes valuable insights into the\n","design and deployment of more efficient and environmentally\n","responsible AI systems.\n","Index Terms—Inference clusters, Performance optimization,\n","Energy efficiency, AI scalability, Sustainable AI.\n","I. INTRODUCTION\n","T\n","HE optimization of inference clusters for large language\n","models (LLMs) holds significant importance due to the\n","ever-increasing demand for efficient and scalable natural lan-\n","guage processing solutions. As LLMs become more sophisti-\n","cated and widely used, the need to improve their performance\n","while minimizing energy consumption becomes paramount.\n","The open-source LLM GPT-Neo, in particular, offers a valu-\n","able platform for exploring innovative approaches to achieving\n","these goals. By focusing on redesigning inference clusters,\n","this research aims to enhance the operational efficiency of\n","GPT-Neo, thereby contributing to the broader field of artificial\n","intelligence.\n","A. Background\n","LLMs have revolutionized the field of natural language\n","processing by enabling machines to understand and generate\n","human language with remarkable accuracy. These models,\n","trained on vast datasets, have demonstrated proficiency in a\n","wide range of tasks, including language translation, summa-\n","rization, and question answering. However, the deployment of\n","LLMs in real-world applications poses significant challenges,\n","particularly in terms of computational requirements and energy\n","consumption. Inference clusters, which facilitate the execution\n","of these models, play a crucial role in addressing these\n","challenges. Effective optimization of inference clusters can\n","lead to substantial improvements in model performance and\n","energy efficiency, making the widespread adoption of LLMs\n","more feasible and sustainable.\n","The relevance of GPT-Neo within this context cannot\n","be overstated. As an open-source alternative to proprietary\n","LLMs, GPT-Neo provides researchers and developers with\n","the flexibility to modify and enhance the model’s architecture\n","and performance. The ability to experiment with different\n","configurations and optimizations makes GPT-Neo an ideal\n","candidate for this research. By leveraging the unique features\n","of GPT-Neo, this study aims to demonstrate how targeted mod-\n","ifications to inference clusters can yield significant benefits in\n","terms of speed, accuracy, and energy consumption.\n","B. Motivation\n","The motivation behind optimizing inference clusters for\n","LLMs is multifaceted. Firstly, there is a growing demand for\n","real-time natural language processing applications, such as\n","virtual assistants, chatbots, and automated content generation\n","systems. These applications require LLMs to process large\n","volumes of data quickly and accurately, necessitating the\n","development of more efficient inference clusters. Secondly,\n","the environmental impact of deploying LLMs on a large scale\n","cannot be ignored. Data centers, which host inference clusters,\n","consume substantial amounts of energy, contributing to carbon\n","emissions and environmental degradation. Enhancing the en-\n","ergy efficiency of inference clusters is therefore essential for\n","reducing the carbon footprint of AI technologies.\n","Furthermore, the financial cost associated with running\n","LLMs in production environments is a significant considera-\n","tion. Organizations that rely on LLMs for their operations must\n","balance the need for high performance with the imperative to\n","control operational expenses. By optimizing inference clusters,\n","it is possible to achieve better performance at a lower cost,\n","thereby making LLMs more accessible to a wider range\n","of users and applications. The potential for cost savings,\n","combined with the environmental benefits, underscores the\n","importance of this research.\n","C. Objectives\n","The primary objectives of this research are to redesign\n","the inference clusters used by GPT-Neo to enhance both\n","performance and energy efficiency, and to empirically evaluate\n","the impact of these modifications. Specific objectives include:\n","developing a novel cluster architecture that optimally balances\n","computational load and energy consumption; implementing\n","software optimizations to improve the efficiency of inference\n","processes; and systematically assessing the performance and\n","2\n","energy efficiency of the redesigned clusters compared to\n","baseline configurations.\n","By achieving these objectives, the research aims to con-\n","tribute valuable insights into the design and optimization of\n","inference clusters for LLMs. The findings are expected to\n","inform best practices for deploying LLMs in various real-\n","world applications, ultimately leading to more efficient and\n","sustainable AI technologies. The research will also provide a\n","framework for further exploration and experimentation with\n","other open-source LLMs, fostering innovation and collabora-\n","tion within the AI research community.\n","II. RELATED STUDIES\n","A comprehensive examination of existing literature on the\n","optimization of large language models (LLMs) and the de-\n","velopment of energy-efficient inference clusters is essential\n","for understanding the current state of the art and identifying\n","areas for further research. This section reviews significant\n","contributions in the areas of performance optimization and\n","energy efficiency as they pertain to LLMs, focusing on the\n","technical advancements and outcomes achieved through vari-\n","ous methodologies.\n","A. Performance Optimization\n","Performance optimization of LLMs has been a focal point\n","of research, with numerous approaches developed to enhance\n","the computational efficiency and processing speed of these\n","models. Advanced parallelization techniques significantly im-\n","proved the throughput of LLMs, enabling more efficient\n","handling of large-scale data during inference processes [1],\n","[2]. Leveraging model pruning and quantization, considerable\n","reductions in computational overhead were achieved without\n","compromising the accuracy of language understanding tasks\n","[3]. The implementation of distributed training frameworks\n","facilitated the scaling of LLMs across multiple processing\n","units, thereby accelerating model training and inference [4],\n","[5]. Techniques such as gradient checkpointing and mixed\n","precision training were employed to reduce memory consump-\n","tion, thus allowing larger models to be deployed on limited\n","hardware resources [6], [7]. Novel architectures, including\n","transformer variants with optimized attention mechanisms,\n","demonstrated significant improvements in processing speed\n","and model efficiency [8]. The adoption of adaptive computa-\n","tion strategies enabled dynamic adjustment of computational\n","resources based on the complexity of input data, enhanc-\n","ing overall performance [9]. Incorporating efficient data pre-\n","processing pipelines streamlined the handling of text data,\n","further boosting the performance of LLMs during inference\n","[10], [11]. Custom hardware accelerators, specifically designed\n","for LLM workloads, provided substantial gains in inference\n","speed and energy efficiency [12], [13]. Advanced software\n","optimizations, such as optimized matrix multiplication li-\n","braries, contributed to faster execution of model operations\n","[14]. Utilizing knowledge distillation techniques, smaller and\n","faster models were derived from larger LLMs, maintaining\n","performance while reducing computational requirements [15],\n","[16]. The integration of intelligent caching mechanisms re-\n","duced redundant computations, thereby improving inference\n","latency and throughput [17].\n","B. Energy Efficiency\n","Research focused on enhancing the energy efficiency of\n","inference clusters for LLMs has yielded various strategies to\n","minimize power consumption while maintaining high perfor-\n","mance levels. Dynamic voltage and frequency scaling (DVFS)\n","techniques were utilized to adjust the power usage of process-\n","ing units based on workload demands, resulting in significant\n","energy savings [9], [18]. The deployment of energy-aware\n","scheduling algorithms ensured optimal utilization of computa-\n","tional resources, thereby reducing overall energy consumption\n","during inference [19]. Implementing efficient cooling systems\n","and heat dissipation techniques in data centers contributed to\n","lower energy usage and improved system reliability [20]. The\n","use of low-power processing units and accelerators specifically\n","designed for LLM tasks reduced the energy footprint of\n","inference operations [21], [22]. Techniques such as power\n","gating and clock gating were employed to disable inactive\n","components of the hardware, thereby conserving energy [23].\n","The development of energy-efficient model architectures, in-\n","cluding those with reduced parameter counts, maintained\n","performance while operating at lower power levels [24], [25].\n","Utilizing energy-efficient memory hierarchies and data storage\n","solutions minimized the power required for data access and\n","retrieval during inference [26]. The adoption of renewable\n","energy sources for powering data centers helped to offset the\n","environmental impact of running large-scale LLM inference\n","clusters [27]. Advanced monitoring and management systems\n","enabled real-time tracking of energy usage, facilitating more\n","efficient energy consumption strategies [17]. Implementing\n","virtualization and containerization technologies allowed for\n","more flexible and efficient use of computational resources,\n","thereby enhancing energy efficiency [28], [29]. Techniques\n","such as workload consolidation and intelligent resource al-\n","location optimized the distribution of tasks across available\n","hardware, reducing idle power consumption [30].\n","III. METHODOLOGY\n","This section outlines the comprehensive methodology em-\n","ployed to redesign the inference clusters and modify GPT-Neo\n","to enhance performance and energy efficiency. The methodol-\n","ogy encompasses the redesign of cluster architecture, specific\n","modifications to the GPT-Neo model, and the evaluation\n","metrics used to assess the improvements achieved through\n","these modifications.\n","A. Cluster Redesign\n","The redesign of the inference clusters involved several\n","strategic changes aimed at improving both performance and\n","energy efficiency. A novel cluster architecture was developed,\n","incorporating advanced interconnect technologies to reduce\n","latency and enhance data transfer speeds across the nodes. By\n","optimizing the spatial arrangement of processing units, signif-\n","icant reductions in communication overhead were achieved,\n","3\n","thereby accelerating inference processes. The integration of\n","high-bandwidth memory (HBM) modules facilitated faster\n","data access and reduced bottlenecks, contributing to overall\n","performance improvements. Energy-efficient power manage-\n","ment techniques, such as dynamic voltage and frequency\n","scaling (DVFS), were implemented to adjust the power usage\n","of the clusters in response to workload demands. Additionally,\n","custom cooling solutions were deployed to maintain optimal\n","operating temperatures, thereby enhancing the reliability and\n","efficiency of the clusters. The adoption of containerization\n","technologies enabled efficient resource allocation and work-\n","load management, further optimizing cluster performance.\n","Advanced monitoring systems were integrated to provide real-\n","time insights into cluster performance and energy usage,\n","enabling more effective management and optimization. The\n","use of machine learning algorithms for predictive maintenance\n","helped to minimize downtime and ensure consistent perfor-\n","mance levels. Finally, the redesigned clusters incorporated re-\n","newable energy sources, reducing the overall carbon footprint\n","of the operations.\n","As illustrated in Figure 1, the integration of high-bandwidth\n","memory (HBM) modules facilitated faster data access and\n","reduced bottlenecks, contributing to overall performance im-\n","provements. Energy-efficient power management techniques,\n","such as dynamic voltage and frequency scaling (DVFS), were\n","implemented to adjust the power usage of the clusters in\n","response to workload demands. Additionally, custom cooling\n","solutions were deployed to maintain optimal operating tem-\n","peratures, thereby enhancing the reliability and efficiency of\n","the clusters. The adoption of containerization technologies en-\n","abled efficient resource allocation and workload management,\n","further optimizing cluster performance. Advanced monitoring\n","systems were integrated to provide real-time insights into\n","cluster performance and energy usage, enabling more effective\n","management and optimization. The use of machine learning\n","algorithms for predictive maintenance helped to minimize\n","downtime and ensure consistent performance levels. Finally,\n","the redesigned clusters incorporated renewable energy sources,\n","reducing the overall carbon footprint of the operations.\n","1) Hardware Configurations: The hardware configurations\n","used in the redesigned clusters were meticulously selected to\n","balance performance and energy efficiency. High-performance\n","GPUs with tensor cores were employed to accelerate the\n","matrix computations integral to LLM inference. Each node\n","was equipped with multiple GPUs, interconnected through\n","high-speed NVLink to enhance data transfer rates and reduce\n","latency. The inclusion of HBM modules provided rapid access\n","to large datasets, minimizing data retrieval times and improv-\n","ing overall throughput. Energy-efficient CPUs with advanced\n","power management features were utilized to handle ancillary\n","tasks, ensuring that the primary computational power was\n","dedicated to LLM inference. Custom-designed cooling solu-\n","tions, including liquid cooling systems, were implemented to\n","maintain optimal temperatures and prevent thermal throttling.\n","The use of solid-state drives (SSDs) with high read/write\n","speeds further accelerated data access and storage operations.\n","Redundant power supplies and failover mechanisms were inte-\n","grated to ensure continuous operation and minimize downtime.\n","Finally, the clusters were configured to operate in a hybrid\n","cloud environment, leveraging both on-premises and cloud-\n","based resources to optimize performance and scalability.\n","2) Software Optimizations: Several software optimizations\n","were implemented to enhance the performance and energy\n","efficiency of the redesigned clusters. Customized deep learning\n","frameworks were developed to take full advantage of the\n","hardware configurations, enabling efficient execution of LLM\n","inference tasks. Optimized matrix multiplication libraries, such\n","as cuBLAS and cuDNN, were employed to accelerate com-\n","putational operations. Advanced scheduling algorithms were\n","used to distribute workloads evenly across the clusters, reduc-\n","ing idle times and maximizing resource utilization. Techniques\n","such as gradient checkpointing and mixed precision training\n","were applied to reduce memory usage and computational\n","overhead. The integration of intelligent caching mechanisms\n","minimized redundant computations and improved inference\n","latency. Container orchestration tools, such as Kubernetes,\n","were used to manage and scale the deployment of LLM models\n","across the clusters. Real-time performance monitoring and\n","management tools provided insights into system performance,\n","enabling continuous optimization and adjustment of resource\n","allocation. The use of automated deployment pipelines en-\n","sured that updates and patches could be applied seamlessly,\n","maintaining system stability and performance. Finally, energy-\n","efficient algorithms were implemented to adjust computational\n","loads dynamically, optimizing energy usage without compro-\n","mising performance.\n","B. Modifications to GPT-Neo\n","Modifications to the GPT-Neo model were aimed at en-\n","hancing its efficiency and performance during inference. These\n","modifications included changes to the model architecture and\n","improvements to the inference algorithms used by GPT-Neo.\n","1) Model Architecture Changes: The architecture of GPT-\n","Neo was modified to optimize its performance for the re-\n","designed inference clusters. A more efficient attention mecha-\n","nism was implemented to reduce the computational complexity\n","of processing long sequences. This mechanism enabled the\n","model to focus on relevant portions of the input data, thereby\n","improving processing speed and accuracy. The model’s depth\n","and width were adjusted to balance the trade-off between per-\n","formance and computational requirements. Techniques such as\n","layer normalization and residual connections were applied to\n","enhance the stability and efficiency of the model. Additionally,\n","the parameter initialization process was optimized to acceler-\n","ate convergence during training and reduce the computational\n","overhead during inference. The integration of a dynamic\n","routing mechanism enabled the model to adaptively allocate\n","computational resources based on the complexity of the input\n","data, further enhancing performance and efficiency.\n","2) Inference Algorithm Improvements: Several improve-\n","ments were made to the inference algorithms used by GPT-Neo\n","to enhance its efficiency and performance. As illustrated in\n","Algorithm 1, a more efficient beam search algorithm was im-\n","plemented to optimize the generation of output sequences, re-\n","ducing computational overhead and improving response times.\n","4\n","GPU 1\n","GPU 2\n","GPU 3\n","GPU 4\n","High-Bandwidth Memory\n","DVFS Module\n","Cooling System\n","Containerization\n","Monitoring System\n","Predictive Maintenance\n","Renewable Energy\n","Fig. 1. Illustration of the redesigned inference cluster architecture incorporating advanced interconnect technologies, high-bandwidth memory, DVFS modules,\n","custom cooling solutions, containerization, advanced monitoring systems, predictive maintenance, and renewable energy sources.\n","Techniques such as early stopping and dynamic batch sizing\n","were employed to minimize unnecessary computations, further\n","enhancing inference efficiency. The integration of a caching\n","mechanism allowed for the reuse of previously computed\n","results, reducing redundant computations and speeding up the\n","inference process. Additionally, the inference pipeline was\n","optimized to take full advantage of the hardware configurations\n","of the redesigned clusters, enabling faster and more efficient\n","processing of input data. Advanced load balancing algorithms\n","were used to distribute inference tasks evenly across the\n","available computational resources, maximizing throughput and\n","minimizing latency. Finally, the incorporation of model paral-\n","lelism techniques allowed for the distribution of model param-\n","eters across multiple GPUs, enabling the efficient handling of\n","larger models and datasets.\n","Algorithm 1 Enhanced Inference Algorithm for GPT-Neo\n","1: Input: X = {x1, x2, . . . , xn}\n","▷Input sequence\n","2: H ←Initialize hidden states\n","3: C ←Initialize cache\n","4: Q ←Initialize beam search queue\n","5: for t = 1 to T do\n","6:\n","A ←Attention(X, H)\n","7:\n","U ←LayerNorm(A)\n","8:\n","R ←ResidualConnect(U, H)\n","9:\n","P ←DynamicRouting(R, X)\n","10:\n","H ←UpdateHiddenStates(P)\n","11:\n","if t ∈Checkpoints then\n","12:\n","C ←Cache(H)\n","13:\n","end if\n","14:\n","O ←GenerateOutput(H)\n","15:\n","if O ∈StopCriteria then\n","16:\n","Break\n","17:\n","end if\n","18:\n","Q ←BeamSearch(O, Q)\n","19: end for\n","20: Output: Y = RetrieveBestSequence(Q)\n","C. Evaluation Metrics\n","The evaluation of the redesigned inference clusters and\n","modified GPT-Neo model was conducted using a comprehen-\n","sive set of performance and energy efficiency metrics. Table I\n","summarizes the key metrics used for this assessment.\n","1) Performance Metrics: Performance metrics were used\n","to assess the efficiency and speed of the redesigned inference\n","clusters and the modified GPT-Neo model. As summarized\n","in Table I, these metrics included throughput, measured as\n","the number of input sequences processed per second, and\n","latency, defined as the time taken to generate an output\n","sequence for a given input. Additionally, the accuracy of the\n","model’s predictions was evaluated using standard benchmarks\n","and datasets. The scalability of the clusters was assessed\n","by measuring the performance gains achieved through the\n","addition of computational resources. The robustness of the\n","model was evaluated by testing its performance under various\n","load conditions and input complexities.\n","2) Energy Efficiency Metrics: Energy efficiency metrics\n","were employed to evaluate the power consumption and envi-\n","ronmental impact of the redesigned inference clusters and the\n","modified GPT-Neo model. As detailed in Table I, these metrics\n","included the power usage effectiveness (PUE), defined as the\n","ratio of total energy consumed by the data center to the energy\n","consumed by the computational resources. Additionally, the\n","energy per inference, measured as the amount of energy\n","required to process a single input sequence, was used to\n","assess the efficiency of the inference process. The carbon\n","footprint of the operations was evaluated by calculating the\n","total greenhouse gas emissions associated with the energy\n","consumption of the clusters. The effectiveness of the energy-\n","saving techniques, such as DVFS and power gating, was\n","assessed by comparing the energy usage before and after their\n","implementation. Finally, the overall energy efficiency of the\n","clusters was evaluated by measuring the performance-to-power\n","ratio, defined as the throughput achieved per unit of energy\n","consumed.\n","5\n","TABLE I\n","EVALUATION METRICS FOR REDESIGNED INFERENCE CLUSTERS AND MODIFIED GPT-NEO\n","Category\n","Metric\n","Description\n","Performance\n","Throughput\n","Number of input sequences processed per second\n","Latency\n","Time taken to generate an output sequence for a given input\n","Accuracy\n","Evaluation of the model’s predictions using standard bench-\n","marks and datasets\n","Scalability\n","Performance gains achieved through the addition of compu-\n","tational resources\n","Robustness\n","Performance under various load conditions and input com-\n","plexities\n","Energy Efficiency\n","Power\n","Usage\n","Effectiveness\n","(PUE)\n","Ratio of total energy consumed by the data center to the\n","energy consumed by the computational resources\n","Energy per Inference\n","Amount of energy required to process a single input sequence\n","Carbon Footprint\n","Total greenhouse gas emissions associated with the energy\n","consumption of the clusters\n","Energy-Saving Techniques\n","Effectiveness of techniques such as DVFS and power gating\n","before and after implementation\n","Performance-to-Power Ratio\n","Throughput achieved per unit of energy consumed\n","IV. RESULTS\n","The results of the experiments conducted to evaluate the re-\n","designed inference clusters and the modified GPT-Neo model\n","are presented in this section. The focus is on performance\n","and energy efficiency improvements achieved through various\n","modifications and optimizations. Detailed analysis of perfor-\n","mance metrics, energy efficiency metrics, and comparisons\n","with baseline models and clusters are provided.\n","A. Performance Analysis\n","The performance analysis involved measuring throughput,\n","latency, accuracy, scalability, and robustness of the redesigned\n","inference clusters and the modified GPT-Neo model. Through-\n","put was measured as the number of input sequences processed\n","per second, while latency was defined as the time taken to gen-\n","erate an output sequence for a given input. The accuracy of the\n","model’s predictions was evaluated using standard benchmarks\n","and datasets.\n","As shown in Figure 2, the redesigned clusters achieved a\n","throughput of 2500 sequences per second when processing five\n","input sequences simultaneously, significantly outperforming\n","the baseline clusters, which achieved a maximum throughput\n","of 1500 sequences per second under the same conditions.\n","Latency measurements, illustrated in Figure 3, indicate that\n","the redesigned clusters generated output sequences with an\n","average latency of 50 milliseconds, compared to 80 millisec-\n","onds for the baseline clusters.\n","The accuracy of the modified GPT-Neo model, evaluated\n","using the standard GLUE benchmark, showed an improvement\n","of approximately 5% over the baseline model, as depicted\n","in Table II. Scalability was assessed through performance\n","gains achieved via the addition of computational resources,\n","with the redesigned clusters demonstrating a linear increase\n","in throughput and a proportional decrease in latency, thereby\n","confirming their superior scalability. Robustness was evalu-\n","ated by testing the model’s performance under various load\n","1\n","2\n","3\n","4\n","5\n","0\n","500\n","1,000\n","1,500\n","2,000\n","2,500\n","Number of Input Sequences\n","Throughput (sequences/sec)\n","Throughput Analysis\n","Redesigned Clusters\n","Baseline Clusters\n","Fig. 2.\n","Throughput analysis comparing redesigned clusters and baseline\n","clusters.\n","conditions and input complexities, revealing consistent high\n","performance across different scenarios.\n","B. Energy Efficiency Analysis\n","The energy efficiency analysis focused on metrics such\n","as power usage effectiveness (PUE), energy per inference,\n","carbon footprint, and the performance-to-power ratio. PUE\n","was defined as the ratio of total energy consumed by the data\n","center to the energy consumed by the computational resources.\n","6\n","1\n","2\n","3\n","4\n","5\n","0\n","20\n","40\n","60\n","80\n","Number of Input Sequences\n","Latency (milliseconds)\n","Latency Analysis\n","Redesigned Clusters\n","Baseline Clusters\n","Fig. 3. Latency analysis comparing redesigned clusters and baseline clusters.\n","TABLE II\n","ACCURACY ANALYSIS USING GLUE BENCHMARK\n","Task\n","Baseline Accuracy (%)\n","Modified GPT-Neo Accuracy (%)\n","CoLA\n","54.8\n","59.7\n","SST-2\n","92.5\n","95.0\n","MRPC\n","88.0\n","91.0\n","STS-B\n","89.5\n","93.2\n","QQP\n","88.5\n","91.5\n","MNLI\n","84.6\n","89.0\n","QNLI\n","90.4\n","94.2\n","RTE\n","71.8\n","77.6\n","Energy per inference measured the amount of energy required\n","to process a single input sequence.\n","As shown in Figure 4, the redesigned clusters consumed\n","22 Joules per inference when processing five input sequences\n","simultaneously, compared to 42 Joules for the baseline clus-\n","ters. The PUE of the redesigned clusters was measured at\n","1.2, indicating a highly efficient energy usage compared to\n","the baseline clusters with a PUE of 1.5. The carbon footprint\n","analysis, depicted in Table III, demonstrated a significant\n","reduction in greenhouse gas emissions for the redesigned\n","clusters.\n","The effectiveness of energy-saving techniques, such as\n","dynamic voltage and frequency scaling (DVFS) and power\n","gating, was assessed by comparing energy usage before and\n","after their implementation. The redesigned clusters demon-\n","strated a 30% reduction in energy consumption through these\n","techniques. The performance-to-power ratio, defined as the\n","throughput achieved per unit of energy consumed, was mea-\n","sured at 113 sequences per Joule for the redesigned clusters,\n","compared to 75 sequences per Joule for the baseline clusters,\n","indicating a substantial improvement in energy efficiency.\n","1\n","2\n","3\n","4\n","5\n","0\n","10\n","20\n","30\n","40\n","50\n","Number of Input Sequences\n","Energy per Inference (Joules)\n","Energy per Inference Analysis\n","Redesigned Clusters\n","Baseline Clusters\n","Fig. 4.\n","Energy per inference analysis comparing redesigned clusters and\n","baseline clusters.\n","C. Comparison with Baseline\n","Comparisons with baseline models and clusters were con-\n","ducted to highlight the improvements achieved through the\n","redesign and modifications. The redesigned clusters and mod-\n","ified GPT-Neo model consistently outperformed the baseline in\n","all evaluated metrics, including throughput, latency, accuracy,\n","scalability, robustness, PUE, energy per inference, carbon\n","footprint, and performance-to-power ratio.\n","Figure 5 illustrates the performance improvement percent-\n","ages over the baseline clusters and models. The redesigned\n","clusters exhibited a 66% improvement in throughput and a\n","37.5% reduction in latency. Accuracy improvements, though\n","modest at 5%, were significant in the context of language\n","model tasks. Scalability and robustness improvements were\n","20% and 15%, respectively, indicating the effectiveness of the\n","redesigned cluster architecture and modified model in handling\n","various load conditions. Energy efficiency metrics showed a\n","20% improvement in PUE, a 47.6% reduction in energy per\n","inference, and a 50.6% increase in the performance-to-power\n","ratio. The results of the experiments conclusively demonstrate\n","the benefits of redesigning inference clusters and modifying\n","the GPT-Neo model in terms of both performance and energy\n","efficiency. These improvements underscore the potential of\n","advanced cluster architectures and optimized model configu-\n","rations in enhancing the capabilities of large language models\n","for real-world applications.\n","V. DISCUSSION\n","The discussion section provides an in-depth analysis of\n","the implications of the results obtained, acknowledges the\n","potential limitations of the study, and suggests directions\n","7\n","TABLE III\n","CARBON FOOTPRINT ANALYSIS\n","Metric\n","Baseline Clusters\n","Redesigned Clusters\n","Total Energy Consumption (MWh)\n","1200\n","800\n","Greenhouse Gas Emissions (Metric Tons CO2)\n","600\n","400\n","Throughput\n","Latency\n","Accuracy\n","Scalability\n","Robustness\n","PUE\n","Energy per Inference\n","Performance-to-Power Ratio\n","0\n","20\n","40\n","60\n","Metric\n","Performance Improvement (%)\n","Performance Improvement over Baseline\n","Fig. 5.\n","Performance improvement percentages over baseline clusters and\n","models.\n","for future research. Through a detailed examination of the\n","findings, this section aims to contextualize the significance of\n","the research within the broader field of large language model\n","optimization.\n","A. Broader Impact of Findings\n","The findings from the redesign of inference clusters and\n","modifications to the GPT-Neo model have far-reaching im-\n","plications for the field of natural language processing. The\n","significant improvements in throughput and latency highlight\n","the potential for more efficient real-time processing of large\n","volumes of data, which is critical for applications such as\n","automated customer support, real-time translation services,\n","and large-scale text analysis. The enhancements in accuracy,\n","although modest, suggest that even small architectural and\n","algorithmic modifications can yield meaningful improvements\n","in performance, thereby enhancing the overall reliability and\n","effectiveness of language models. Furthermore, the demon-\n","strated scalability and robustness indicate that the redesigned\n","clusters and modified model can handle varying loads and\n","complex input scenarios, making them suitable for deploy-\n","ment in diverse operational environments. The reduction in\n","energy consumption and carbon footprint underscores the\n","environmental benefits of optimizing computational resources,\n","aligning with global efforts to develop more sustainable AI\n","technologies. These findings collectively contribute to the\n","ongoing discourse on balancing performance with energy\n","efficiency in the development and deployment of large-scale\n","AI systems.\n","B. Constraints and Limitations\n","Despite the promising results, the study is not without its\n","limitations. One primary limitation pertains to the scope of\n","the evaluation, which focused on specific performance and\n","energy efficiency metrics. While comprehensive, the study\n","did not account for other potential factors such as the long-\n","term operational stability of the redesigned clusters and the\n","potential impact of hardware failures or network disruptions.\n","Additionally, the modifications made to the GPT-Neo model,\n","while effective, were limited to architectural and algorithmic\n","changes; further improvements could potentially be achieved\n","through more extensive hyperparameter tuning and exploration\n","of alternative optimization techniques. The experimental setup,\n","although rigorous, was constrained by the availability of\n","computational resources, which may limit the generalizability\n","of the results to different hardware configurations and oper-\n","ational environments. Furthermore, the reliance on standard\n","benchmarks and datasets, while necessary for consistency, may\n","not fully capture the diverse range of real-world scenarios in\n","which the modified model and clusters would be deployed.\n","These limitations should be considered when interpreting the\n","results and their broader applicability.\n","C. Prospects for Future Exploration\n","Future research should build on the findings of this study\n","to explore additional avenues for optimizing large language\n","models and their underlying computational infrastructure. One\n","potential direction involves investigating the impact of ad-\n","vanced hyperparameter tuning techniques on the performance\n","and efficiency of the modified GPT-Neo model. Exploring\n","the integration of novel hardware accelerators, such as tensor\n","processing units (TPUs) and application-specific integrated\n","circuits (ASICs), could further enhance the performance and\n","energy efficiency of inference clusters. Additionally, research\n","could examine the potential benefits of incorporating more\n","sophisticated cooling solutions and advanced power manage-\n","ment strategies to further reduce energy consumption. Another\n","promising area of exploration involves the application of fed-\n","erated learning and distributed training techniques to optimize\n","the training and inference processes across geographically\n","8\n","dispersed data centers. Finally, future studies should consider\n","a broader range of evaluation metrics, including operational\n","stability, fault tolerance, and adaptability to dynamic network\n","conditions, to provide a more holistic assessment of the\n","optimized clusters and model.\n","VI. CONCLUSION\n","The research conducted on the redesign of inference clusters\n","and the modifications made to the GPT-Neo model has yielded\n","significant insights and advancements in the field of natural\n","language processing. The comprehensive analysis presented\n","in this article highlights the substantial improvements in per-\n","formance and energy efficiency achieved through strategic ar-\n","chitectural and algorithmic changes. This section summarizes\n","the key findings and contributions of the research, providing\n","final thoughts and reflections on the broader implications of\n","the work.\n","A. Summary of Contributions\n","The primary contributions of this research include the\n","development of a novel cluster architecture that incorporates\n","advanced interconnect technologies, high-bandwidth memory\n","modules, and energy-efficient power management techniques.\n","These innovations have led to significant reductions in latency\n","and communication overhead, thereby enhancing the overall\n","performance of the inference clusters. The modifications to\n","the GPT-Neo model, including the implementation of a more\n","efficient attention mechanism, dynamic routing, and optimized\n","parameter initialization, have further improved the model’s\n","processing speed and accuracy. The integration of software\n","optimizations, such as advanced scheduling algorithms, in-\n","telligent caching, and real-time performance monitoring, has\n","enabled more efficient resource allocation and workload man-\n","agement. The empirical evaluation conducted using compre-\n","hensive performance and energy efficiency metrics has demon-\n","strated the superior scalability, robustness, and environmental\n","sustainability of the redesigned clusters and modified model.\n","These contributions collectively advance the state of the art\n","in large language model optimization, providing a robust\n","framework for future research and development in this domain.\n","B. Final Remarks\n","The findings of this research underscore the critical impor-\n","tance of balancing performance with energy efficiency in the\n","development and deployment of large-scale AI systems. The\n","demonstrated improvements in throughput, latency, and energy\n","consumption highlight the potential for more sustainable and\n","efficient natural language processing solutions. The broader\n","implications of this work extend beyond technical advance-\n","ments, emphasizing the necessity of integrating environmental\n","considerations into the design of computational infrastructure.\n","By continuing to refine and expand upon the optimizations\n","presented in this article, researchers and practitioners can con-\n","tribute to the development of AI technologies that are not only\n","more powerful but also more environmentally responsible. The\n","research provides a solid foundation for future exploration,\n","encouraging further innovation and collaboration within the\n","AI research community. Through ongoing efforts to optimize\n","inference clusters and language models, the field can achieve\n","significant strides toward more sustainable and effective AI\n","solutions, ultimately benefiting a wide range of applications\n","and industries.\n","REFERENCES\n","[1] G. Choquet, A. Aizier, and G. Bernollin, “Exploiting privacy vulnera-\n","bilities in open source llms using maliciously crafted prompts,” 2024.\n","[2] D. Fares, “The role of large language models (llms) driven chatbots\n","in shaping the future of government services and communication with\n","citizens in uae,” 2023.\n","[3] L. Danas, “Security and interpretability in large language models,” 2024.\n","[4] J. Kundu, W. Guo, A. BanaGozar, U. De Alwis, S. Sengupta, P. Gupta,\n","and A. Mallik, “Performance modeling and workload analysis of dis-\n","tributed large language model training and inference,” arXiv preprint\n","arXiv:2407.14645, 2024.\n","[5] R. Fredheim, “Virtual manipulation brief 2023/1: Generative ai and its\n","implications for social media analysis,” 2023.\n","[6] J. Owens and S. Matthews, “Efficient large language model inference\n","with vectorized floating point calculations,” 2024.\n","[7] A. Laverghetta Jr, “A psychometric analysis of natural language infer-\n","ence using transformer language models,” 2023.\n","[8] D. De Bari, “Evaluating large language models in software design: A\n","comparative analysis of uml class diagram generation,” 2024.\n","[9] S. R. Cunningham, D. Archambault, and A. Kung, “Efficient training\n","and inference: Techniques for large language models using llama,” 2024.\n","[10] K. V. Day, “Training a massively multimodal transformer on youtube\n","data: pre-training and parameter efficient fine-tuning on hpc infrastruc-\n","ture,” 2023.\n","[11] F. Liang, Z. Zhang, H. Lu, C. Li, V. Leung, Y. Guo, and X. Hu,\n","“Resource allocation and workload scheduling for large-scale distributed\n","deep learning: A survey,” arXiv preprint arXiv:2406.08115, 2024.\n","[12] F. Dall’Agata, “Instructing network devices via large language models,”\n","2024.\n","[13] M. Jakesch, Assessing the Effects and Risks of Large Language Models\n","in AI-Mediated Communication.\n","Cornell University, 2022.\n","[14] C. Donner, “Misinformation detection methods using large language\n","models and evaluation of application programming interfaces,” 2024.\n","[15] T. R. McIntosh, T. Susnjak, T. Liu, P. Watters, and M. N. Halgamuge,\n","“From google gemini to openai q*(q-star): A survey of reshaping the\n","generative artificial intelligence (ai) research landscape,” arXiv preprint\n","arXiv:2312.10868, 2023.\n","[16] S.-h. Huang and C.-y. Chen, “Combining lora to gpt-neo to reduce large\n","language model hallucination,” 2024.\n","[17] Z. Du and K. Hashimoto, “Exploring sentence-level revision capabilities\n","of llms in english for academic purposes writing assistance,” 2024.\n","[18] E. Vaillancourt and C. Thompson, “Instruction tuning on large language\n","models to improve reasoning performance,” 2024.\n","[19] A. Gundogmusler, F. Bayindiroglu, and M. Karakucukoglu, “Mathemat-\n","ical foundations of hallucination in transformer-based large language\n","models for improvisation,” 2024.\n","[20] T. Goto, K. Ono, and A. Morita, “A comparative analysis of large\n","language models to evaluate robustness and reliability in adversarial\n","conditions,” 2024.\n","[21] M. Huang, A. Shen, K. Li, H. Peng, B. Li, and H. Yu, “Edgellm:\n","A highly efficient cpu-fpga heterogeneous edge accelerator for large\n","language models,” arXiv preprint arXiv:2407.21325, 2024.\n","[22] H. Gupta, “Instruction tuned models are quick learners with instruction\n","equipped data on downstream tasks,” 2023.\n","[23] T. Hata and R. Aono, “Dynamic attention seeking to address the\n","challenge of named entity recognition of large language models,” 2024.\n","[24] J. Stojkovic, C. Zhang, ´I. Goiri, J. Torrellas, and E. Choukse, “Dy-\n","namollm: Designing llm inference clusters for performance and energy\n","efficiency,” arXiv preprint arXiv:2408.00741, 2024.\n","[25] H. Fujiwara, R. Kimura, and T. Nakano, “Modify mistral large perfor-\n","mance with low-rank adaptation (lora) on the big-bench dataset,” 2024.\n","[26] K. Fujiwara, M. Sasaki, A. Nakamura, and N. Watanabe, “Measuring\n","the interpretability and explainability of model decisions of five large\n","language models,” 2024.\n","[27] K. Dave, “Adversarial privacy auditing of synthetically generated data\n","produced by large language models using the tapas toolbox,” 2024.\n","9\n","[28] G. Fazlija, “Toward optimising a retrieval augmented generation pipeline\n","using large language model,” 2024.\n","[29] A. Katal, S. Dahiya, and T. Choudhury, “Energy efficiency in cloud\n","computing data centers: a survey on software technologies,” Cluster\n","Computing, vol. 26, no. 3, pp. 1845–1875, 2023.\n","[30] T. Dyde, “Documentation on the emergence, current iterations, and\n","possible future of artificial intelligence with a focus on large language\n","models,” 2023.\n",". \n","Answer:\n","\n","\n","\n","\n","RESPONSE:\n","\n","\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 21.75 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.49 GiB is free. Process 181900 has 12.25 GiB memory in use. Of the allocated memory 11.48 GiB is allocated by PyTorch, and 656.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2462038207.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RESPONSE:\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_a_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1228882785.py\u001b[0m in \u001b[0;36mmake_a_query\u001b[0;34m(prompt, tokenizer, model)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Generate a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     output = model.generate(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3263\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_running_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3265\u001b[0;31m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3267\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 459\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 21.75 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.49 GiB is free. Process 181900 has 12.25 GiB memory in use. Of the allocated memory 11.48 GiB is allocated by PyTorch, and 656.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["#now the same for the augmented one\n","\n","\n","p_augmented =  \"\"\"{PROMPT} \\n\\nCollect information from the following article: {BODY}. \\nAnswer:\"\"\".format(BODY=concatenated_documents, PROMPT=query_for_rag)\n","\n","messages = [\n","    {'role': 'system', 'content': new_system_prompt},\n","    {'role': 'user', 'content': p_augmented}\n","]\n","prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","print(\"PROMPT:\\n\\n\")\n","print(p_augmented)\n","\n","print(\"\\n\\n\\n\")\n","\n","print(\"RESPONSE:\\n\\n\")\n","print(make_a_query(prompt, tokenizer, model))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4fae9c86e7324e0694532f1af804a478":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa5681a285b6407190ec9f71fac20a37","IPY_MODEL_c39d0e0b9d024ab5ac7f5f37939d6e76","IPY_MODEL_342d2a1dc1184dc893d781b4c1d7091c"],"layout":"IPY_MODEL_3886c889d3bf48fd9f6e04dad567a99b"}},"fa5681a285b6407190ec9f71fac20a37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ddb2c259f964acbb0cabe50f4c59b8a","placeholder":"​","style":"IPY_MODEL_c84e098203e94171afb25db9519a8885","value":"Loading checkpoint shards: 100%"}},"c39d0e0b9d024ab5ac7f5f37939d6e76":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_551c225a76724aa09291f1a086026a61","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_34db60b320244dda959a3a8009d4a9c0","value":2}},"342d2a1dc1184dc893d781b4c1d7091c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18ecbd12e21b46e3b03991b2becd82cc","placeholder":"​","style":"IPY_MODEL_756f796ddc704f6c96c4fd044a54a2e3","value":" 2/2 [00:52&lt;00:00, 23.58s/it]"}},"3886c889d3bf48fd9f6e04dad567a99b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ddb2c259f964acbb0cabe50f4c59b8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c84e098203e94171afb25db9519a8885":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"551c225a76724aa09291f1a086026a61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34db60b320244dda959a3a8009d4a9c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"18ecbd12e21b46e3b03991b2becd82cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"756f796ddc704f6c96c4fd044a54a2e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6cb40c1c7e0042a3a4e19605be2a6727":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45b803a0b306447289a4800bbf501b2b","IPY_MODEL_eafd4da2cc8e4fc0adcbfd285892a2f2","IPY_MODEL_44f55deff78f4b9caa641d35ea39eabd"],"layout":"IPY_MODEL_64539f9d562548a2b8404d33767369b0"}},"45b803a0b306447289a4800bbf501b2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3d9b170e38d42ad9b94a058d276a6d9","placeholder":"​","style":"IPY_MODEL_431d5de00ad94ef28856add47dc9cc9a","value":"Loading checkpoint shards: 100%"}},"eafd4da2cc8e4fc0adcbfd285892a2f2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_26fab9b95dc34b71ad86e69dc369413e","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fe7773f2ef02480c802098c35a54cee3","value":2}},"44f55deff78f4b9caa641d35ea39eabd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5123a2fce010408ea18bf81aaa57e559","placeholder":"​","style":"IPY_MODEL_aae139a108ba485d8ec9b6567797746f","value":" 2/2 [00:43&lt;00:00, 19.93s/it]"}},"64539f9d562548a2b8404d33767369b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3d9b170e38d42ad9b94a058d276a6d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"431d5de00ad94ef28856add47dc9cc9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26fab9b95dc34b71ad86e69dc369413e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe7773f2ef02480c802098c35a54cee3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5123a2fce010408ea18bf81aaa57e559":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aae139a108ba485d8ec9b6567797746f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e7bcdbda90d4ed38c2cfa377fde31ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0429ea9993734b4eb634ed89722b6a58","IPY_MODEL_fdf787bba4d1448d976d8545ddeb568f","IPY_MODEL_57735f2c2fd14e4493c87a9650c74744"],"layout":"IPY_MODEL_75e00fbc905743ae881e1a1b08d172e8"}},"0429ea9993734b4eb634ed89722b6a58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12144838327149b6be72319361563f3d","placeholder":"​","style":"IPY_MODEL_a18034c0c1094d21b21a2cb2b7722582","value":"tokenizer_config.json: 100%"}},"fdf787bba4d1448d976d8545ddeb568f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_654e36d993a54b909dfeebe1c149c998","max":54528,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eabbcea1ad7c4577a25fd717b3c27596","value":54528}},"57735f2c2fd14e4493c87a9650c74744":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7aca10f27cb5402f834e9cae8d844a71","placeholder":"​","style":"IPY_MODEL_0b3b072de29e47aea3e82002f8a142b8","value":" 54.5k/54.5k [00:00&lt;00:00, 2.18MB/s]"}},"75e00fbc905743ae881e1a1b08d172e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12144838327149b6be72319361563f3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a18034c0c1094d21b21a2cb2b7722582":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"654e36d993a54b909dfeebe1c149c998":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eabbcea1ad7c4577a25fd717b3c27596":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7aca10f27cb5402f834e9cae8d844a71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b3b072de29e47aea3e82002f8a142b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"313411d94e7c40358d4a14a5e4737751":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f42d4a5ef9e54342bac76283d11c6c61","IPY_MODEL_20dfaeb1f1a54119b187c16086c9faee","IPY_MODEL_cf4bbc9d22774447b66c3a24afd7a058"],"layout":"IPY_MODEL_04e48f117e8447d6bf0bb90a6282d036"}},"f42d4a5ef9e54342bac76283d11c6c61":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7739c1138b0464e99d1e0aa1ad4954a","placeholder":"​","style":"IPY_MODEL_00c1b112af6b49c488f58748c0bf5e7c","value":"tokenizer.json: 100%"}},"20dfaeb1f1a54119b187c16086c9faee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1086cfc799694513964790669fdc2f4c","max":9085657,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7a6863a529b424d80f33d8b6c05ea1c","value":9085657}},"cf4bbc9d22774447b66c3a24afd7a058":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a5b7e23e80543deadfdd9a6d99517c1","placeholder":"​","style":"IPY_MODEL_eb92cbd4dd254824b8331f6fcb53fdd4","value":" 9.09M/9.09M [00:00&lt;00:00, 15.6MB/s]"}},"04e48f117e8447d6bf0bb90a6282d036":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7739c1138b0464e99d1e0aa1ad4954a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00c1b112af6b49c488f58748c0bf5e7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1086cfc799694513964790669fdc2f4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7a6863a529b424d80f33d8b6c05ea1c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9a5b7e23e80543deadfdd9a6d99517c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb92cbd4dd254824b8331f6fcb53fdd4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40c1c6020e2548609a3c1f3df2444bae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b8982135967434dbbf7d7e88866e958","IPY_MODEL_ac8933ac8d3c4bc0bd08aa4468a3d88e","IPY_MODEL_8bf52a0a1d7c445fbaa542d4af6f7cf8"],"layout":"IPY_MODEL_b89ce04db22f4221b0af786fcd853cee"}},"3b8982135967434dbbf7d7e88866e958":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1622ab807630437783b08b7d070256a8","placeholder":"​","style":"IPY_MODEL_074f8bd803ea4c01ba5c2c2cc16e79e7","value":"special_tokens_map.json: 100%"}},"ac8933ac8d3c4bc0bd08aa4468a3d88e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4c0812c66a64358ba03b339f89619ca","max":296,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9af3ad1405c144508385a7a89c37966b","value":296}},"8bf52a0a1d7c445fbaa542d4af6f7cf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5910390d38e4574a973843528d4771d","placeholder":"​","style":"IPY_MODEL_4ed46d8881cd4b539a4f7277a02e3da6","value":" 296/296 [00:00&lt;00:00, 7.51kB/s]"}},"b89ce04db22f4221b0af786fcd853cee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1622ab807630437783b08b7d070256a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"074f8bd803ea4c01ba5c2c2cc16e79e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4c0812c66a64358ba03b339f89619ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9af3ad1405c144508385a7a89c37966b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b5910390d38e4574a973843528d4771d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ed46d8881cd4b539a4f7277a02e3da6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}