{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 01 - Large Language Models**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lab 01:** Introduction to Deep Learning Models, and their Training, Fine-Tuning, and Evaluation\n",
    "\n",
    "In this lab, we will explore the basic functionality of PyTorch, focusing on key operations such as data loading, data transformations, and using GPUs for acceleration. You will be introduced to fundamental components like **Tensors**, **DataLoaders**, and **Neural Networks** that are essential for building and training deep learning models. \n",
    "\n",
    "The lab is divided into several exercises, each designed to provide hands-on experience with different PyTorch features. You will begin by creating a very simple neural network on a very simple problem, with all that is involved (i.e., model creation, dataset and data loader creation, training loop). Then you will move on to applying transformations to the **CIFAR-10 dataset**, creating a **DataLoader**, and moving computations to a GPU (if available). As you progress through the exercises, you will deepen your understanding of PyTorch's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors\n",
    "**Tensors** in PyTorch Tensors are the primary data structure in PyTorch. They are similar to NumPy arrays but have added functionality that supports GPU acceleration and automatic differentiation (*autograd*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Perform basic tensor operations\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "c = a + b  # Element-wise addition\n",
    "print(f\"Tensor c: {c}\")\n",
    "\n",
    "# Check tensor's shape and type\n",
    "print(f\"Shape of tensor c: {c.shape}\")\n",
    "print(f\"Data type of tensor c: {c.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that torch tensors use, by default, float32 (single precision) data type. NumPy, instead, defaults to float64 (double precision).\n",
    "\n",
    "This happens for performance-related reasons: float32 operations are faster than float64, as the following code snippet shows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "mat_size = 1000\n",
    "\n",
    "M1_64 = torch.randn(mat_size, mat_size, dtype=torch.float64)\n",
    "M2_64 = torch.randn(mat_size, mat_size, dtype=torch.float64)\n",
    "\n",
    "M1_32 = torch.randn(mat_size, mat_size)\n",
    "M2_32 = torch.randn(mat_size, mat_size)\n",
    "\n",
    "t64 = timeit(lambda: M1_64 @ M2_64, number=100)\n",
    "t32 = timeit(lambda: M1_32 @ M2_32, number=100)\n",
    "\n",
    "print(f\"Time for matrix multiplication (float64): {t64:.4f}s\")\n",
    "print(f\"Time for matrix multiplication (float32): {t32:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional precision obtained with float64 is not typically necessary. \n",
    "\n",
    "If strictly needed, you can change the data type of a tensor using the .to() method. For example, to convert tensor c to float64 (double precision), you can use the following code:\n",
    "\n",
    "```python\n",
    "c = c.to(torch.float64) # or, \n",
    "c = c.double()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 1: A simple neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 Building and Understanding the Simple Linear Model**\n",
    "\n",
    "In this exercise, we implement a simple linear model using PyTorch to understand the foundational concepts of model parameters, including **weights** and **biases**. A univariate linear model is one of the most basic forms of machine learning models and is defined by the equation $y = wx + b$\n",
    "\n",
    "where:\n",
    "- $x$ is the input feature,\n",
    "- $w$ is the weight,\n",
    "- $b$ is the bias,\n",
    "- $y$ is the output or prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Definition: SimpleLinearModel**\n",
    "\n",
    "The `SimpleLinearModel` class is a subclass of `nn.Module`, which is the base class for all neural network modules in PyTorch. This class defines a single linear layer using `nn.Linear`, which performs the linear transformation:\n",
    "\n",
    "`output = input * w + b`\n",
    "\n",
    "The `SimpleLinearModel` class is structured as follows:\n",
    "\n",
    "- **`__init__(self, input_size, output_size)`**:\n",
    "  - This is the constructor method where the linear layer is defined. The `input_size` and `output_size` parameters specify the dimensions of the input and output features, respectively. In this exercise, both the input and output sizes are set to 1, indicating a single feature input and a single prediction output.\n",
    "\n",
    "- **`forward(self, x)`**:\n",
    "  - This method defines the forward pass of the model, where the input `x` is passed through the linear layer. The forward pass computes the output by applying the linear transformation described above.\n",
    "\n",
    "This simple model is trained on a synthetic dataset to learn the relationship between the input features and the target labels. During training, the model iteratively adjusts its weights `w` and bias `b` to minimize the error between the predicted and actual outputs. After training, we can inspect these learned parameters to understand how the model has adapted to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple linear model\n",
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = SimpleLinearModel(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `torch.device` for GPU Acceleration\n",
    "\n",
    "PyTorch provides seamless support for running operations on both CPUs and GPUs using the `torch.device` object. By leveraging the computational power of GPUs, we can significantly speed up the training process of deep learning models, especially when working with large datasets and complex networks.\n",
    "\n",
    "In this example, we first check if a GPU is available on the machine. In particular, we check if CUDA is available: CUDA is a software layer that provides an interface to the underlying hardware (GPU). If CUDA (and, as a consequence, a GPU) is detected, the tensors are moved to the GPU memory to accelerate computations. Otherwise, the computations fall back to the CPU (i.e., the tensor are kept on the main memory). This flexibility allows the same code to run efficiently on both CPU-only systems and systems equipped with GPUs.\n",
    "\n",
    "The `torch.device` method is a convenient way to handle this device-agnostic approach, ensuring that tensors and models are placed on the correct device before training or inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"The device is set to: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can move the model to the currently selected device (either cuda, or cpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Note that the model has a `linear` attribute (which we created). This layer has its own attributes: `weight` and `bias`. These are the parameters that the model will learn during training.\n",
    "\n",
    "The initial values are randomly generated, and the model will adjust them during training to minimize the loss function.\n",
    "\n",
    "Finally, note that these tensors have the `requires_grad` attribute set to `True`. This attribute is used by PyTorch to determine which tensors should have their gradients computed during the backward pass (i.e., which tensors are learnable parameters). This is used by PyTorch to keep track of the computational graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weight\", model.linear.weight)\n",
    "print(\"Bias\", model.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criterion and Optimizer\n",
    "\n",
    "In deep learning, two crucial components used during the training process of a model are the **criterion** and the **optimizer**. Understanding these components is essential for effectively training and fine-tuning neural networks.\n",
    "\n",
    "##### Criterion (Loss Function)\n",
    "\n",
    "The **criterion**, also known as the **loss function**, is a measure of how well the model's predictions match the actual target values. During training, the loss function evaluates the difference between the predicted outputs and the true labels, quantifying the error of the model. The goal of training is to minimize this error, thereby improving the model's accuracy. \n",
    "\n",
    "Common loss functions include:\n",
    "\n",
    "- **Cross Entropy**: Used for multi-class classification problems, like our CIFAR-10 and CIFAR-100 tasks. We typically use PyTorch's `CrossEntropyLoss`, which takes as input the predicted *logits* (not the probabilities, so unnormalized probabilities that have not been passed through a softmax, is applied) and the ground truths. Alternatively, we could combine `LogSoftmax` and `NLLLoss` (Negative Log Likelihood Loss) together, or applying the softmax ourselves and then using a Negative Log Likelihood function. However, for numerical reasons, we generally prefer using the `CrossEntropyLoss` class directly. \n",
    "- **Mean Squared Error (MSE)**: Typically used for regression tasks, where the model predicts continuous values.\n",
    "\n",
    "In this exercise, we use a Mean Squared Error loss, since the task involves predicting a continuous value.\n",
    "\n",
    "##### Optimizer\n",
    "\n",
    "The **optimizer** is an algorithm or method used to adjust the model's weights to minimize the loss function. It updates the model parameters (weights and biases) based on the gradients computed during backpropagation. The optimizer aims to find the optimal set of parameters that reduce the loss, improving the model’s performance on the training data.\n",
    "\n",
    "Some common optimizers include:\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD)**: A basic optimizer that updates the model parameters using a small, randomly selected subset of data (mini-batch) instead of the entire dataset. It’s useful for handling large datasets and reducing computational cost.\n",
    "- **Adam**: A more advanced optimizer that combines the benefits of two other extensions of SGD, Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). It adjusts the learning rate for each parameter dynamically, making it well-suited for complex tasks with sparse gradients.\n",
    "\n",
    "In this exercise, we use **Stochastic Gradient Descent (SGD)** as our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning tasks, the `DataLoader` and *datasets* provided by PyTorch are essential tools for efficiently handling large datasets and batching input data for model training. \n",
    "\n",
    "If we have a \"traditional\" dataset (e.g., an X matrix and a y vector), we can use the `TensorDataset` class to wrap the input and target tensors. This class allows us to access a slice of the dataset using the index notation (e.g., `dataset[0:10]`).\n",
    "\n",
    "The `DataLoader` class is used to load data from a `Dataset` object. It provides features such as shuffling, batching, and parallel data loading, making it easier to iterate over the dataset during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset (for simplicity, we use a small synthetic dataset)\n",
    "# X is the input features, y is the target labels\n",
    "\n",
    "n_pts = 2048\n",
    "# TODO: Generate a random dataset containing n_pts samples\n",
    "# Notes:\n",
    "# - The input features X should be a tensor of shape (n_pts, 1)\n",
    "# - you can generate the X's randomly (e.g. using torch.rand to sample uniformly from [0,1], or torch.randn to sample from a normal distribution)\n",
    "# - The target labels y should be a linear function of X with some noise added, y = w*x + b + gaussian noise\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "trainloader = DataLoader(dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "The **training loop** is the core component of the training process in deep learning. It involves repeatedly passing the training data through the model, computing the loss, and updating the model's parameters to minimize the loss. Understanding the training loop is essential for effectively training neural networks and fine-tuning models to achieve high performance.\n",
    "\n",
    "The training loop typically consists of several key steps that are executed for a number of **epochs** (iterations over the entire dataset). Let's break down the principal functions of the training loop:\n",
    "\n",
    "1. **Forward Pass**\n",
    "\n",
    "   During the forward pass, the input data is fed through the neural network to produce predictions. These predictions are compared to the actual labels using the **criterion** (loss function) to compute the loss. This step involves:\n",
    "   \n",
    "   - Passing the input data to the model.\n",
    "   - Computing the model's output (predictions).\n",
    "   - Calculating the loss by comparing the model’s predictions to the true labels.\n",
    "\n",
    "2. **Loss Computation**\n",
    "\n",
    "   The computed loss indicates how well or poorly the model is performing on the training data. The goal is to minimize this loss, so it acts as a guide for the optimizer to adjust the model's weights. The loss function provides a differentiable quantity that the optimizer can use to update the model parameters.\n",
    "\n",
    "3. **Backward Pass (Backpropagation)**\n",
    "\n",
    "   The backward pass, also known as **backpropagation**, is where the gradients of the loss with respect to each model parameter are calculated. These gradients indicate how much each parameter contributes to the loss, and they are used to update the model's parameters in the direction that reduces the loss:\n",
    "   \n",
    "   - Compute the gradients by performing backpropagation.\n",
    "   - These gradients are stored for use by the optimizer.\n",
    "\n",
    "4. **Parameter Update**\n",
    "\n",
    "   After computing the gradients, the **optimizer** uses them to update the model's parameters (weights and biases). This step adjusts the parameters to minimize the loss function. The optimizer’s update rule determines how the parameters are changed based on the gradients, the learning rate, and potentially other factors (like momentum or weight decays). This involves:\n",
    "   \n",
    "   - Using the optimizer to adjust the model's parameters based on the computed gradients.\n",
    "   - Updating the weights to reduce the loss on the next forward pass.\n",
    "\n",
    "5. **Zero the Gradients**\n",
    "\n",
    "   After updating the parameters, the gradients need to be zeroed before the next iteration. This step ensures that gradients from the previous batch do not accumulate with those from the current batch, which could lead to incorrect updates:\n",
    "   \n",
    "   - Set the gradients to zero using `optimizer.zero_grad()` to prevent accumulation from previous batches.\n",
    "\n",
    "6. **Repeat for All Batches and Epochs**\n",
    "\n",
    "   The above steps are repeated for each batch in the dataset and for each epoch until the model converges to an optimal solution or reaches the desired number of epochs. Over multiple epochs, the model learns to generalize better to the training data and, ideally, to new, unseen data.\n",
    "\n",
    "In summary, the training loop is a repetitive process that allows the model to learn from the data by minimizing the loss through gradient-based optimization. Understanding each function within the loop is crucial for successfully training deep learning models and fine-tuning them for specific tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "\n",
    "# TODO: keep track of the loss computed at each step, \n",
    "# the current value for the weight of the linear layer,\n",
    "# and the current bias value for the linear layer.\n",
    "# Note: store the values in the lists losses, weights, biases\n",
    "losses = ...\n",
    "weights = ...\n",
    "biases = ...\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 3))\n",
    "\n",
    "ax[0].plot(losses)\n",
    "ax[0].set_title(\"Loss\")\n",
    "ax[0].set_xlabel(\"Steps\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(weights)\n",
    "ax[1].set_title(\"Weight\")\n",
    "ax[1].set_xlabel(\"Steps\")\n",
    "ax[1].set_ylabel(\"Weight\")\n",
    "ax[1].grid()\n",
    "\n",
    "ax[2].plot(biases)\n",
    "ax[2].set_title(\"Bias\")\n",
    "ax[2].set_xlabel(\"Steps\")\n",
    "ax[2].set_ylabel(\"Bias\")\n",
    "ax[2].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the learned weights and biases\n",
    "weights = model.linear.weight.data\n",
    "biases = model.linear.bias.data\n",
    "\n",
    "print(f'Learned weights: {weights}')\n",
    "print(f'Learned biases: {biases}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Exercise 2: First Model Training**\n",
    "\n",
    "In this exercise, you will explore the concept of **transfer learning** by training a model on the **CIFAR-10 dataset** and then adapting it to a different dataset, **CIFAR-100**.\n",
    "\n",
    "Transfer learning is a powerful technique in deep learning where a model developed for a particular task is reused as the starting point for a model on a second task. This approach is especially useful in fields like **Natural Language Processing (NLP)**, where training a model from scratch requires immense computational resources and vast amounts of data. Instead, pre-trained models are often used as backbones, and only the final classification head is fine-tuned for specific tasks. This is also sometimes done for **Large Language Models (LLMs)**, although we often go a step further and not even fine-tune models, but use them with *in-context learning*. This, however, is a topic for later time.\n",
    "\n",
    "In this exercise, you will see how transfer learning works in practice using image datasets, but the principles you'll learn are directly applicable to NLP and LLMs, which are a major focus of this course. Understanding how to fine-tune pre-trained models is crucial because it's a common technique used to adapt powerful language models to a wide range of applications, such as sentiment analysis, text classification, or question answering.\n",
    "\n",
    "The **CIFAR-10** and **CIFAR-100** datasets are both standard datasets in computer vision. They contain 32x32 color images, but while CIFAR-10 has 10 classes, CIFAR-100 has 100 classes, making it a more challenging task.\n",
    "\n",
    "In this exercise, you will:\n",
    "1. **Train a model from scratch on CIFAR-10** and evaluate its performance.\n",
    "2. **Compare the performance on CIFAR-10** of two models:\n",
    "   - The model trained from scratch from point 1.\n",
    "   - A model pre-trained on ImageNet (another dataset of images) and then fine-tuned on CIFAR-10.\n",
    "3. **Modify the classification head** to adapt the model for CIFAR-100.\n",
    "\n",
    "By the end of this exercise, you will have a deeper understanding of the benefits of transfer learning and how pre-training can significantly improve the performance of deep learning models, especially in NLP and with LLMs, on new tasks with limited data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Loading the CIFAR-10 Dataset and Creating a DataLoader\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 testing images.\n",
    "\n",
    "The process starts by downloading the dataset. Torchvision support the CIFAR-10 dataset, so we can use the `CIFAR10` class to download the dataset directly. \n",
    "\n",
    "We apply a simple transformation to the images, which start out as PIL images ([PIL](https://pypi.org/project/pillow/) is a famous Python libraries that handles images), to convert them into PyTorch tensors. We do this using the `transforms.ToTensor()` transformation.\n",
    "\n",
    "Next, we will apply various transformations that will be shown to be useful for the learning task. These transformations help prepare the data for training by normalizing and augmenting the images.\n",
    "\n",
    "After that, we will use a `DataLoader`, which allows us to efficiently load batches of data and shuffle them during training.\n",
    "\n",
    "Once the dataset is loaded into the DataLoader, we can fetch a batch of data and inspect its shape to verify that everything is working as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"~/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CIFAR-10 dataset and apply the transformations\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=dataset_dir, train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# Create a DataLoader\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=64, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "# Get a batch of data to verify that everything is working\n",
    "# dataiter = iter(trainloader)\n",
    "images, labels = next(iter(trainloader)) #dataiter.next()\n",
    "\n",
    "print(f\"Batch of images shape: {images.shape}\")\n",
    "print(f\"Batch of labels shape: {labels.shape}\")\n",
    "print(f\"Possible classes: {', '.join(trainset.classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the batch size is of 64x3x32x32:\n",
    "- 64 is the batch size, i.e. how many images are processed at once by the model\n",
    "- 3 is the number of channels (RGB)\n",
    "- 32x32 is the size of the image\n",
    "\n",
    "We can plot these images in an 8x8 grid to get a sense of what they look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(8,8, figsize=(8,8))\n",
    "plt.rcParams.update({'axes.titlesize': 'small'})\n",
    "plt.tight_layout()\n",
    "# remove vertical space\n",
    "plt.subplots_adjust(hspace=0.25, wspace=0.1)\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        # note that we need to change the order of the dimensions.\n",
    "        # Torch prepare the images to be (C, H, W),\n",
    "        # But matplotlib expects (H, W, C)\n",
    "        # (H = height of the image, W = width of the image, C = number of color channels)\n",
    "        ax[i,j].imshow(images[i*8 + j].permute(1,2,0))\n",
    "        ax[i,j].set_title(trainset.classes[labels[i*8 + j]])\n",
    "        ax[i,j].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Data Transformations\n",
    "\n",
    "In deep learning, especially in computer vision tasks, **data transformations** are crucial for preparing input data to enhance model performance and generalization. Transformations involve modifying the inputs (e.g., images) in the dataset to make the model more robust to variations in input data and prevent overfitting.\n",
    "\n",
    "In this exercise, we will apply several common transformations to the CIFAR-10 and CIFAR-100 datasets. It's important to note that the transformations applied to the **training** data differ from those applied to the **validation** data:\n",
    "\n",
    "##### Training Transformations\n",
    "Training transformations are designed to augment the dataset, effectively increasing the diversity of the input images seen by the model during training. This helps improve the model's ability to generalize to unseen data. Common training transformations include:\n",
    "\n",
    "1. **Random Crop**: Randomly crops a given portion of an image. This helps the model become invariant to the position of objects within the image.\n",
    "2. **Random Horizontal Flip**: Randomly flips an image horizontally. This technique ensures that the model learns features regardless of the orientation of objects.\n",
    "3. **Normalization**: Adjusts the pixel values to have a mean of 0 and a standard deviation of 1. Normalization helps speed up training and can lead to better model performance.\n",
    "\n",
    "##### Validation Transformations\n",
    "Validation transformations, on the other hand, are intended to provide a consistent evaluation of model performance by ensuring the input data is in a standardized format. Typically, validation transformations include:\n",
    "\n",
    "1. **Normalization**: Similar to training, normalization is applied to adjust the pixel values to have a mean of 0 and a standard deviation of 1. This ensures that the distribution of values in the validation samples is the same one used in the training samples (since the model already learned to expect that distribution of values).\n",
    "\n",
    "So, the key difference is that training transformations often include **random augmentations** (such as cropping and flipping) to introduce variability, whereas validation transformations are more standardized and deterministic to provide a stable basis for evaluating the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find the mean value and the standard deviation of the images in the dataset\n",
    "# Note:\n",
    "# - The images are available in trainset.data\n",
    "# - Scale the images to the [0,1] range before proceeding\n",
    "# - Compute a different mean and standard deviation for each color channel\n",
    "images_mean = ...\n",
    "images_std = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train_example = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally for the data augmentation\n",
    "    transforms.RandomCrop(32, padding_mode='reflect', padding=5),  # Randomly crop the image for the data augmentation\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize(images_mean, images_std)  # Normalize the tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the transformations, we can pass the transformation objects to the `transform` argument of the `CIFAR10` class. This way, the transformations are automatically applied when loading the data using the `DataLoader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the same image looks like after applying these transformations. \n",
    "\n",
    "Note that we are creating a different transformation, without the normalization step (& conversion to tensor), for the visualization. This occurs because the normalization step would make the image look strange, as it would change the pixel values to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # note the padding_mode policy: this implies that the \"missing\" pixels\n",
    "    # will be reflected from the border pixels\n",
    "    # (more info in the documentation => https://pytorch.org/vision/main/generated/torchvision.transforms.RandomCrop.html)\n",
    "    transforms.RandomCrop(32, padding_mode='reflect', padding=5)\n",
    "])\n",
    "\n",
    "im = transforms.ToPILImage()(images[10])\n",
    "n_transforms = 9\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize=(4,4))\n",
    "# TODO: Visualize the original image ,then apply different transformations to it\n",
    "# Note: each time you apply the viz_transform transformation, a random transformation will be applied to the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Extra stuff!</span>\n",
    "\n",
    "Other transformations are already supported in PyTorch, such as:\n",
    "- **RandomRotation**: Rotates the image by a random angle.\n",
    "- **RandomResizedCrop**: Crops the image to a random size and aspect ratio, then resizes it to the specified size.\n",
    "- **ColorJitter**: Randomly changes the brightness, contrast, saturation, and hue of an image.\n",
    "- **RandomAffine**: Applies a random affine transformation to the image.\n",
    "- **RandomPerspective**: Applies a random perspective transformation to the image.\n",
    "- **RandomErasing**: Randomly selects a rectangle region in an image and erases its pixels.\n",
    "\n",
    "You can find out more about those transformations in the [official documentation](https://pytorch.org/vision/stable/transforms.html).\n",
    "\n",
    "Try applying some of these transformations to the CIFAR-10 dataset and observe how they affect the images. You can also experiment with different parameters to see how they influence the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CIFAR-10 dataset and apply the transformations\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='/home/fgiobergia/data', train=True, download=True, transform=transform_train_example\n",
    ")\n",
    "\n",
    "# Create a DataLoader\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=64, shuffle=True, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **2.0 Building and Training a Simple Neural Network for CIFAR-10 Classification**\n",
    "\n",
    "In this exercise, we extend our exploration of machine learning models by constructing a simple neural network to classify images from the CIFAR-10 dataset using PyTorch. Unlike the linear model, which is limited to learning linear relationships between features, this neural network introduces multiple layers and non-linear activation functions, allowing it to capture more complex patterns in the data.\n",
    "\n",
    "A neural network consists of interconnected layers of nodes, or neurons, where each layer applies a linear transformation to its input, followed by a non-linear activation. The architecture we will implement includes:\n",
    "\n",
    "1. **Input Layer**: Accepts the flattened input image, with each pixel value as a feature.\n",
    "2. **Hidden Layers**: Two fully connected layers that transform the input into higher-level features using linear transformations and ReLU activation functions.\n",
    "3. **Output Layer**: A final fully connected layer that maps the transformed features to the 10 output classes of the CIFAR-10 dataset.\n",
    "\n",
    "By training this model, we aim to understand how neural networks can learn complex representations of data, enabling them to perform more sophisticated tasks such as image classification. This exercise will also introduce key concepts such as non-linearity, deep learning, and the importance of layer depth in enhancing model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a more complex neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # TODO: Define the linear layers for the model\n",
    "        # Notes:\n",
    "        # - Each layer requires specifying the number of input and output features\n",
    "        # - The first layer takes as input a flattened image (32x32x3) and should produce 512 features\n",
    "        #   (i.e., it encodes the image into a 512-dimensional space)\n",
    "        # - The second layer should produce 256 features\n",
    "        # - The output layer should produce 10 features (these will be the logits)\n",
    "\n",
    "        # number of input features: 32*32*3 (image size)\n",
    "        self.fc1 = ...\n",
    "        self.fc2 = ...\n",
    "        self.fc3 = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: implement the forward pass\n",
    "        # Notes:\n",
    "        # - first step: flatten the input (preserve the batch size, flatten the rest)\n",
    "        # - apply the first linear layer with ReLU activation\n",
    "        # - apply the second linear layer with ReLU activation\n",
    "        # - apply the output layer (no activation needed for the output layer)\n",
    "        x = ...\n",
    "        x = ...\n",
    "        x = ...\n",
    "        x = ...\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformations\n",
    "\n",
    "Note that we are using fixed means and standard deviations for the normalization step. These values are the means and stds of the ImageNet dataset, which has been used to pretrain the ResNet model (so the model now expects this normalization!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the training and test datasets\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding_mode='reflect', padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "batch_size = 1024\n",
    "trainset = torchvision.datasets.CIFAR10(root=dataset_dir, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=dataset_dir, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(model, testloader):\n",
    "    model.eval() # NOTE: required to set the model to evaluation mode (some layers may behave differently during training and evaluation)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): # NOTE: we disable gradient tracking for validation, to save memory!\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Accuracy of the network on the test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # TODO: let's implement the training loop for the model\n",
    "        # Notes:\n",
    "        # - First, zero the parameter gradients\n",
    "        # - Next, get the model's predictions\n",
    "        # - Compute the loss\n",
    "        # - Perform backpropagation (i.e. compute all gradients)\n",
    "        # - Finally, update the model's parameters\n",
    "        ...\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Extra stuff!</span>\n",
    "\n",
    "Now the model has a bunch more weights and biases (parameters) to learn. Unlike before, we don't know exactly what the \"right\" values should be. Pick a few random parameters from the various layers and keep track of how their values change over time!\n",
    "\n",
    "(Note: make sure to extract the values from the tensors using the .item() method, as shown in the previous example, otherwise the computation graph will keep track of the values and you will run out of memory!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the model's performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can visualize the loss behavior over time. This can help us reach a few conclusions about the training process. \n",
    "\n",
    "For instance, a \"noisy\" loss may indicate that the learning rate may be too large, or that the batch size is too small. If, after reaching the target number of epochs, the loss is still decreasing, it may be a sign that the model could benefit from additional training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(losses)\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Extra stuff!</span>\n",
    "\n",
    "While this model performs better than random guess (~10% accuracy), it still far from perfect. Compute the confusion matrix on the validation set. Which classes does the model classify better? Which classes does it struggle with? Which classes are most often confused with each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Load and Train a Model on CIFAR-10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root=dataset_dir, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=dataset_dir, train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Load a pre-trained model (ResNet18)\n",
    "model = models.resnet18(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)  # CIFAR-10 has 10 classes\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "val_model(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(losses)\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Load and Train a pre-trained Model on CIFAR-10**\n",
    "\n",
    "In this exercise, we will load a pre-trained model (ResNet-18) from PyTorch's model zoo, modify it to fit the CIFAR-10 dataset, and fine-tune it. The pre-trained model has been trained on the ImageNet dataset, a large-scale dataset with 1.2 million images across 1000 classes. By leveraging the pre-trained model's knowledge, we can significantly reduce the training time and computational resources required to achieve high performance on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load a pre-trained model (ResNet18)\n",
    "# Notes:\n",
    "# - To load a pre-trained model, call models.resnet18 and set the weights argument to 'DEFAULT'\n",
    "# - The model has a fully connected layer at the end (called `fc`) that needs to be replaced with a new one of the appropriate size\n",
    "# - To get the number of input features for the new layer, you can access the in_features attribute of the original fc layer\n",
    "\n",
    "\n",
    "pretrained_model = ...\n",
    "...\n",
    "pretrained_model = pretrained_model.to(device) # move the model (all layers, even new ones) to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(pretrained_model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = pretrained_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model(pretrained_model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(losses)\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Change the Classification Head of an already trained Model on CIFAR-100**\n",
    "\n",
    "In this exercise, we will modify the classification head of a pre-trained model on CIFAR-10 to adapt it to the CIFAR-100 dataset. This process involves changing the output layer of the model to accommodate the 100 classes in CIFAR-100 instead of the 10 classes in CIFAR-10. By changing the classification head, we can repurpose the pre-trained model to perform well on a different task with minimal additional training. \n",
    "\n",
    "We do this by employing a common technique in transfer learning, known as **layer freezing**, to adapt a pre-trained model to a new task with minimal training time. Layer freezing involves setting the `requires_grad` attribute of specific model parameters to `False`, effectively preventing them from being updated during the backpropagation process. By freezing all layers except for the final classification head, we retain the knowledge encoded in the pre-trained layers while allowing the model to learn task-specific features through the unfrozen layers.\n",
    "\n",
    "This approach is usually developed in NLP and LLMs since the training of these models requires vast computational resources and large amounts of data. By fine-tuning the pre-trained model on a specific task, we can leverage the knowledge learned from the original training data and adapt it to new tasks with limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-100 dataset\n",
    "trainset_cifar100 = torchvision.datasets.CIFAR100(root=dataset_dir, train=True, download=True, transform=transform_train)\n",
    "trainloader_cifar100 = torch.utils.data.DataLoader(trainset_cifar100, batch_size=100, shuffle=True)\n",
    "\n",
    "testset_cifar100 = torchvision.datasets.CIFAR100(root=dataset_dir, train=False, download=True, transform=transform_test)\n",
    "testloader_cifar100 = torch.utils.data.DataLoader(testset_cifar100, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the classification head for CIFAR-100 (100 classes)\n",
    "\n",
    "model = pretrained_model\n",
    "model.fc = nn.Linear(model.fc.in_features, 100)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we freeze all layers except for the newly modified classification head by iterating through the model's parameters and setting `requires_grad = False` for each. Subsequently, we explicitly enable gradient computation (`requires_grad = True`) for the parameters in the classification head. This approach allows us to fine-tune the output layer to classify the 100 classes in the CIFAR-100 dataset while preserving the general features learned from the CIFAR-10 dataset, ensuring that the model adapts effectively to the new task using the information already learned from the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except the classification head\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# TODO: Unfreeze the classification head\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader_cifar100:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader_cifar100):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_model(model, testloader_cifar100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(losses)\n",
    "ax.set_xlabel(\"Steps\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 3: PyTorch Computational Graph**\n",
    "\n",
    "In this exercise, we will explore how PyTorch constructs a computational graph dynamically when performing operations on tensors. This graph is fundamental to PyTorch’s ability to compute gradients automatically during the backward pass, which is essential for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Define tensors with requires_grad=True to track operations\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "c = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# Define a function involving these tensors\n",
    "y = (a*b*c+c)*c + b\n",
    "\n",
    "# Visualize the computational graph\n",
    "make_dot(y, params={\"a\": a, \"b\": b, \"c\": c})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Backpropagation in PyTorch**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the following example, and let PyTorch build the computational graph for us. \n",
    "\n",
    "![computational_graph](./computational_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "import torch\n",
    "\n",
    "# TODO: define the tensors and compute the loss\n",
    "# Define theta1, theta2, x, and y as tensors, then compute the loss\n",
    "# Notes:\n",
    "# - set theta1 = 2.0, theta2 = 3.0, x = 1.0, y = 3.0\n",
    "# - the loss is defined as (theta1 * theta2 * x - y)^2\n",
    "# - all the tensors should be specified to require gradients\n",
    "theta1 = ...\n",
    "theta2 = ...\n",
    "x = ...\n",
    "y = ...\n",
    "\n",
    "loss = ...\n",
    "\n",
    "# TODO: Visualize the computational graph\n",
    "make_dot(..., params=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform backpropagation to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Gradients:\")\n",
    "print(\"dL/d(theta1):\", theta1.grad)\n",
    "print(\"dL/d(theta2):\", theta2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Partial Derivatives\n",
    "\n",
    "![backpropagation](./backpropagation.png)\n",
    "\n",
    "Given the function:\n",
    "\n",
    "$\\mathcal{L} = \\left( \\theta_1 \\theta_2 x - y \\right)^2$\n",
    "\n",
    "we can use the chain rule from calculus, and apply it from the end of the computational graph backwards to compute the partial derivatives of the loss function $\\mathcal{L}$ with respect to the parameters $\\theta_1$ and $\\theta_2$.\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial c} = \\frac{\\partial c^2}{\\partial c} = 2c$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial a} = \\frac{\\partial \\mathcal{L}}{\\partial b} \\frac{\\partial b}{\\partial a} = 2c \\frac{\\partial ax}{\\partial a} = 2cx$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial \\mathcal{L}}{\\partial c} \\frac{\\partial c}{\\partial b} = 2c \\frac{\\partial (b - y)}{\\partial b} = 2c$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_1} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\frac{\\partial a}{\\partial \\theta_1} = 2cx \\frac{\\partial \\theta_1 \\theta_2}{\\partial \\theta_1} = 2cx \\theta_2$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_2} = \\frac{\\partial \\mathcal{L}}{\\partial a} \\frac{\\partial a}{\\partial \\theta_2} = 2cx \\frac{\\partial \\theta_1 \\theta_2}{\\partial \\theta_2} = 2cx \\theta_1$\n",
    "\n",
    "so: \n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_1} = 2 (\\theta_1 \\theta_2 x - y) x \\theta_2$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_2} = 2 (\\theta_1 \\theta_2 x - y) x \\theta_1$\n",
    "\n",
    "---\n",
    "\n",
    "By changing the variables with the values given in the code, we can compute the partial derivatives of the loss function with respect to the parameters $\\theta_1$ and $\\theta_2$.\n",
    "In fact, considering the values:\n",
    "\n",
    "- $x = 1$\n",
    "- $y = 3$\n",
    "- $\\theta_1 = 2$\n",
    "- $\\theta_2 = 3$\n",
    "\n",
    "we can compute the partial derivatives of the loss function $\\mathcal{L}$ with respect to the parameters $\\theta_1$ and $\\theta_2$.\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_1} = 18$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_2} = 12$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Extra stuff!</span>\n",
    "\n",
    "The previous gradient computations have involved each weight ($\\theta_1$, $\\theta_2$) only once. However, in practice, the same weight may be used multiple times to compute the same loss. For instance, we may have a loss involving two terms, using the same weight $\\theta$ twice. In these cases, we typically sum multiple loss contributions and weigh each of them differently (e.g., with a coefficient). \n",
    "\n",
    "Or, as we saw earlier, we generally compute losses for mini-batches of data. This means that we use the same weight multiple times (once per sample in the mini-batch). \n",
    "\n",
    "Consider the following losses. Compute the gradients w.r.t. the model's parameter $\\theta$ analytically:\n",
    "- $\\mathcal{L} = |\\theta x - y| + \\lambda (\\theta x - y)^2$\n",
    "- $\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^N (\\theta x_i - y_i)^2$\n",
    "\n",
    "How can PyTorch handle these cases? (Remember, the AccumulateGrad node!).\n",
    "\n",
    "Now, consider that a new gradient should be computed for every batch of data. How can PyTorch handle this? (Remember, the zero_grad() method!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
